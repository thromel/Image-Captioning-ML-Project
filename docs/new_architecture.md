Excellent. I’ll initiate a deep research task focused on enhancing your existing Show, Attend and Tell-based image captioning system. The priority will be improving semantic accuracy and diversity of captions, with secondary emphasis on model size efficiency. The research will explore modular upgrades to the encoder, attention, decoder, and language embedding components, emphasizing transformer-era architectures.

I’ll also ensure:

* Pretrained models and licensing terms are clearly noted
* Key implementation patterns and integration examples are included
* Evaluation strategies and inference optimizations are covered comprehensively

I’ll get back to you with a detailed and structured report covering all aspects of this modernization strategy.


# Enhancing the Image Captioning System: Modern Architectures and Techniques

***Introduction:*** Image captioning models have progressed significantly since the original **“Show, Attend and Tell”** CNN+LSTM with soft attention. Modern approaches use transformers, richer vision backbones, and large-scale pretraining to generate more accurate and diverse captions. **Figure 1** illustrates this evolution: (a) the traditional encoder-decoder (CNN + attention + RNN) pipeline, (b) a basic transformer-based model (vision features encoded by CNN, then a Transformer decoder), and (c) a vision-language pretrained transformer model that leverages large image-text datasets. Our goal is to redesign the current ResNet-101 + LSTM system into a **modular** framework incorporating these advances – improving semantic accuracy and linguistic diversity, while keeping an eye on efficiency. Below, we explore upgrades in each component (vision encoder, attention mechanism, decoder, language embeddings, and fusion techniques), as well as training strategies, evaluation metrics, inference methods, and practical implementation considerations (modularity, PyTorch 2.x integration, deployment, and licensing).

## 1. Vision Encoder Upgrades

Modern vision backbones can provide **richer visual features** and better integration with attention mechanisms than the legacy ResNet-101. We evaluate several candidate encoders:

* **Vision Transformers (ViT)** – ViT treats an image as a sequence of patch tokens and applies a pure Transformer encoder with multi-head self-attention across the entire image. This global receptive field allows ViT to capture long-range relationships (e.g. context between distant objects) that ResNet’s localized convolutions might miss. ViTs pre-trained on large datasets (ImageNet-21k or JFT-300M) achieve superior classification accuracy and transfer well to tasks like captioning. The attention granularity in ViT is at the patch level, meaning the model can attend to fine image details or global layout as needed. On the downside, vanilla ViT has quadratic complexity in the number of patches – for high-resolution images this can be computationally intensive (though patch merging or smaller patch sizes can mitigate this). **Pretrained models:** ViT weights (e.g. B/16, L/14, huge variants) are available openly (many under Apache-2.0 or similar licenses via the Timm or HuggingFace hubs). These provide a drop-in replacement for a CNN encoder, outputting a sequence of patch feature vectors instead of a 2D feature map. In a captioning model, we can feed those vectors to the attention module or directly to a Transformer decoder. Given that model size is a secondary concern and we have powerful GPUs, a large ViT (with, say, 24 or more layers) can be used to maximize feature richness.

* **Swin Transformer** – Swin is a hierarchical vision transformer that introduces a **shifted window** approach to limit self-attention to local regions, then merges patches progressively. This design is **computationally efficient** (linear complexity w\.r.t. image size) yet still allows cross-region interactions by shifting the window positions at each layer. Swin encoders achieved state-of-the-art results in detection and segmentation, indicating they learn strong multi-scale features. For captioning, Swin’s features provide a mix of local detail and global context, potentially improving descriptions of both small objects and overall scene. In comparisons, Swin and ViT are often among the top performers in accuracy vs. speed trade-offs. **Pretrained weights:** Microsoft released Swin-B, Swin-L etc. under MIT license, readily usable in PyTorch (e.g. via `timm` library). These models are somewhat heavier than ResNet-101 (Swin-L has \~197M params vs. 44M in ResNet-101) but the accuracy gain may justify it, especially since we prioritize caption quality over model size.

* **ConvNeXt** – This is a **next-generation CNN** that revisits ConvNet design with modern tweaks (ResNet-like architecture with larger kernels, SiLU activations, LayerNorm, etc.). ConvNeXt models (e.g. ConvNeXt-Large, \~200M params) have matched transformer accuracy on ImageNet while retaining the convolutional inductive biases. For our purposes, ConvNeXt could be a strong encoder if we want *CNN simplicity with SOTA performance*. It produces a feature map like ResNet, so integrating it into the existing attention module is straightforward (spatial feature vectors similar to what we have now). It may not offer the explicit long-range attention of ViTs, but its large receptive fields and deep layers give it rich semantic features. **Pretrained models:** ConvNeXt weights are available under permissive licenses (the official implementation is in PyTorch and models are in `timm`). Using ConvNeXt might also ease future deployment on edge devices, since CNNs are often easier to optimize on limited hardware (e.g. via depthwise conv acceleration).

* **EfficientNetV2** – EfficientNetV2 is an updated family of efficient CNNs that optimize the depth/width/resolution scaling with Neural Architecture Search. These models achieve excellent accuracy with relatively few FLOPs by using fused MBConv layers and other training tweaks. For a captioning encoder, EfficientNetV2 could drastically reduce computation while still providing high-quality features (e.g. EfficientNetV2-L is \~120M params but very strong on classification). **Feature richness:** It may not capture as wide a range of high-level concepts as a ViT trained on enormous data, but it excels at general visual feature extraction. EfficientNetV2 might be a good choice if we later target **lightweight inference** (e.g. on mobile or edge) – we could even **swap in** an EfficientNet encoder in our modular design when exporting a smaller model. **Availability:** Pretrained EfficientNetV2 weights (B0-B3 for smaller, up to L) are available from Google Brain (Apache-2.0 license) and in libraries like TensorFlow Hub and PyTorch implementations.

* **CLIP Vision Encoder** – CLIP (Contrastive Language-Image Pretraining) is a model trained on 400 million image-text pairs to align image and text embeddings. Its vision encoder (which can be a ResNet-50/101 or ViT-B/16, ViT-L/14) produces embeddings highly correlated with semantic concepts and their textual descriptions. Using a CLIP encoder in our captioning model can significantly enhance *semantic accuracy*: the features are already aligned to recognize a wide variety of objects, actions, and attributes as described in natural language. For example, CLIP’s features might more easily distinguish fine-grained categories or relate an image to concepts not explicitly in the training captions. In practice, one approach (as done by *ClipCap*) is to freeze the CLIP image encoder and feed its output to the decoder via a small learned projection. This leverages CLIP’s semantic knowledge without requiring us to train the encoder from scratch. **Pretrained models:** OpenAI’s CLIP ResNet50/ViT-B models are available (code is MIT-licensed). However, OpenAI’s model card notes that “any deployed use – whether commercial or not – is out of scope”, raising some ambiguity for commercial use. To stay on the safe side, we can use **OpenCLIP**, a community reproduction of CLIP (trained on the LAION dataset) released under MIT license. These models (e.g. OpenCLIP ViT-H/14) are fully open and even larger than the original CLIP, offering excellent vision features. We should be mindful that CLIP encoders are **computationally heavy** (ViT-L/14 has \~300M params). But given our target environment (GPU servers) this is acceptable, and for edge deployment we could switch to a smaller CLIP or different backbone.

* **DiT (Document Image Transformer)** – DiT is a vision transformer tailored for document images (pretrained on text-heavy images like scanned documents). Its architecture is similar to ViT (self-attention over image patches). For general image captioning on MS-COCO, DiT is not an obvious choice since it specializes in layouts of text/figures in documents. Unless our captioning domain includes OCR or document understanding, a standard ViT or CNN will be more effective. However, DiT’s existence shows the flexibility of transformers – if needed, one could fine-tune DiT on natural images, but it would require learning visual features from scratch if not already present. We mention DiT mainly for completeness, as our emphasis is on *natural images*. Pretrained DiT weights (from Microsoft’s document AI research) are available, but they come from a specific domain (documents) and likely under a research license. We will prioritize encoders known to perform well on MS-COCO (which contains everyday scenes).

**Summary:** Upgrading the encoder from ResNet-101 to a **ViT or Swin Transformer** is a promising route for better feature representations. These provide *global attention and richer semantic embedding* of the image content, which should improve caption accuracy and possibly diversity (by recognizing more elements in the scene). **Commercial considerations:** ViT, Swin, ConvNeXt, EfficientNetV2 are all published with permissive licenses (e.g. Apache-2.0 or MIT), so using them (and even fine-tuning their weights) in a product is feasible. CLIP’s vision model offers a semantic head-start; we’d likely opt for an open variant of it to avoid license concerns. In our modular design, we can implement an `ImageEncoder` interface so that swapping between these options is as simple as changing a configuration file. For instance, we might start experiments with a CLIP ViT-L/14 encoder for maximum accuracy on GPU, and later try an EfficientNetV2 or smaller Swin for a faster model that could run on edge devices.

## 2. Attention Mechanism Enhancements

The original Show-Attend-Tell model used a **single-head “soft” attention** (an additive attention mechanism) to focus on image features when generating each word. We can improve upon this in several ways:

* **Scaled Dot-Product Attention (Transformers)** – Modern attention uses the scaled dot-product formulation introduced by Vaswani et al. (2017). Given queries \$Q\$, keys \$K\$, and values \$V\$, the attention is \$\text{Attn}(Q,K,V) = \text{softmax}!\big(QK^T/\sqrt{d\_k}\big),V\$. This is typically implemented in multiple heads (see below). This mechanism is not only more efficient than the older additive attention, but also integrates naturally into Transformer architectures. By scaling the dot-products by \$\sqrt{d\_k}\$ (where \$d\_k\$ is key dimensionality), it prevents extremely large values that could saturate the softmax. Adopting scaled dot-product attention in our caption decoder will make it compatible with transformer-based encoders and allow **multiple attention heads** attending in parallel. In practice, frameworks like PyTorch provide optimized kernels for this operation (especially when using `nn.MultiheadAttention`), so we gain speed and quality.

* **Multi-Head Attention** – Instead of producing one attention distribution over image regions per timestep, multi-head attention uses several independent attention heads that each focus on potentially different aspects. Each head has its own learned linear projections of queries, keys, and values, so it might attend to different object positions or image attributes. The results from all heads are then concatenated and linearly combined. For captioning, multi-head attention can improve the model’s ability to **capture multiple relevant objects or features simultaneously**. For example, one head might attend strongly to a “person” region while another head attends to a “kite” in the background, enabling the decoder to incorporate both into the description ("a person flying a kite"). This could enhance caption *completeness and diversity*, as the model is less likely to ignore secondary elements. We can illustrate the idea with pseudocode:

  ```python
  # Pseudo-code for multi-head cross-attention (text attends to image regions)
  for each head h = 1...H:
      Q_h = Linear_h^Q(decoder_hidden_state)      # project decoder state
      K_h = Linear_h^K(image_region_features)     # project image features (N_regions x d)
      V_h = Linear_h^V(image_region_features)
      attn_weights_h = softmax( Q_h @ K_h.T / sqrt(d_k) )  # size (1 x N_regions)
      head_output_h = attn_weights_h @ V_h                # weighted sum of values (1 x d_v)
  # Concatenate all head outputs and project
  combined = Linear^O( concat(head_output_1 ... head_output_H) )
  ```

  This would replace the single-head additive attention in our LSTM decoder. In a Transformer decoder, multi-head attention is built-in for both self-attention and cross-attention sublayers.

* **Cross-Modal (Co-Attention) Mechanisms** – In encoder-decoder models, “cross-attention” refers to the decoder attending to encoder outputs (image features, in our case) at each decoding step. A standard Transformer decoder already does this: each decoder layer has a multi-head cross-attention block where queries come from the textual hidden states and keys/values from the image encoder output. We can extend this idea to **more elaborate co-attention** architectures: for instance, some research employs a **bi-directional co-attention**, where image features attend to text as well as text attending to image (this was seen in some VQA models and the *ViLBERT* two-stream model). In captioning, typically we generate text from image, so one-directional attention (text querying image) suffices; however, we might have an intermediate fusion module where image and text features refine each other. For example, a **co-attention block** could take image region features and caption embeddings and perform two attentions: image-to-text and text-to-image, then update both representations. This is similar to architectures like **Parallel Co-Attention** networks where an iterative process allows modalities to influence each other. *Architectural diagram:* A co-attention module could be depicted with two inputs (image features, text features) and two outputs, where each output is the result of attending to the other modality. In code, it might look like:

  ```python
  # One iteration of co-attention
  img_attended = MultiHeadAttention(query=image_feats, key=text_feats, value=text_feats)
  text_attended = MultiHeadAttention(query=text_feats, key=image_feats, value=image_feats)
  image_feats = Norm(image_feats + img_attended)
  text_feats = Norm(text_feats + text_attended)
  ```

  This kind of module could be used if we integrate a multimodal transformer (as in many vision-language pretraining models). **Impact:** Cross-modal attention ensures that the caption words are grounded in specific image content, and if iterative, that image features are filtered by relevant text context. This often improves alignment and can make attention maps more interpretable (we can visualize which words attend to which image regions and vice versa).

* **Object-Level Attention** – The current system likely uses a feature map (e.g. \$14\times14\$ spatial features from ResNet) for attention. An alternative is **object-based attention**, pioneered by Anderson et al. (2018) with the Bottom-Up Top-Down model. This approach uses a pretrained object detector (like Faster R-CNN) to extract region features for salient objects in the image, then attends over those **discrete object regions** instead of uniform grid cells. The advantage is that the attention targets have clear semantic meaning (each is an object or part of one), which can improve both the *diversity* and *interpretability* of captions. For example, rather than a diffuse attention over many background pixels for the word "men", the model might attend specifically to three detected person regions when generating "Three men wearing suits". This often yields captions that enumerate objects more explicitly. **Figure 3** shows how an object-based encoder/decoder pipeline works: an object detector proposes regions, producing appearance features (CNN features for each region) and geometry features (bounding box coordinates). These region features are then fed to the encoder-decoder model, which learns to attend to them when constructing the caption. In practice, implementing this means adding a preprocessing step: e.g. run a Faster R-CNN (with ResNet-101 backbone, as in the original Bottom-Up model) on each image to get \~10-100 region vectors. Then our attention mechanism (which could still be multi-head) operates over this set of vectors. Many research works have shown improved CIDEr/SPICE scores using object-based features, since the model can more easily ground nouns to actual objects. We should note that using a detector adds computational overhead and complexity (and the detector itself might have licensing restrictions if not open-source). But a compromise could be using a **pre-extracted dataset of region features** (as was common in 2018-2020 captioning research) so that at runtime, we only do lookups. If we prefer a single-stage model, the trend now is towards detector-free (using grid or patch features) because end-to-end training is easier. But nothing stops us from incorporating object attention as an option in our modular design.

* **Advanced Attention Variants** – There have been various tweaks proposed to improve attention for captioning:

  * *Adaptive Attention:* Instead of always relying on image features, Lu et al. (2017) introduced an adaptive gate that decides how much to attend to the image vs rely on the language model at each timestep. In effect, the model can choose to “listen” to the image when generating content words, and to rely on its own history when generating filler words or grammar. This improved metrics and gave more sensible behavior (e.g. for words like "the", the model might not attend to any image region). We can integrate this by adding a sigmoid gate that weights the context vector from attention and a “visual sentinel” vector (representing no-attention scenario).
  * *Attention on Attention (AoA):* Proposed by Huang et al. (2019), AoA adds a second attention layer that acts on the output of the first, essentially modeling an *attention-over-attention* score. The AoA module produces an *information vector* and an *attention gate* that together determine the final attended features. This was shown to improve performance by filtering out less relevant information even after the initial attention, yielding more precise captions.
  * *X-Linear Attention:* (Pan et al. 2020) extends attention with bilinear pooling, enabling modeling of higher-order interactions between image and text features. This “cross-modal multiplicative interaction” was part of the X-LAN model, which achieved strong results on COCO. It’s a more complex module (involving computing attention weights in multiple dimensions) – we might not implement it from scratch due to complexity, but it’s worth noting as a state-of-the-art idea.
  * *Spatial and Geometric Attention:* Herdade et al. (2019) added an explicit term to the attention that accounts for the geometric relationships between objects (distance and relative size). They modulate the attention weights by learned functions of the bounding box coordinates, so that, for example, the model can understand object proximity (useful for relational captions like "man **standing next to** a horse"). This is relevant if we use object regions: we could concatenate geometry features (e.g. \[x, y, width, height]) with visual features and let the attention network learn to use them, or explicitly implement the formula from Herdade et al. (which they refer to as **geometric attention**).

  Each of these variants aims to improve either the **focus** of attention (making it more precise and informative) or the **flexibility** (allow the model to decide when to use image info). Incorporating them would involve some custom coding, but our modular design could allow plugging in different Attention modules. For instance, we can have a base class `AttentionModule` and derive `AdditiveAttention`, `MultiHeadAttention`, `AdaptiveAttention` (with a gate), etc., and easily swap them.

**Impact on Caption Quality:** By enhancing the attention mechanism, we expect:

* **Better Diversity & Completeness:** Multi-head and object-based attention help capture multiple elements in the scene, which should reduce instances of missing objects in captions. A richer attention can enable the model to say “a man **and** a woman walking two dogs” where a simpler attention might only mention “a man walking a dog” if it tended to lock onto one object at a time. In metrics, this often boosts SPICE (which measures object and relation coverage) and CIDEr, since more reference details can be matched.
* **Interpretability:** Attention weights are often visualized to explain model decisions. With the original soft attention, we can overlay a heatmap on the image for each word. Multi-head attention adds nuance: we could visualize each head separately (though with many heads this becomes unwieldy), or average them to see overall importance. Object-based attention is particularly interpretable – we can draw bounding boxes around the regions with highest attention for a given word, giving a clear indication of what the model “thinks” each word refers to. This is valuable for debugging and for users (especially in assistive tech) to trust the system. Techniques like Grad-CAM can also be applied to Transformers to get similar visual explanations.
* **Robustness:** Advanced attention mechanisms can make the model more robust to clutter or multiple objects. E.g., geometric attention helps disambiguate which objects are near each other, possibly preventing errors like describing the wrong object relationships. Adaptive attention can prevent noise from irrelevant background features by learning to sometimes “ignore” the image. These improvements might not directly reflect in basic metrics like BLEU, but in human evaluations, they lead to captions that are more *faithful* and *less confused* about the scene.

In summary, we will move from the old single-head Bahdanau-style attention to a **Transformer-based multi-head attention** framework. This naturally aligns with using a Transformer decoder (next section). We will keep the door open to **object-level features** and **gating mechanisms** if experiments show they improve results. The modular design means we can maintain multiple Attention implementations and toggle them. We will cite relevant papers (many are open access) for any adapted components – e.g. if we use AoA, the original code (MIT-licensed) can be referenced. These attention enhancements are all research-proven and do not pose licensing issues by themselves (they are more like design patterns than standalone assets).

## 3. Modern Decoder Architectures

Upgrading the decoder is crucial for improving fluency and allowing integration of powerful pre-trained language capabilities. The current system’s decoder is an LSTM (possibly with attention). While LSTMs can generate decent captions, **Transformer-based decoders** have become the state-of-the-art for language generation tasks. We will compare and consider several options:

* **Transformer Decoder (GPT-style)** – A Transformer decoder consists of self-attention layers (where the caption words attend to previous words) and cross-attention layers (attending to image features), plus feed-forward networks. Using a transformer decoder (as in models like *AoANet, Transformer-Caption* or more recent ones) offers better parallelization during training (processing all timesteps at once with masking) and typically yields more fluent sentences than LSTM. It also removes recurrence, which can alleviate some issues like forgetting long-range context. A *GPT-style* decoder refers to a decoder-only transformer pretrained on massive text corpora (like OpenAI’s GPT-2 or GPT-3). For captioning, GPT-2 (117M to 1.5B parameters, trained on WebText) is a compelling choice: we can use a pre-trained GPT-2 model and **fine-tune it with image context**. For instance, *Hugging Face’s Transformers* library allows us to attach a vision encoder to a GPT-2 decoder via the `VisionEncoderDecoderModel` wrapper, which sets up cross-attention from the GPT-2 layers to the image encoder output. In code, it’s as simple as:

  ```python
  from transformers import VisionEncoderDecoderModel, ViTModel, GPT2LMHeadModel
  enc = ViTModel.from_pretrained('google/vit-base-patch16-224')
  dec = GPT2LMHeadModel.from_pretrained('gpt2')  # pre-trained GPT-2 weights
  model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(enc, dec)
  model.config.decoder_start_token_id = tokenizer.bos_token_id
  model.config.pad_token_id = tokenizer.pad_token_id
  # Then fine-tune on <image, caption> pairs...
  ```

  This approach was demonstrated in a blog titled "Image Captioning with ViT and GPT2" and yields a strong baseline. A GPT-style decoder brings **rich linguistic knowledge** (it has essentially learned English language modeling from billions of words). It produces varied vocabulary and more human-like syntax than an LSTM trained only on COCO captions (which are simplistic and limited). We can expect improvements in fluency (e.g., avoiding repetitive or telegraphic phrases) and possibly in correctness (GPT has seen factual text, though it can also produce plausible but incorrect content – we need to ground it with the image to avoid that). Integration challenges include: (1) GPT-2 uses byte-pair encoding with a specific vocabulary, which may differ from the one used in our dataset (we will likely just use GPT-2’s tokenizer for captions); (2) GPT-2 was not trained with an encoder, but adding cross-attention is a standard extension – indeed, models like *GIT2* (by Microsoft, one of the top models on nocaps) effectively use a GPT-like decoder for image captioning. Another challenge is **efficiency**: a large GPT-2 has up to 1.5B parameters in the language model alone. Fine-tuning that requires a lot of GPU memory. We might choose a medium-sized model (say GPT-2 base or large \~774M) initially, and ensure to use mixed precision and gradient checkpointing to manage memory. The benefit is we don’t have to train from scratch – we leverage the existing weights, which is much more data-efficient.

* **Seq2Seq Transformers (BART, T5)** – Another decoder approach is to use a sequence-to-sequence transformer architecture like BART or T5. **BART** (Facebook, 2019) is an encoder-decoder transformer pretrained as a denoising autoencoder (it learned to reconstruct corrupted text). **T5** (Google, 2019) is also encoder-decoder, pretrained on a massive text corpus with multiple tasks formulated as text-to-text. We can use the *decoder* part of these models for captioning, feeding image features as the “encoder output”. There are a few ways to do this:

  * Use the **full encoder-decoder**: Treat the image encoder as producing some kind of “pseudo-language” tokens that BART/T5’s encoder can ingest. For example, one could quantize image feature vectors into discrete tokens or project them to word embedding dimension and feed as if they were a sequence of prefix tokens to T5. In fact, Google’s *SimVLM* approach did something like this: treat image patches as a prefix sequence of tokens, and then do language modeling on the caption continuation.
  * More straightforward is using the **cross-attention** capability of the T5/BART decoder: One can bypass the text encoder entirely (or feed a dummy token there) and let the decoder attend directly to image features. This may require minor code adaptation because those models expect an encoder output. However, projects have done this by substituting the text encoder with an identity function (pass image features as “encoder hidden states”). For example, to use T5, we could take a ViT encoder to produce patch features, then use a small neural mapper to create “encoder hidden states” of shape (patch\_count, hidden\_dim) to feed into T5’s cross-attention layers. During generation, T5’s decoder will attend to these as it produces the caption.

  The advantage of BART/T5 is **powerful pretraining**: BART has a strong grasp of syntax and even some semantics (it was shown to be good at summarization, for instance), and T5 was trained with a fill-in-the-blank objective on huge data (C4 dataset). This means the decoder can handle more complex sentence structures or uncommon words better than an LSTM. These models also inherently support **bidirectional context** in the encoder (though not in decoder which is autoregressive). In practice, using them might yield captions that are more descriptive or use varied phrasing (BART has a large vocabulary and was not just predicting very literal captions).

  *Integration issues:* The main challenge is **dimension matching** – e.g., T5-base expects encoder hidden size of 768. If our image features are 2048-d (from a ResNet or Swin), we need a learned linear projection to 768, and perhaps also need to add positional encodings (some encoding of patch or region index) so that T5 knows there is an ordering (though for image regions order is arbitrary – we might just rely on self-attention to treat them as a set). Additionally, BART/T5 use a shared embedding for input and output tokens; for captions, that’s fine if we stick to English. We should make sure special tokens (e.g. end-of-sequence) are handled properly. From a **licensing** perspective: BART is under the MIT license (as part of fairseq or via Hugging Face – essentially open), and T5 is Apache-2.0 (everything from TensorFlow T5 to HF’s implementation). So no concerns there; we can use checkpoints like `facebook/bart-large-cnn` or `t5-base` freely.

* **Flamingo** – *Flamingo* (DeepMind, 2022) is a large-scale decoder that can ingest interleaved vision and text (few-shot learning on images+text). It consists of a frozen language model (e.g. Chinchilla or GPT-like) and **gated cross-attention** layers that read image features (from a CNN or ViT) into the model at appropriate points. Flamingo is a **huge model** (with billions of parameters) and proprietary, but it represents a trend of **LLM (Large Language Model) as decoder** for vision tasks. In our context, we likely won’t train a Flamingo-scale model from scratch, but we can borrow ideas:

  * Use a **frozen LLM** and learn a small set of adapter layers to interface with image. For example, Flamingo keeps the LLM frozen and only trains the gating networks that inject visual information. We could simulate something similar with a pre-trained GPT-3-sized model if it were available. However, GPT-3 is not open; instead, we might consider **LLaMA or Falcon** (open LLMs) if we were to go very large, but deploying those is non-trivial and likely beyond our scope for now.
  * The *concept* of Flamingo – a flexible module that can condition a big language model on images – could be applied in a smaller scale: e.g., take GPT-2 or T5, freeze most of it, and just train some projection from image features into the model’s layers (like prefix tuning or adapter tuning). This would cut down training cost and potentially preserve the language model’s full strength.

  In summary, Flamingo as-is might be too large to integrate (and no public weights are fully available), but it points toward using **few-shot capable decoders**. We may table this for future, since our current aim is a mid-scale system. If in a year open models like Flamingo become available, our modular framework could potentially swap in such a decoder.

* **Perceiver IO** – The Perceiver IO (DeepMind, 2021) is a unique architecture that can flexibly ingest and emit arbitrary modalities by using a latent array and repeated cross-attention. For image captioning, a Perceiver-based model could take image pixels or features as input and produce text tokens as output. The **strength** of Perceiver IO is its ability to scale to very high-dimensional inputs (like full images, video, etc.) by converting them to a fixed-size latent bottleneck via cross-attention, then decoding from that latent. It’s also very flexible: you don’t need to design a CNN or patch scheme; the model learns to attend to what's important. Using Perceiver for captioning is still a bit experimental, but it could potentially handle multi-modal input (if later we add audio or other signals to captioning). The Perceiver IO model is relatively lightweight compared to huge transformers (one can choose the latent size, e.g. 256, and depth as needed). One challenge: the original Perceiver IO wasn’t trained on captioning, so we’d train it from scratch or fine-tune it if a pretrained checkpoint for V+L tasks existed. There has been some work on using Perceiver for images (e.g., *Perceiver AR* for autoregressive text, *Perceiver for multimodal* for video+audio). Since our priority is accuracy, we might not choose Perceiver as the first option, but we note it as a design that could unify encoder/decoder into one (it can learn an implicit decoder). If we do try it, the code is open-source (Apache license) and we can integrate it as a drop-in model for research purposes.

**Replacing the LSTM:** Transitioning to any of these decoder architectures requires adjusting how training data is handled:

* We’ll need to use subword tokenization (e.g. GPT-2’s Byte-Pair Encoding or T5’s SentencePiece) instead of word-level tokens, to match the pretrained models. This improves handling of rare words and allows unlimited vocab. MS-COCO captions are relatively short (max \~20 words), so sequence length isn’t a big problem even for transformers.
* The loss function remains cross-entropy on next-token prediction, compatible with these decoders. We can initialize the decoder with pre-trained weights and then fine-tune on caption data using the standard teacher-forcing approach (feeding ground-truth tokens). After that, we can consider finetuning with RL (SCST) which we discuss later.
* During inference, we must implement autoregressive generation with these models, which is slightly more complex than with an LSTM because of self-attention caching. Libraries like Hugging Face’s `generate()` can handle this for us, or we can manually loop, caching the key/value tensors for each layer to avoid recomputation. The complexity is manageable with existing tools.

**Integration Example:** We illustrated GPT-2 integration via Hugging Face’s VisionEncoderDecoder. Similarly, one could do `VisionEncoderDecoderModel.from_encoder_decoder_pretrained(enc_model_name, 't5-base')` to get a ViT + T5 model. This automatically sets up cross-attention layers in T5’s decoder to attend to the encoder’s output. We might want to customize some of it (e.g., tie encoder weights if needed or adjust positional encodings), but it’s a great starting point. Another approach used in research (like the **ClipCap** model) is to add a small neural network that maps image features to a sequence of hidden states that serve as a prefix for a GPT-2 decoder. This “prefix tuning” approach keeps GPT-2 essentially unchanged and just learns the prefix transformation. It was shown to work well for COCO captions. We could replicate this: have our new ResNet/ViT encoder produce an embedding, then an MLP generates a 10-token-length prefix which is fed into GPT-2, and GPT-2 then generates the caption continuation. This is parameter-efficient (only the MLP is trained) and retains GPT-2’s full language model ability.

**Benefits of Modern Decoders:** By adopting a transformer-based or pre-trained language decoder, we anticipate:

* Greatly improved **linguistic diversity and fluency**. The model will not be as biased towards the small training corpus phrasing. It may use more synonyms or varied sentence structures (which could improve METEOR and SPICE scores, and definitely human preference).
* Possibly better **longer captions or detailed captions**: Transformers can handle longer sequences without forgetting the start, so if needed the model could generate two sentences or a very detailed single sentence. (COCO doesn’t require long paragraphs, but if we ever use this on say a dense-captioning task, it helps.)
* **Easier fusion with embeddings:** If we want to incorporate BERT or other embeddings (section 4), a transformer decoder naturally can take those as initial inputs or context.
* **Future extensibility:** If later we gather a much larger caption dataset or want to do **image paragraph captioning**, having a transformer decoder is almost necessary to scale to that scenario.

Finally, **licensing**: GPT-2, BART, T5 are all open. Flamingo’s concept we’d implement ourselves in a limited way, not using any closed source. Using these pre-trained weights (GPT-2, etc.) is allowed in commercial products (they are under MIT or Apache). We just need to attribute possibly in documentation. Integrating them via Hugging Face’s library also ensures we abide by model card conditions (none of these have a non-commercial clause except some newer LLMs like LLaMA which we are not using).

Given our target environment (powerful GPUs), a transformer decoder is appropriate. We might start with a moderately sized one (like GPT-2 base or BART-base) to validate the pipeline, then scale up. We should design the code such that `CaptionDecoder` could be an LSTM or a HuggingFace transformer model – enabling us to easily experiment and even ensemble different decoders if desired.

## 4. Language Model Embeddings and Context

In the current system, they have “optionally integrated BERT embeddings.” This likely means they used pre-trained BERT to provide word embeddings or context vectors to the decoder (perhaps concatenating BERT’s output with the LSTM hidden state at each time step). The idea is to infuse external language knowledge. We want to systematically examine options for **language model embeddings**:

* **BERT, RoBERTa, DeBERTa (Contextual Encoders)**: BERT (Devlin et al. 2018) and its improved variants RoBERTa (Facebook) and DeBERTa (Microsoft) provide powerful contextual embeddings for words. For example, in a caption “a **bank** on a river bank”, BERT would produce different vectors for the two instances of “bank” because of context. In a captioning model, one could use BERT to encode the partial sentence generated so far to inform the next-word prediction. However, BERT is bidirectional and not designed for incremental, left-to-right use (it sees the whole sentence). One workaround is to use **BERT embeddings for the ground-truth caption** during training and feed those to the decoder; but at test time, we wouldn’t have ground truth future words, so we’d need to either run BERT on the fly for each partially generated sequence (which is very slow), or use a left-to-right variant of BERT. RoBERTa and DeBERTa are similar (they’re also MLM-based encoders). DeBERTa in particular has an improved architecture with disentangled attention and enhanced mask decoder that yields very high language understanding performance. These models could provide richer semantic signal for choosing words – e.g., they might help not just predict the next word but ensure the whole sentence structure is coherent. In practice, though, integrating a full BERT into the caption generation loop is cumbersome and doubles the computation (imagine at each time step running BERT on the sequence so far). A simpler use is: use BERT embeddings as **initialization** for the decoder’s word embeddings. For instance, one could take BERT’s learned 768-dimension embedding matrix (the input word embeddings) and use it to initialize our decoder’s embedding matrix, so that the model starts with some notion of semantic closeness (e.g., “dog” and “puppy” embeddings will be near each other because BERT trained on large corpus). This doesn’t give context, but it gives a better starting point than random. Then during training, these embeddings can further adapt to the caption domain.

* **GPT Embeddings**: If we use GPT-2 or GPT-like decoder, by default it has its own embeddings for subword tokens, learned from its pretraining. These are very good (they encode a lot of distributional info from billions of words). We likely wouldn’t need to swap them out. However, we might consider using **GPT’s hidden states as embeddings** for something like re-ranking or scoring captions. For example, one could generate a caption and then run a large language model (like GPT-J or GPT-3) to get a “perplexity” score indicating how fluent that sentence is. That could be an auxiliary metric to select between candidates. While not directly part of generation, it’s a possible use of language models (this touches on evaluation, section 7).

* **T5 Embeddings**: T5 also has its internal embeddings for text. If we integrate T5 as decoder, we automatically use those. T5 is trained with a vast vocabulary and even multi-task objectives, so its embeddings carry a lot of information (including some semantics and even some world knowledge). Like GPT-2, we likely trust the pre-trained embeddings and won’t alter them much except fine-tune.

* **CLIP Text Embeddings**: CLIP has a text encoder that produces embeddings aligned to images. One way to use it is as a **target** or reward: e.g., ensure that the caption we generate, when fed into CLIP’s text encoder, is close in embedding space to the CLIP image embedding (this is essentially CLIPScore maximization). This could be done as a re-ranking method or even as an additional loss during training. Another use is to initialize our decoder’s word embeddings or output layer with CLIP’s text embeddings. CLIP’s text model is a transformer that outputs a single vector for the whole text (and per-token embeddings internally). It has a vocab of 49408 BPE tokens (similar to GPT-2’s, actually). We could map our caption tokens to CLIP tokens and use CLIP’s embedder to get embeddings, feeding those to an LSTM decoder in place of learned embeddings. During training, CLIP’s text encoder could be frozen (so it’s providing a static embedding space), or fine-tuned. Given CLIP was trained on a huge corpus (image alt texts from the web), its language space might be beneficial: for instance, it might cluster synonyms or related concepts, helping the model generalize to words not frequently seen in COCO (like more fine-grained class names that are present in CLIP’s vocab). If using a transformer decoder, mixing CLIP embeddings is less straightforward (you’d basically be replacing the decoder entirely if you tried to use CLIP’s text transformer). A more synergistic approach is multi-modal training like **LiT (Locked-image Text tuning)** where image encoder is frozen and only text decoder is trained – but that’s more about classification. For captioning, we can treat CLIP’s text model as a source of **features** or constraints rather than as the primary decoder.

* **OpenAI’s BERT (Transformer-XL, etc.)**: There are other models like Transformer-XL or XLNet (permutation-based LM) – these are less commonly used now with the advent of GPT-2/3 and BART/T5. They have more complex behaviors (like Transformer-XL can handle long sequences via recurrence). For captions, sequence length is short, so those aren’t needed. A straightforward transformer decoder with positional embeddings suffices.

**Contextual Representation Quality:** Generally, encoder-style language models (BERT-type) provide excellent **semantic understanding** of text, while decoder-style models (GPT-type) provide excellent **generative fluency**. For captioning, our need is primarily generation, so leaning on a decoder (GPT/BART/T5) is wise. If we wanted to judge or rerank captions based on semantic similarity, BERT-based BERTScore or a CLIP text encoder could be used (we discuss metrics in section 7). RoBERTa and DeBERTa are improvements over BERT in that they train on more data and handle nuances (DeBERTa uses disentangled attention to better capture word order and relative importance). Using RoBERTa-large or DeBERTa-v3-large embeddings could slightly improve any embedding initialization scheme, but the difference might be marginal in the end caption quality compared to the bigger architectural changes we are making.

**Resource Requirements:** Including a large language model in the loop does increase computation:

* BERT-base has 110M params, BERT-large 340M. GPT-2 large has 774M. Running these per time step is costly. That’s why the integration must be carefully chosen (e.g., use GPT-2 as the decoder itself, rather than something like using BERT in addition to an LSTM).
* Memory: The cross-attention decoder approach (ViT + GPT-2) will roughly sum the memory needs of the encoder and decoder. With ViT-B and GPT-2, that might be around 170M + 117M = 287M params, which one 16GB GPU can handle in float16 for training with small batch. If we choose bigger (ViT-L + GPT-2 xl 1.5B) we might need multi-GPU or gradient checkpointing.
* Speed: Transformers are highly parallelizable on GPU, so generating a caption with a 12-layer transformer may actually be *faster* than doing it with an LSTM of similar size on a GPU, because the GPU can utilize its matrix cores fully. The self-attention for \~20 tokens is trivial. So runtime is not a big worry; the bigger cost is training time with these larger models, but again we leverage pretraining to converge faster.
* If we ever port to CPU or mobile, a huge LM is a problem – but we could distill or quantize it. There are distilled versions of BERT (DistilBERT, MiniLM) and even GPT-2 (someone trained a DistilGPT-2) that might offer 50-70% of the performance at \~ half the size. Those could be fallback options for lighter inference. We can also explore quantization (INT8 for transformer weights) which some libraries support, potentially running a 1.5B model on CPU with reasonable speed for single-sentence generation (but still, edge devices might need <100M parameter models for real-time).

**Licensing and Availability:**

* **BERT/RoBERTa**: BERT is Apache 2.0 licensed (Google released it openly). RoBERTa’s model weights are available via HuggingFace (the original RoBERTa paper/model is from Facebook AI; the weights are also basically open, and the code was in fairseq under MIT). No known restrictions; can be used commercially. DeBERTa has multiple versions – DeBERTa V3 (the latest) was released by Microsoft under an MIT license on HuggingFace (`microsoft/deberta-v3-base` etc.) which explicitly allows commercial use.
* **GPT-2**: OpenAI’s GPT-2 code and weights were released openly (the weights have an OpenAI License on their GitHub, but it allows usage, and the model is integrated in Transformers library under MIT). We should double-check if there’s any limitation – OpenAI’s license for GPT-2 is actually quite permissive; it’s not like the GPT-3 API. Many people use GPT-2 in products (for text generation assist, etc.).
* **T5**: Released by Google under Apache 2.0 (including model weights). This is very open for commercial use.
* **BART**: The code (fairseq) is MIT. The weights for BART large (via HuggingFace or fairseq) are also effectively public domain (they might inherit fairseq’s MIT or CC-BY, but nothing prohibits usage).
* **CLIP text**: As mentioned, OpenAI’s official CLIP weights are somewhat encumbered by their note, but the OpenCLIP reimplementation has text encoders too (they trained ViT-B/32, etc., text model from scratch on LAION). Those are under MIT. If we were to use CLIP’s text embeddings actively, we’d use OpenCLIP’s version to be safe.
* **Others**: If we consider very new models like GPT-3 or PaLM, those are closed or only accessible via API (not suitable for our integration, plus licensing/commercial use is not permitted for weights we can’t even obtain).

**Planned Approach:** Given all this, our likely path is to **use a pre-trained transformer decoder (GPT-2 or T5)**, thus inherently using its embeddings. We might not explicitly need BERT/RoBERTa in that case. However, we could still experiment with **BERT-assisted training**: one idea – use BERTScore or a BERT-based critiquer in the loss (as a kind of self-critical reward that measures similarity to references beyond n-grams, see training section). Another idea – after generating a caption, pass it through a BERT or T5 encoder to refine it (some two-pass generation where the second pass corrects errors, analogous to using a language model to polish). These are advanced ideas that we can explore once the core system is working.

In summary, **embedding-level upgrades** are largely subsumed by moving to a transformer decoder. The new decoder’s embedding matrix will have been trained on massive text data, giving us much better initial representations than the old random/GloVe embeddings with possibly some BERT spice. We will ensure any embedding or tokenizer we use covers the vocabulary we need (COCO is mostly common words, so GPT-2’s vocab is fine). We should also ensure consistency: if using GPT-2’s tokenizer, the references for evaluation might need to be tokenized similarly for BLEU, etc., but since BLEU/CIDEr operate at the word level typically, we’ll detokenize to compare to reference words. That’s manageable.

To avoid any confusion: all the pre-trained language models we integrate will be documented with their licenses. We’ll maintain a list: e.g., *“This product uses GPT-2 (c) OpenAI 2019, released under MIT license”*, etc., to satisfy license requirements. Open-source community models make life easier in this regard.

## 5. Multi-Modal Fusion Techniques

When combining vision and language features, beyond basic attention, there are specialized **fusion techniques** that can enhance how the model represents the image-text pair. We’ll look at several approaches that go beyond “just attend and concatenate”:

* **Co-Attention and Bilateral Fusion:** As touched on earlier, co-attention involves **two-way interactions** between image and text representations. In practice, a model like **LXMERT or ViLBERT** uses two parallel streams (one for vision, one for text) that meet through **cross-attention layers**. For example, ViLBERT (2019) had 12 layers where at each layer, image region features attend to textual features and text attends to image, and then they’re fed to the next layer. This yields deeply fused multi-modal representations, as opposed to a shallow fusion where, say, you attend to image once at decoder. In captioning, a similar idea can be implemented: one could have a *multimodal encoder* after the unimodal encoders. Our pipeline could be: Image -> CNN/Transformer encoder -> region features; Text -> (optional) word embeddings or partial RNN; then a **multimodal transformer** that takes both sets as input (perhaps with modality-specific tokens or some initialization) and produces joint representations used for decoding. However, since captioning is one-directional (we generate text from image), an alternative is to perform **K rounds of co-attention before decoding each word** – essentially an iterative refinement of attention. Research like **Stacked Attention Networks** (for VQA) did something analogous (answer attends image multiple times). In generation, this could be heavy to do per timestep, so it might be better to do co-attention **prior** to generation: e.g., generate a set of image-aware textual context vectors and then sequentially decode from them. The recently popular approach is **Transformer encoder-decoder with multimodal encoder pretraining** (like in ALBEF, BLIP, etc.), which we’ll discuss now.

* **ALBEF (Align BEfore Fuse)** – This method explicitly separates **alignment** from **fusion**. It uses two encoders: an image encoder and a text encoder (e.g. a ViT and a BERT). First, it trains them with an **Image-Text Contrastive (ITC) loss** to align their embedding spaces (so that an image embedding is close to its caption embedding). Then it employs a multimodal encoder (essentially a transformer that takes the image tokens and text tokens together) with **cross-modal attention** to fuse features for downstream tasks. In image captioning, ALBEF can be fine-tuned by adding a decoder head to generate text. The key innovation is that by the time fusion happens, the modalities are roughly in one space, making cross-attention easier and more effective. For our architecture, we can incorporate this idea by using a contrastive pretraining step (with say CLIP’s approach) and then initializing our decoder or multimodal layers with that alignment in mind. If not pretraining from scratch, we could also take a pretrained ALBEF model (they released checkpoints) and modify it for captioning. But building from scratch, the concept is: **use contrastive objectives to tie vision and language before/while training the captioning model**. This way, the decoder doesn’t have to learn from scratch that, e.g., the image region that looks like a dog corresponds to the word “dog” – the encoders already embed “dog” text and dog image closely. ALBEF also uses **momentum distillation** to deal with noisy data, which might be beyond our scope unless we pretrain on uncurated web data. Importantly, ALBEF is an open model (NeurIPS 2021), and its code is MIT licensed. Pretrained weights (on COCO and Visual Genome) are available. We could leverage those weights to bootstrap our system or at least evaluate their performance vs. our approach.

* **BLIP (Bootstrapping Language-Image Pretraining)** – BLIP (Salesforce, 2022) extends these ideas and introduces a **Mixture of Decoder** architecture and a two-stage training: first a captioning model that can generate pseudo-captions for image-text pairs (to **bootstrap** learning from noisy data), then use those pseudo-captions for further pretraining. BLIP’s architecture includes:

  * A **Unimodal image encoder** (often a ViT, possibly pretrained like CLIP’s ViT).
  * An **LM**: BLIP-1 had a *Vision-language Transformer* that could act as both an encoder and a decoder (they call it a **Mixture of Encoder-Decoder, MED**). BLIP-2 (2023) takes a different route: it uses frozen pre-trained encoders (image and text) and introduces a small learned **Q-Former** (query transformer) to connect them. In BLIP-2, the image features from a frozen CLIP ViT are fed into the Q-Former which has a set of learnable “query” vectors that attend to those features. The output of Q-Former (e.g. 32 query embeddings) then serve as a prompt to a frozen language model (like a pre-trained Flan-T5 XL), via cross-attention. This way, BLIP-2 achieves strong results with much fewer trainable parameters.

  For our system, BLIP-2 offers a blueprint for **modularity**: we could plug in a *frozen* strong image encoder (like CLIP ViT-G) and a *frozen* strong text model (like a T5 or GPT-J), and just learn the intermediate transformer that bridges them. The intermediate is relatively small (\~< 100M parameters). This approach could yield a **compute-efficient training** and leverage very powerful models without fine-tuning them (which is great if we have limited training data or limited ability to modify those models). The downside is that frozen models may not adapt to our specific style (COCO captions are a bit different than web text; a frozen LM might produce more flowery language). But human evaluation often favors using such models because they are more fluent.

  Concretely, to attempt something BLIP-2-like: we could freeze our chosen vision encoder (say CLIP ViT-L/14) and freeze a language model (say GPT-2 or T5), then insert a small Transformer that takes the image encoder’s output and produces a fixed number of “queries” which are fed as key/value to the language model’s cross-attention. We would train only that small Transformer (and perhaps a linear projection to match dimensions). This drastically reduces training cost – one could even train it on COCO from scratch in a short time. The resulting model’s performance might approach what fully fine-tuning a larger model would do, with the advantage that if the backbone and LM are high-quality, the output is high-quality.

  **Open source:** BLIP’s code (from Salesforce’s LAVIS library) is under a BSD license and they provide pretrained weights for BLIP-1 and BLIP-2. We can get inspiration or even use their components directly (the LAVIS library allows mixing and matching pre-trained components for tasks like captioning, under a unified interface). This might accelerate development – for instance, to test a ViT+T5 strategy, we could use BLIP-2’s ViT-G/FlanT5-XL model (noting FlanT5-XL weights are open from Google). The only catch is these large models (ViT-G, T5-XL) are very heavy – we might test smaller BLIP (like BLIP-2 with ViT-B and T5-base) to gauge.

* **Gated Fusion** – Some earlier captioning works introduced **gating mechanisms** to fuse multimodal features. One example is the *Mixed Fusion* technique where the image feature vector and the textual hidden state are combined via a gating scalar or vector. A simple instance: \$g = \sigma(W\_v v + W\_h h)\$, and then output state \$ = g \* \tanh(W\_o \[v;h])\$. This allows the model to **decide how much to rely on vision vs language** at each step. We already discussed one such gating (Adaptive Attention’s “visual sentinel”). Another is in the *Meshed-Memory Transformer* (Cornia et al. 2020) where they had gating at multiple decoder layers to combine multi-level image features. We can incorporate gating in our decoder’s cross-attention: e.g., after computing the attended image context, have a gate that multiplies it elementwise, or have a gate on the entire cross-attention block output vs the self-attention output. This can sometimes stabilize training (prevent the decoder from over-relying on either modality). It can also be used to inject other modalities (if we had attributes or audio, we could gate them similarly).

  Another form of gating is **FiLM (Feature-wise Linear Modulation)** from Perez et al. (used in VQA), where the text generates coefficients that modulate the image features (e.g., a learned affine transform on visual features conditioned on text). For captioning, one could imagine a reverse FiLM: use image to modulate certain neurons in the language model. This is speculative, but it’s a flexible fusion technique.

* **ALBEF/BLIP style Fusion vs. Classic:** In a classic encoder-decoder, fusion is implicit in the attention at each decoding step. In ALBEF/BLIP, fusion happens in a **dedicated joint transformer** before or during decoding. The advantage of the latter is that you can pretrain that joint space on tasks like image-text matching or masked language modeling with image, which might not be straightforward with a plain decoder. For our architecture, we have to decide: do we want a single integrated model (like a one-stage transformer that input image and directly outputs text) or a two-stage (image encoder + separate decoder)? The two-stage with cross-attention is easier to implement with existing libraries. The one-stage (where you prepend image tokens to text tokens and just do self-attention on all of them) is actually what *SimVLM* did (treat image patches as “words” and train a language model). That approach did extremely well on captioning (SimVLM’s large model reached SOTA on COCO without using CIDEr optimization, scoring \~143 CIDEr). One-stage has elegance and uses a unified transformer, but it’s less modular (you can’t easily swap out the language part without retraining the whole thing). Given we want modularity, we’ll stick to a two-stage design (distinct encoder and decoder components).

* **CoCa (Contrastive Captioner)** – CoCa (Google, 2022) is a model that jointly trained a dual-purpose head: one for contrastive image-text alignment and one for generative captioning. Architecturally, it’s an encoder-decoder transformer (image in, text out) but also produces a CLIP-like embedding for the image that can be matched with text embeddings. During training, they optimize both the contrastive loss and captioning loss. This multi-task training led to SOTA results on many benchmarks (including zero-shot captioning and retrieval). The CoCa model uses a large ViT (they mention ViT-G/14) and a large decoder, plus uses a huge training set (JFT-3B and ALIGN data). While we probably cannot replicate CoCa’s training, we can embrace the concept of **multi-task losses**: for example, while training our caption model, also train it to produce a good multimodal embedding (maybe via an auxiliary classifier that tries to distinguish matching vs non-matching image-caption pairs). This could improve the learned representations. It’s like having a bit of CLIP inside the captioner. Implementing that would entail adding a projection layer on top of the image encoder and one on text (e.g., take the decoder’s BOS token output as a summary) and adding a contrastive loss term. It’s extra complexity but not too hard with modern frameworks. We might consider it if pure caption supervision seems insufficient.

* **Code-level Fusion Example:** To make this concrete, suppose we want to implement a **fused multimodal encoder** before decoding. We could do:

  ```python
  # Pseudocode for a simple multimodal fusion module (1 layer)
  # image_feats: [N_regions, D], text_feats: [T_tokens, D] (e.g., text feats could be embedding or an LSTM output if we have partial caption context)
  combined = concat([image_feats + img_pos_embeds, text_feats + text_pos_embeds], dim=0)
  # Mask so that text tokens cannot attend to future text tokens (if decoding) and image tokens perhaps attend among themselves freely.
  attn_out = MultiHeadSelfAttention(combined, combined, combined, mask=attn_mask)
  combined = Norm(combined + attn_out)
  mlp_out = FeedForward(combined)
  combined = Norm(combined + mlp_out)
  # Then extract back out the sequences
  image_feats_fused = combined[:N_regions]
  text_feats_fused  = combined[N_regions:]
  ```

  This is a very basic joint transformer layer. If we stack N such layers, we get something like a unified transformer. Depending on mask, we can allow full image<->text attention and text self-attention only leftward. This essentially turns our system into a unified Transformer that can both encode image and decode text. Such an architecture might be overkill for now, but it’s conceptually how advanced models treat fusion.

* **Visualization Strategies for Fusion:** With complex fusion, debugging where the model gets information becomes non-trivial. We will incorporate **attention visualization tools**. For example:

  * For a given generated caption, we can extract the cross-attention matrices from the decoder for each word. We can then overlay those attention weights on the image (heatmaps or bounding boxes) to see what the decoder focused on at that word. Many caption papers include such figures to qualitatively show alignment (e.g., showing attention on the “giraffe” region when the word “giraffe” is output).
  * If using co-attention or a multimodal encoder, we can visualize **which image regions and words are strongly connected**. One could pick an image region and see the attention distribution over words (to see which words it influenced most), or vice versa. Since multi-head attention has many heads, a common technique is to average or select a few heads that correspond to intuitive alignments. Some attention heads might attend broadly (not interpretable), but often at least one head in cross-modal layers learns near one-to-one alignments (like an implicit grounding of words to image parts).
  * Another visualization approach is **Grad-CAM** adapted for image+text: for a given output word or for the whole caption, compute gradients of some score w\.rt. the image feature map to see what image pixels affect it. This could highlight regions the model is sensitive to when generating the caption.
  * **Embedding space visualization:** If we train a contrastive embedding (like CoCa or CLIP style), we can also project image and caption embeddings in 2D (via PCA/TSNE) to see if indeed matching pairs cluster, etc. This is more for analysis than operation.

  Ultimately, these visualization methods will help ensure our fusion is working as intended (e.g., if we see the model attending to the wrong things for words, we can adjust training or architecture).

**Modularity for Fusion:** We should design our code such that fusion method is abstracted. For example, we might have:

* An `Encoder` module (could be pure vision encoder or a multimodal encoder that processes text as well).
* A `Decoder` module (maybe expecting that the Encoder already fused info, or maybe it still does cross-attn).
* We could have a configuration switch: *fusion\_type* = `cross_attention` (meaning a classic encoder-decoder) vs `co_attention` (some bilateral layers) vs `none` (meaning no explicit fusion, e.g., if we did one-stage).

Considering our chosen approach, we’ll likely stick to the straightforward **cross-modal attention in the decoder** (which already fuses as needed) for now, and possibly explore **co-attention pre-fusion** if time permits or if needed for performance. The nice thing is many of the latest methods (ALBEF, BLIP) come mostly into play during pretraining on huge data. For fine-tuning on COCO, a simpler model with a good backbone and decoder often suffices to reach high scores, especially if we incorporate some form of pretraining or initialization.

**Licensing for Fusion Models:** Most of the mentioned fusion models (ALBEF, BLIP, etc.) have code and papers publicly available:

* ALBEF’s code is on GitHub (MIT License). Their pretrained models (trained on Conceptual Captions, COCO, etc.) are also downloadable. We can possibly use their pretrained ViT and multimodal transformer as initialization (though their text encoder was BERT-like and they didn’t include a caption decoder in pretraining – they did MLM and ITM primarily).
* BLIP is part of Salesforce LAVIS which is BSD-3. Very open.
* CoCa was Google internal, but the idea is out – if we replicate it ourselves, nothing restricts that.
* If we directly used something like BLIP-2’s pretrained combination (which includes CLIP ViT-L and a Flan-T5 XL, plus their Q-former), we need to check each component: CLIP ViT-L (OpenCLIP variant) is MIT, Flan-T5 XL is Apache-2.0, and the Q-former weights presumably fall under Salesforce’s BSD license. So that composite should be fine to use with attribution. The only caution is Flan-T5’s training data included some Google data that might have usage terms, but since it’s released under Apache, it’s generally clear for use.

In conclusion, **multi-modal fusion techniques** will be a significant part of maximizing our model’s performance. We lean towards using **cross-attention (decoder attends to encoder)** as the primary fusion, enhanced by possibly:

* Pre-aligning the feature spaces (via contrastive loss or by using CLIP features).
* Using gating mechanisms to modulate fusion.
* Optionally adding a co-attentional transformer encoder for an extra fusion step (especially if using object regions, a co-attentional encoder can refine object representations in context of likely caption words).

Our system’s modularity will allow experimentation: e.g., we can run ablations where we turn off the contrastive alignment loss to see its effect, or replace the image features from CNN vs CLIP to see how it affects fusion. The end goal is a model that deeply understands the image and can express it in natural language, which these fusion methods are all about facilitating.

## 6. Training Methodologies

Improving the architecture is one side of the coin; **how we train** the model is equally important for achieving high caption quality. We explore training strategies beyond standard supervised learning:

* **Curriculum Learning:** Rather than shuffling randomly through training data, curriculum learning presents samples in a *meaningful order*, typically from easiest to hardest. For image captioning, “easy” could mean images with very salient, singular objects and clear captions, whereas “hard” might be crowded scenes or requiring fine-grained distinctions. Some works have designed curricula based on properties like caption length, word frequency, or even model confidence. Zhang et al. (2022) propose a **cross-modal similarity** metric to define difficulty: images whose captions are *atypical* for those images (low similarity to what a vision-language model would predict) are considered hard. They use a pretrained model to estimate how aligned an image and caption are; high alignment means the caption is straightforward (easy), low means it’s a quirky or complex description (hard). We can implement a curriculum by first training on a subset of data that is easier. For example:

  * Phase 1: train on captions that have one object and short length (we can filter MS-COCO for captions under 8 words, containing one noun).
  * Phase 2: add in more normal data.
  * Phase 3: include the hardest cases (very long captions or those with multiple clauses).

  Another approach: start training with a high image-text similarity threshold (ensuring model sees well-matched pairs first). Over epochs, gradually include lower similarity pairs (which might contain rare or oddly phrased captions).

  **Benefit:** Curriculum can speed up convergence and improve generalization. The model builds confidence on simple descriptions before attempting to master complex ones, somewhat akin to how humans learn. It may also avoid getting stuck in bad local minima that can occur if the model is overwhelmed early with very hard examples (which it may just learn to predict something safe for).

  We must ensure that by the end, the model has seen the full data distribution; curriculum is about order, not about throwing away data. We also need an automatic way to rank difficulty. Using an existing model (like CLIP or a captioning model) to score image-caption pairs is one solution. We could use CLIP: compute CLIP similarity for each training pair and bucket them. Or use an **object count** heuristic: images with more distinct objects are harder. Or use caption perplexity under a language model: if a caption is linguistically complex/uncommon, maybe it's harder.

  Curriculum learning was shown to give higher performance in some setups without extra data. We should be careful that curriculum doesn’t inadvertently bias the model to simplistic captions – so we might increase difficulty fairly quickly so the model trains fully on all data eventually.

* **Reinforcement Learning with Human Feedback (RLHF):** Inspired by how ChatGPT was refined, RLHF in captioning involves using *human judgments* as a reward signal. Two known attempts:

  1. **Off-line Human Feedback (Seo et al. 2020):** They collected human ratings for a bunch of image-caption pairs (maybe rating how good the caption is) and then used those ratings to directly optimize the caption generator with policy gradients. Specifically, they had a base model, asked humans to score some of its outputs, then treated those scores as rewards – using an off-policy RL algorithm to improve the model to get higher rewards. They reported that even a small amount of human feedback (a few thousand rated captions) generalized to better performance on unseen images, as judged by separate human evaluators. This shows that human feedback can help capture aspects of caption quality that metrics miss (like factual correctness, tone, etc.).
  2. **Integrating RLHF with Supervised Learning (Adarsh et al. 2024):** A recent study combined standard cross-entropy training with RLHF, introducing a custom loss that accounts for human preference rankings. They likely used a setup where humans (or a proxy model) compare two captions and prefer one, and then use that to derive a reward (similar to how InstructGPT was trained, using a reward model).

  For our project, implementing RLHF fully would require obtaining human feedback, which might be outside the scope unless we use existing datasets (some previous COCO evaluation campaigns have human scores that could be repurposed). Alternatively, we can simulate “human-like” feedback with a heuristic or another model’s judgment (not as good as real human, but easier). For example, use a **learned reward model**: train a small model to predict a human score given (image, caption) using whatever data we have (could even use the reference captions as pseudo positive examples vs. some negative captions as bad examples). Then use that model’s output as a reward in RL. This approaches the idea of *learning a reward function* for captions that correlates with human preferences.

  RLHF’s promise is to optimize what we actually care about (human satisfaction with the caption) rather than proxies like CIDEr. It can target things like avoiding offensive or biased language as well, if feedback includes that. In an interactive setting, one could even refine the model over time from user feedback (e.g., thumbs up/down on captions).

  Implementation-wise, RL training for captioning often uses the REINFORCE algorithm or a variance-reduced version (self-critical is a special case). We’d sample captions from our model (which is our policy), get a reward (e.g., human score or a learned model’s score), then adjust the model to increase probability of high-reward captions. We have to be careful about reward design: if humans highly reward very flowery but possibly less precise captions, the model might start deviating from factual description. It’s important to balance factual accuracy and human preference (which hopefully align for the most part – humans prefer accurate captions).

  Practically, we might not have immediate access to human raters, so as a proxy, we could incorporate **CIDEr optimization and perhaps CLIPScore optimization** as a kind of pseudo-RLHF (since CLIPScore correlates with human judgment of relevance, and CIDEr with consensus). The combination of optimizing both could be interesting (some works tried this: adding CLIPScore into the reward with CIDEr) – albeit one study found only modest gains. If time permits, we could even use ourselves or colleagues to rate some captions and see if fine-tuning on that improves results qualitatively.

* **Multi-Task and Auxiliary Task Training:** Multi-task learning can help the model develop better **internal representations** that generalize. Several multi-task strategies are relevant:

  * **Captioning + Classification:** As in the paper MLAIC (IJCAI 2018), one can train the CNN encoder simultaneously to do object classification on the image. For instance, alongside generating a caption, the model must predict a set of object labels present (like a multi-label classification of MS-COCO categories). This encourages the encoder to really recognize all objects, not just the ones mentioned in captions (captions sometimes skip objects). The classification loss provides a stronger training signal for visual feature extraction. In MLAIC, they found this helped learn an “object-rich encoder” and improved the model’s ability to ground contextual info. We can implement this by adding an auxiliary output from the encoder: e.g., pass the image features through a sigmoid classifier for the 80 COCO object classes (or even 1000 ImageNet classes if we want more generality). We’d need image labels; COCO images have category labels, although not exhaustively for all objects (only those that were annotated). But the caption dataset doesn’t directly come with a full label set per image. We could use the captions themselves to derive a pseudo label set (extract nouns). Or use an external model to detect objects in each image to supply labels. Alternatively, train on Visual Genome region descriptions to get more labels. This is a bit involved but feasible.
  * **Captioning + Detection/Localization:** Instead of classification, one could train a portion of the model to do object detection or grounding. E.g., use the Faster R-CNN head on the encoder’s features (if using a CNN backbone) to detect objects, sharing features with captioning. This is heavier but would align with recent trends of **unified models** (like OFA or Unified-IO that do multiple vision-language tasks). A simpler variant: use a pre-trained detector to provide attention supervision (ensuring attention sometimes focuses on ground-truth bounding boxes when generating certain words). We have to be careful not to over-constrain free-form captioning with detection labels, but it can be helpful.
  * **Syntactic or Semantic Annotation Task:** MLAIC also had a “syntax generation” task where the decoder was co-trained to output a sequence of Part-of-Speech tags or some syntax annotation for the caption. The idea was to regularize the language model with knowledge of syntax, thus reducing ungrammatical outputs. We could similarly attach an auxiliary head to the decoder that at each step predicts, say, the part of speech of the word it just produced, or whether a word is part of a noun phrase, etc. If we have a parser to generate those labels for training captions, we can supervise that. This might help especially if we had small data, but given we’ll likely leverage large pre-trained LMs, syntax might already be well-handled. Still, it’s an interesting regularizer.
  * **Other Vision-Language Tasks:** We could multi-task on something like **image-text retrieval** (learn to encode images and captions into a common space as well). For example, simultaneously train the model to produce a good caption *and* to produce an image-text matching score for the right caption vs a wrong caption. This is similar to what we described for CoCa’s contrastive objective. It can improve the visual semantic alignment. Another task: **Visual Question Answering (VQA)** – though a different output format, if we had a unified model, we could feed question+image and have it answer, sharing encoder with captioning. This would require a multi-task dataset. If we confine to MS-COCO domain, the VQA v2 dataset uses COCO images for Q\&A. Joint training on VQA and captioning has been attempted and can help (because it forces the model to focus on different details – captions give general description, VQA drills into specific properties).

  Multi-task training generally helps to **regularize** (prevent overfitting to one form of description) and can improve representation learning so that the model knows more about the world (through classification labels or answering questions). The drawback is it complicates training (need to balance losses) and requires additional data/annotations. We can make many tasks from COCO itself (like the classification one). If we use external data (like ImageNet labels, or Visual Genome region captions), we have to ensure license compatibility – COCO and Visual Genome are both open for research (Creative Commons), so that should be fine in an academic sense. For a product, using those to train is also generally fine as they are widely used in industry for model training (one just shouldn’t redistribute images without permission, but model weights derived are okay).

  We’ll likely at least incorporate **label prediction as an auxiliary task**, since COCO has 80 object classes labeled on a subset of instances. This could be done via a simple binary cross-entropy for each class presence. We might also incorporate a **pretraining on caption plus retrieval** like CLIP: e.g., train the image encoder and an auxiliary text encoder (maybe the decoder’s embedding averaged) to predict if a caption is true for an image or not (ITM task). This is relatively easy: create negatives by shuffling captions among images. Many VLP models do this (ALBEF, etc.). That encourages the image encoder to produce discriminative features and the text decoder to produce distinctive language that matches the image.

* **Self-Supervised Pretraining:** Before fine-tuning on MS-COCO’s 120k images (which is not very large by modern standards), we can do pretraining on **unlabeled or weakly labeled data**:

  * **Vision-Language Pretraining on Web Data:** Datasets like *Conceptual Captions (CC3M & CC12M)*, *SBU Captions*, *LAION*, *Google’s ALIGN (JFT-3B + alt text)* provide millions of image-text pairs. These are noisy – not every caption is a perfect description – but models like CLIP, ALBEF, SimVLM have shown great success using them. We could take a two-phase approach: first train our model (or just the encoder+decoder without attention? But attention is integral) on a large corpus of image-text pairs with a generic objective. Some common objectives:

    * **Masked Language Modeling (MLM):** Hide some words in the caption and train the model to predict them, given the image and the rest of the caption. This is like how BERT is trained but multi-modal – forces the model to use the image to fill in blanks (e.g., caption: "a <mask> sitting on a branch" with an image of a bird – model should infer “bird”). This was used in models like UNITER, Oscar, VinVL.
    * **Image-Text Matching (ITM):** Classify whether a given image and caption correspond or not. Teaches cross-modal alignment generally. ALBEF and others often do contrastive learning plus an ITM head.
    * **Contrastive Learning (CLIP-style):** Train the image encoder and text encoder (or the decoder’s initial hidden state) such that correct pairs have higher similarity than incorrect pairs. This yields a common embedding space. We could apply a contrastive loss where the decoder’s CLS token embedding (if we use one, or perhaps an average of word embeddings) must be close to the image embedding.
    * **Caption Generation on Big Data:** We can actually use our model to generate captions on these big datasets (like Conceptual 12M) to pretrain, even without strong guarantees of quality. *SimVLM* did a form of this – they had a “Prefix Language Modeling” objective which essentially *is* captioning: feed the image (as patch tokens) then predict the caption text tokens in a language-model fashion. They trained on 1.8B image-text pairs, which gave them an extremely strong model when fine-tuned on COCO (they achieved \~143 CIDEr without CIDEr optimization). We obviously can’t train on 1.8B pairs from scratch easily, but we could take a subset of e.g. CC12M and run maybe a few epochs of image-to-text generation training. Even a million extra pairs might help the model not to overfit to COCO phrasing and to gain knowledge of more visual concepts. *OFA* (one for all model by Wang et al. 2022) similarly used multi-task training on a mix of datasets including CC and COCO to get a very powerful model.

  If we have time and resources, pretraining is one of the best ways to improve caption performance. Since our architecture is modular, we could even initialize the vision encoder from CLIP (which is essentially pretraining) and the decoder from GPT-2 or T5 (which is pretraining on text). This already covers a lot of ground without explicit multi-modal pretraining. But there is still a gap – the connection between the modalities, which CLIP partly covers via its alignment. We might want to further do a brief joint pretraining: for instance, freeze those large parts and only train the cross-attention and maybe top layers on a big dataset, to tune them to work together. If using an approach like BLIP-2, the Q-former would be trained on big data, which they did. If we use their Q-former weights, that’s effectively using their pretraining.

  * **Self-Supervised on the same data:** If we don’t want external data, we can still do self-supervised tasks on COCO itself or augment COCO. For example, **back-translation**: generate captions in another language using a translator, then translate back to English to get paraphrased captions (to augment training with more varied phrasing). Or simply use dropout noise as a regularizer (like hide parts of image during training to force model to guess from context).
  * **Curriculum (again):** One could treat pretraining on large simple data as the ultimate curriculum for fine-tuning on the target data.

  Given time constraints, we’ll probably leverage existing open pretrained models (CLIP, GPT, etc.) rather than doing a huge pretraining ourselves. But we can simulate small-scale pretraining: e.g., first train the model on COCO+VisualGenome captions combined (VG has region-level captions that could help detail description) for a few epochs, then fine-tune on COCO official for final tuning.

* **Loss Functions and Optimization:**

  * **Cross-Entropy (XE) Loss:** This is the standard teacher-forcing loss where we maximize the likelihood of the ground truth caption tokens. This gets the model to produce plausible captions, but not necessarily *optimal* ones for metrics because of exposure bias (at test time it might generate sequences not seen during teacher forcing). It also tends to produce safe, average captions because maximizing likelihood of references with limited diversity can converge to a generic style.
  * **CIDEr Optimization (Self-Critical Sequence Training - SCST):** In SCST (Rennie et al. 2017), after pretraining with XE, you switch to using the model itself to sample captions and then treat the metric (CIDEr) as a reward. The gradient is computed such that if a sampled caption has higher CIDEr than a baseline (typically the model’s own greedy output), the likelihood of those sampled words is increased; if it’s lower, decreased. This directly optimizes the expected CIDEr score of the model’s output. SCST led to significant metric improvements (CIDEr especially) and became a standard step for competition models. We likely will implement this: after initial training, perform SCST for a certain number of iterations. We will use CIDEr-D (the COCO leaderboard version of CIDEr) as the reward, possibly mixed with another metric to avoid overfitting to n-grams. Pure CIDEr optimization sometimes leads to overly verbose or repetitive captions that hack the metric. However, many top models do it and mitigate issues by early stopping or mixing with XE (some use a weighted sum of XE and RL losses to keep the language natural).
  * **SPICE or Diversity Reward:** SPICE measures semantic content (via scene graphs), but optimizing it directly is hard because it’s not differentiable. RL can optimize it but SPICE is slow to compute per sample. Alternatively, we can include a reward for using rare words or penalize repeating words, to foster diversity. Some research has e.g. added an **inverse word frequency reward** to encourage using less frequent words (thus more descriptive ones) – this must be balanced so as not to produce nonsense. We can design a simple term: for each word the model outputs that is in the reference, give a small reward weighted by (inverse corpus frequency). Or penalize if the caption is too similar to the training captions distribution (to avoid cookie-cutter outputs). These are more experimental, but since the question explicitly mentions *linguistic diversity*, we should consider these. A safer approach: generate multiple captions with stochastic decoding and then present diverse ones (we’ll cover in inference), rather than trying to make each single caption artificially diverse at risk of error.
  * **BERTScore/CLIPScore as Reward:** These metrics measure similarity in embedding space. If we optimize them, we potentially get captions that are semantically on-point even if wording differs from references. A recent paper found combining CLIPScore with CIDEr in RL did not hugely improve over CIDEr alone – possibly because CIDEr already ensures correctness for typical cases, and CLIPScore might just encourage more generic but image-relevant statements. Still, it’s worth testing. We could set reward = CIDEr + λ \* CLIPScore, tuning λ. CLIPScore has the advantage of not requiring reference captions at all (it checks image vs output similarity), so optimizing it might yield captions that describe details the original references missed (which is good for real-world use, but might not raise CIDEr if those details aren’t in references).
  * **Negative Rewards:** We can assign negative reward for things we want to avoid. For instance, if a caption uses the word “man” when the person is actually female (though our training set may not label that – tricky unless we have an oracle), or if it says something implausible (“a sunny night”). Some people have used a language prior to penalize ungrammatical output or repeated phrases. We can incorporate a simple penalty for repeating the same trigram, which is a common problem in language generation. E.g., if caption contains a duplicate trigram, subtract some points. This can be part of RL or even as a constrained decoding approach.
  * **Human Preference Modeling:** If we had a small set of human-ranked outputs, we could train a *reward model* to predict the ranking and use that as our reward function. That’s essentially the OpenAI RLHF approach. Without a dataset, we might skip this, but if the client/user has an idea of what style they prefer (say more creative vs. more factual), we could incorporate that by designing a reward accordingly (like rewarding longer sentences with more adjectives for creativity, or shorter factual sentences for precision).

* **Training Efficiency with PyTorch 2.x:** On the engineering side, PyTorch 2’s `torch.compile` and distributed training can accelerate our training runs. We plan to use **mixed precision (FP16)** to handle large models faster (via `torch.cuda.amp.autocast`). Given mid-to-high GPUs, we might use multiple GPUs for training larger models (Data Parallel or Distributed Data Parallel). We will ensure the training code is deterministic where possible (set seeds, etc.) for reproducibility, especially since RL introduces some randomness.

* **Monitoring and Evaluation during Training:** We’ll not only monitor loss, but also periodically evaluate generated captions on validation set with metrics (BLEU, CIDEr, etc.) and possibly some diversity metrics (like distinct n-grams). This way we can see if, for example, CIDEr goes up but diversity goes down, etc. Early stopping criteria might be CIDEr or a composite metric on val set.

In summary, our training recipe might look like:

1. **Phase 0 (Optional Pretraining):** Initialize encoders/decoders from pretrained weights (CLIP, GPT, etc.). Optionally run a few epochs on a broader dataset (e.g., Conceptual Captions) with cross-entropy to prime the model.
2. **Phase 1 (Supervised Learning on COCO):** Train with cross-entropy on COCO train set (possibly with curriculum – e.g., first epoch focus on easier captions).
3. **Phase 2 (Metric Optimization):** Fine-tune with RL (SCST) to directly improve CIDEr (and perhaps SPICE/CLIPScore). Use a mix of greedy and sampled outputs to compute self-critical gradient.
4. **Phase 3 (Finetune for Diversity/Fluency if needed):** If after RL the captions become a bit repetitive or weird, we might fine-tune a bit with a weighted sum of cross-entropy (to keep it fluent) and RL (to keep metrics) – some works do a few iterations of this to balance.
5. **Phase 4 (Multi-task or Auxiliary loss fine-tuning):** We could either incorporate multi-task from the start or fine-tune the model with an auxiliary loss (like classification) for a few epochs to embed that knowledge, then maybe another round of fine-tuning on captioning. In practice, often multi-task is done concurrently to avoid too many phases.

Each phase we have to be cautious about overfitting; COCO has 120k train, 5k val, so not tiny but not huge either. Using heavy regularization (dropouts in transformer, data augmentation like random resize/crop) can help. We can also augment data by using the 5 reference captions in training as separate training examples (which is standard). Some approaches even augment with synonyms (e.g., replace some words with synonyms or use slight paraphrases to augment training – possibly using back-translation or a paraphrasing model). If needed, we can try a simple augmentation: for each caption, maybe randomly swap two adjectives or something to create a variant (must be careful not to create nonsense though).

**Licensing note:** The training methodologies themselves don’t carry license concerns, but the data does:

* MS-COCO is under a Creative Commons Attribution license (for images), and the captions are from crowd workers (should be fine to use; they’re often treated as public domain in academic usage). For commercial use, one should attribute the COCO dataset if distributing anything derived, but using the model commercially is generally accepted (COCO is widely used in industry for training).
* Conceptual Captions (Google) is a dataset derived from alt-text on the web with certain cleaning – it’s under a CC BY 4.0 license as well (free to use with attribution). Using it to train a model doesn’t require attribution on the product, but acknowledging in documentation is good.
* Visual Genome is under CC0 for region descriptions (basically public domain as far as I know).
* Any time we train on data scraped from the web (LAION, etc.), there’s potential for problematic content (we should be mindful of model possibly picking up biases or harmful language). Part of training methodology might include **filtering** or moderating – for example, not generating certain terms. That’s beyond the scope of typical caption project, but RLHF could be used to mitigate harmful outputs if needed by penalizing them. Since COCO is pretty benign (no offensive content generally), our model should be fine in that regard. But if using large web data, we might inadvertently incorporate bad language – we’d then have to post-process or filter outputs (we can integrate a list of banned words that if the model outputs, we replace with something).

With these strategies, we aim to achieve a model that not only scores well on metrics but also *feels* qualitatively better – describing images more like a human (with correct details and varied expression). The multi-step training might be computationally heavy, but given our environment we can schedule this properly and use techniques like gradient accumulation to fit large models on available GPUs.

## 7. Evaluation Metrics and Benchmarking

Traditional metrics like BLEU (which measures n-gram overlap with references) have been used for image captioning, but they are known to be imperfect proxies for quality. We will incorporate a broader set of **evaluation metrics** to get a well-rounded view:

* **BLEU (Bilingual Evaluation Understudy):** Historically adopted from machine translation, BLEU counts matching n-grams between the generated caption and one or more reference captions, with a brevity penalty. For COCO, typically BLEU-4 (up to 4-grams) is reported. BLEU is quick to compute and was used in older papers, but it **has limitations** for captioning: it rewards precision of overlap but not recall, and it doesn’t consider synonyms or meaning. Often a correct caption can get low BLEU if wording differs from references. We’ll still compute BLEU-1,2,3,4 for completeness (as many papers list them), but we won’t over-optimize for BLEU.

* **METEOR (Metric for Evaluation of Translation with Explicit ORdering):** METEOR scores are based on unigram matches between output and reference, but unlike BLEU, METEOR uses stemming and synonym matching, and combines precision and recall with more weight on recall. It also includes a penalty for word order differences. METEOR tends to correlate better with human judgments than BLEU on captioning. It ranges 0-1 (or 0-100 as a percentage). A good COCO model might have METEOR in the high 20s. We will compute METEOR; it helps capture if the model is using synonyms or paraphrases (because it gives partial credit for synonyms via WordNet). For example, if reference says “kid” and our caption says “child”, BLEU would score 0 on that word, METEOR would recognize the match. METEOR’s license is not an issue (it’s just a metric, code is usually freely available).

* **ROUGE-L:** While not explicitly asked, sometimes ROUGE (from summarization) is reported. It’s recall-focused (longest common subsequence). COCO evaluations sometimes include ROUGE-L. We can include it for completeness, though it often tracks similar trends as METEOR.

* **CIDEr (Consensus-based Image Description Evaluation):** CIDEr became popular for COCO since it was designed for captions. It computes TF-IDF weighted n-gram similarity between candidate and the set of reference captions. By weighting rarer n-grams more, it emphasizes capturing specific details that are not common in all images. It also averages over references to reward consensus: if an n-gram appears in many references, matching it is considered important. CIDEr is typically reported as CIDEr-D (which applies some length clipping to avoid gaming by extremely verbose outputs). On COCO, human performance is around 85-100 CIDEr (depending on interpretation), and state-of-the-art models approach 130-150 CIDEr (they often exceed average human reference due to effectively averaging style of multiple references). We will definitely use CIDEr, as it’s a primary metric for improvement and well-correlated with human judgment on factuality and adequacy. Our RL fine-tuning specifically will target this. If our model is good, we expect CIDEr to jump significantly after SCST optimization (often by \~10-20 points).

* **SPICE (Semantic Propositional Image Caption Evaluation):** SPICE (Anderson et al. 2016) takes a very different approach: it parses reference and candidate captions into **scene graphs** (objects and relationships) and computes an F-score on the graph tuples (like (object), (attribute, object), (object1, relation, object2)). This directly measures if the caption covers the salient objects and their attributes/relations similarly to references. SPICE correlates well with human judgments on content selection (especially for identifying if an object mentioned is correct). It doesn’t care about fluency or exact phrasing. We will use SPICE – though it’s slower to compute (because of running a parser and matching). For development, we might not run SPICE on every epoch (to save time) but definitely on final results. One thing: SPICE can highlight if our model misses an object that references mention, or hallucinates something. If we find SPICE is lagging behind others, it might indicate issues in content selection. Models that use object detection features or explicit attention often excel in SPICE (because they catch all objects). We’ll aim for a high SPICE; current SOTA might be around 25-30 SPICE on COCO. SPICE also provides a break-down by semantic category (Objects, Attributes, Relations F-score) which can be insightful. For example, if the model does well on objects but poorly on relations, it might say “man and woman” but not correctly link “man *holding* an umbrella”. We can use this analysis for targeted improvements (like adding relation-oriented loss if needed).

* **BERTScore:** BERTScore (Zhang et al. 2020) computes similarity between candidate and references by aligning words based on cosine similarity of contextual embeddings (from BERT or RoBERTa). It gives Precision, Recall, F1 scores. BERTScore effectively measures semantic similarity; it can catch paraphrases and logical equivalences that n-gram metrics miss. We can use BERTScore with RoBERTa-large or DeBERTa as the underlying model (there are recommended settings). It correlates with human judgment of caption quality better than BLEU or METEOR in many cases. A high BERTScore F1 means the model’s description conveys nearly the same meaning as references even if words differ. We should be careful: if references aren’t exhaustive, the model might describe something extra and BERTScore might slight penalize it (if that extra thing isn’t in references, but BERT might still see it as somewhat similar context). Nonetheless, it’s a good sanity check especially if our model uses synonyms often (it’ll get credit via BERTScore even if BLEU misses it). We’ll report BERTScore as supplementary.

* **CLIPScore:** CLIPScore (Hessel et al. 2021) uses CLIP embeddings to score how well the caption matches the image, *without* needing references. It’s simply cosine similarity between CLIP(image) and CLIP(caption). Despite its simplicity and not using references, it has a surprisingly high correlation with human judgments of caption correctness and relevance. The advantage is that it can detect if the caption is describing something not in the image (lower CLIPScore), or if it’s accurately describing the image. It will not penalize creative phrasing as long as semantics align. We plan to use CLIPScore as a metric to verify that our model’s outputs stay relevant to the image. For instance, sometimes RL optimization of CIDEr can lead to captions that satisfy common n-grams but could become a bit detached from the image (like adding a common phrase “on a sunny day” to many captions even when it’s indoors, to gain n-gram overlap). CLIPScore would drop in such cases, telling us something’s off. We can thus balance training or choose a model checkpoint that is a good compromise (perhaps slightly lower CIDEr but higher CLIPScore).

  Since CLIP is a powerful model, a high CLIPScore from our output indicates it aligns with how CLIP interprets the image. If our model learns to maximize CLIPScore, it would essentially be trying to produce the caption CLIP implicitly expects. Actually, one research used CLIPScore as a reward in RL (called ClipRL or similar) and it improved descriptive quality. But as mentioned, combining with CIDEr was only modestly effective. We’ll use it mostly for evaluation.

* **Diversity Metrics:** To quantify *linguistic diversity*, we can compute:

  * **Distinct-N:** fraction of unique n-grams in all generated captions / total n-grams. Often reported in dialog generation, we could use distinct-1 (unigram diversity) and distinct-2 (bigram diversity) for our model’s outputs. If a model always produces the same generic sentence, distinct-n will be low. A high distinct-1 means a rich vocabulary usage across images, and distinct-2 indicates varying combinations of words. We have to be careful because if the model describes very different images, it naturally should have high distinctness – so some baseline like reference captions distinctness can be computed for comparison. COCO’s references themselves have a certain diversity (though workers sometimes repeated phrases).
  * **Vocabulary size** on a set of images: how many unique words the model uses. This is simpler but indicative (e.g., does it use 500 different words or only 200?). A more diverse model will use more varied words given the varied content.
  * **Sentence length variance:** Are all outputs roughly the same length (which could indicate a template style) or do they vary appropriately? If we see the model always produces \~10-word sentences even when an image is simple (should maybe produce shorter) or complex (maybe should produce longer), it might be sticking to a learned length prior. Encouraging varying lengths can be good. We might not optimize for this explicitly, but we can measure average length vs references.

  These diversity metrics are not standard in COCO leaderboard, but since the question emphasizes diversity, we’ll include them in analysis. Sometimes there’s a trade-off: a model that is carefully optimized for CIDEr might converge to a safe mode that uses common words (since rare word errors cost score). But we want to encourage correct use of specific words when appropriate. We’ll see if RL plus maybe some diversity reward can boost these.

* **Human Evaluation Protocols:** Automated metrics don’t tell the whole story, so human eval is important. In research papers, human eval is done in a couple of ways:

  * **Pairwise Comparison:** Show a human an image and two captions (ours vs a baseline or ours vs ground truth) and ask which is better or if they’re equally good. This can give a win/tie/loss rate.
  * **Rating:** Have humans rate each caption on several criteria, like relevance (does it describe the image correctly?), fluency (is it grammatical and natural?), and detail (does it describe enough details?). Usually a Likert scale 1-5. Then average those.
  * **Turing Test style:** Ask humans if they think the caption was written by a human or a machine, or if it is an acceptable caption for the image.

  COCO’s 2015 human eval used a metric called **“Meteorology”** or something – basically they asked if a caption is at least as good as a human-written one for that image; the percentage of captions that meet that bar is reported as “Humanscore” \~ around 50% for best models back then. We could do a small human study if possible in our environment to validate qualitative aspects. But since this is a research/planning context, we may not actually recruit annotators. Instead, we’ll rely on the variety of automated metrics to approximate quality from different angles.

  For in-house testing, we (or stakeholders) can manually check some outputs especially for tricky cases (complex scenes, uncommon objects) to see if the model says something sensible and not something incorrect or unsafe.

* **NoCaps and Generalization:** The question mentions MS-COCO, but also we might consider evaluating on *nocaps* (the benchmark for novel object captioning, where some objects in images were not seen in COCO training). Modern models using CLIP or large vocab can handle novel objects due to pretraining. If we integrate CLIP or an LLM, our model could likely name objects beyond the 80 COCO classes. We might test it informally on some images containing novel objects (like “microscope” which is not common in COCO) to see if it can do so. Metrics like **CIDEr** rely on references containing the object name – in nocaps, references do include novel object names but the model must know them. If our model uses a large pretrained vocabulary, it might succeed. For formal nocaps eval, metrics are similar but they break results into in-domain, near-domain, out-of-domain categories. BLIP-2 and friends excel at nocaps (some even zero-shot). We won’t focus too much on it in initial training, but it’s good to keep in mind for future: diversity also means being able to handle new concepts.

**Benchmarking Examples:** We should gather some baseline numbers to know what to aim for:

* A typical baseline (say a Transformer with ResNet encoder) might get \~110 CIDEr, 27 BLEU-4, 0.36 BLEU-1, \~0.27 METEOR, \~21 SPICE on Karpathy test split. The current SOTA (as per PapersWithCode) like *mPLUG* or *GIT2* can reach \~140 CIDEr, \~0.30 METEOR, \~25 SPICE. Human references vs human references get 100 CIDEr by definition, SPICE for humans might be around 20-30 depending on how they measure (multiple human references vs each other).

From the MDPI review: Table or lines around \[46†L35-L43] show:

* SimVLM: CIDEr \~143.3, SPICE \~33.7 (not sure, maybe those lines in \[46]).
* OFA: CIDEr \~145.3, SPICE \~32.5.
* CoCa: CIDEr \~143.6, SPICE \~33.9.
* GIT: CIDEr \~144.8, SPICE \~32.2.
* GIT2: CIDEr \~151.1, SPICE \~?? Possibly these are on test (maybe a certain slice).

Anyway, these are very high, approaching machine vs human surpassing. Not all improvements reflect in human eval always; some say beyond \~125 CIDEr, further gains give diminishing returns in human eyes, but that’s debatable.

We will use **multiple metrics** to guide development:

* If CIDEr is good but SPICE is low, likely the model is gaming n-grams without truly mentioning the right objects – possibly a warning sign of “sentence that has common words but misses specifics.” We’d then emphasize object recognition (maybe via multi-task detection or by adjusting training).
* If CLIPScore is low but CIDEr is high, model might be overfitting references – e.g., adding stuff that wasn’t in the image because references had it. We want CLIPScore and CIDEr both high; many recent models explicitly track CLIPScore to ensure they don’t degrade on it when optimizing CIDEr.
* If BERTScore is high but CIDEr low, could mean model phrased correctly but just different from references (so maybe fine in reality, and it might indicate the references were limited). In such case, maybe we’d lean more on BERTScore/CLIPScore to trust the model.
* Diversity metrics will ensure we don’t produce dull captions. If distinct-1 and distinct-2 of our model are significantly lower than those of ground-truth captions (we can compute ground-truth distinctness easily on the dataset), then our model might be too mode-collapsed. We’d consider injecting more randomness at inference or penalizing repetition.

**Human Evaluation Plan:** If time and resources allow, we can do a small-scale human eval:

* Show \~50 images with our caption vs another model’s caption (or vs one of the references) and ask an evaluator which is better or if equal. This could give a rough “win rate” for our model. If we compare to, say, the previous system (ResNet+LSTM baseline), we can quantify improvement.
* Alternatively, get absolute ratings: e.g., on a scale 1-5, how well does this caption describe the image? And maybe a separate scale for fluency. Or a yes/no: “Is there any error in this caption?” – error rate is important for user trust (even one wrong detail can mislead).
* We must be cautious of evaluator fatigue – usually one uses multiple annotators and averages.

We’ll also consider **edge cases** evaluation: Are there specific scenarios where the model fails? e.g., counting (some models say “two” when there are three objects), gender (inferring gender incorrectly), unusual objects (not recognized). For these, human eyes are the best judge. If we find consistent issues, we might incorporate targeted data or constraints (like a rule to not guess gender if not sure, though COCO people often label gender from appearance which is not always reliable either).

Finally, we should ensure our evaluation code and pipeline is solid:

* We’ll use publicly available implementations for these metrics when possible (e.g., MS-COCO evaluation script for BLEU, METEOR, CIDEr, ROUGE, and the SPICE Java code). We’ll cite the standard sources for metric definitions for completeness in documentation.
* We must ensure to do proper tokenization as expected by each metric (COCO script expects certain tokenization, like splitting punctuation, lowercasing, etc., to match how references are processed). If using the COCO evaluation toolkit, it handles it. BERTScore and CLIPScore require minimal processing (they use model-specific tokenization or just raw input for CLIP).

By evaluating on this broad set, we can provide stakeholders a detailed picture:

* A table of metrics (BLEU, METEOR, CIDEr, SPICE, CLIPScore, etc.) comparing our new model to the old one and to some baselines.
* Perhaps some qualitative examples where our model shines (and possibly where it fails) – showing images with captions and maybe highlighting attention.
* If needed, an explanation that optimizing one metric can affect others (e.g., why we do RL on CIDEr but also check SPICE to ensure we don’t drop object coverage).

Since the user specifically said “go beyond BLEU” and included these metrics, we will certainly do so. We’ll also explain metrics in the report with citations:
e.g., explaining CIDEr’s correlation, SPICE’s semantic matching, CLIPScore’s reference-free nature, etc., to justify why we consider them.

## 8. Inference Strategies for Diverse and Efficient Captioning

During inference (generation time), the decoding strategy can significantly influence the properties of the output. We compare and plan to utilize various strategies:

* **Greedy Decoding:** This picks the highest-probability word at each step. It’s fast and deterministic, but often yields *safe, generic captions*. For instance, many greedy decoders often output a common sentence structure (“a \[object] is \[doing something]”). It can also get stuck in loops if the model probabilities aren’t well-behaved (though a good model usually ends the sentence properly). Greedy is a baseline; it maximizes the single-sequence probability but not necessarily the overall caption quality (especially if the model has learned to hedge its bets).

* **Beam Search:** Beam search explores multiple hypotheses in parallel. We maintain *B* (beam size) candidates at each step, expanding each by possible next words, and keep the top *B* by cumulative probability. This allows the decoder to consider alternatives that might temporarily look less likely but lead to a better final caption. Beam search tends to improve accuracy metrics (especially BLEU and CIDEr) because it often finds a caption closer to references than greedy would. However, larger beams can cause the issue of *repetition* or dullness (the infamous “beam search degeneration” where output becomes very banal or repeats, found in dialogue models). In image captioning, beam sizes 3 to 5 are typical; beyond that, returns diminish and sometimes quality degrades (because the model probability might favor repeating known phrases to maximize likelihood, which beam search will exploit). We will likely use beam size \~5 for final caption generation when evaluating on metrics, as this is standard in COCO evaluations (and most papers report beam search results). For diversity, beam search by itself doesn’t introduce randomness, so it won’t create novel phrasings the model never considered; it just picks the best from what the model has learned.

* **Diverse Beam Search:** A modification of beam search where beams are encouraged to be different from each other. One approach is *Group Beam Search* (GBS) by Vijayakumar et al. (2018), which divides beams into groups and adds a diversity penalty for beams that are too similar to ones already in the group. For instance, if two beams start with the same bigram, you penalize the probability of continuing them in the same way. The goal is to get multiple distinct captions that are all high-likelihood. This is useful if we want to generate a **set of captions** for one image (say top-3 diverse captions). It’s not typically used when you only need one final caption, but we could use it in a reranking context: generate, say, 10 diverse beams, then pick the best according to some secondary criterion (like CLIPScore). This can sometimes yield a better result than plain beam search, because the highest probability beam might be slightly generic, whereas another beam that’s almost as good probability-wise might mention a specific detail that also scores better on CLIPScore or human preference.

  * If integration complexity is low, we might implement a simplified diverse beam: ensure no two beams in the final set are identical (which can happen if the model has a strong mode) and encourage exploring different descriptions (maybe by injecting slight negative feedback if a beam uses a word that another beam already used at the same position).
  * However, COCO evaluation only allows one caption per image typically. If we return multiple, it’s more for optional outputs (like an app that shows alternate captions). It’s a nice feature though for user-facing applications (like pressing a button to “generate another caption”).

* **Top-K Sampling:** Instead of always picking the best word, we sample the next word from the model’s probability distribution, but restricted to the **top K** most likely words at that step. Words outside the top-K are treated as having zero probability. This prevents extremely unlikely words from being chosen (which could produce nonsense), yet allows some variability among the high-ranked words. For example, if an image could be “cat” or “kitten”, top-2 sampling might sometimes pick “kitten” even if “cat” is slightly more likely, giving variety. A common choice is K=5 or 10. Smaller K yields more conservative outputs; larger K (like 50) yields more surprise but also risk of irrelevant words if the model’s tail probabilities are poor. We can adjust *temperature* in sampling as well: a temperature <1 makes the distribution sharper (closer to greedy), >1 makes it flatter (more random).

  * Sampling can increase **linguistic diversity** across different runs or different images. If we want each caption to be individually more diverse, we could use a bit of sampling at inference to avoid the same stock phrases. But pure sampling can degrade the *expected* metric scores because it might occasionally produce a less-than-optimal word. A compromise is to use sampling for the first word or two (to get a diverse start) and then beam search from there – but that’s an unusual technique.
  * Perhaps more straightforward: generate multiple captions via sampling and then choose the best by some criterion (like average of metrics or a learned ranker). This is related to MBR, discussed next.
  * We might use top-K or nucleus sampling to generate *candidate lists* for evaluation or re-ranking. In a deployment scenario where one might want a creative caption, sampling is the go-to. But in COCO benchmark terms, people usually use beam search.

* **Nucleus Sampling (Top-p):** Instead of a fixed K, nucleus sampling (Holtzman et al. 2019) chooses a dynamic cutoff where it includes all words whose cumulative probability is <= p (say p=0.9). This adapts to the uncertainty of the model: if the model is very confident (one word has 90% probability), then it’ll essentially pick from just that one; if the model is uncertain (many likely options), it allows more choices. This tends to produce more natural, less degenerate text than top-K, because it doesn’t artificially limit to K if the distribution is flat (it might include more in that case), and it doesn’t include long-tail low-probability words if the distribution has a sharp drop (like beyond the nucleus threshold). Nucleus sampling is often preferred for open-ended generation because it avoids the pitfall of picking a weird low-probability word that top-K with large K might.

  * For captioning, nucleus sampling could allow, for example, sometimes saying “on a sofa” vs “on a couch” (both plausible, share probability mass). The randomness can lead to slight paraphrases each run.
  * We should test a bit if sampling degrades factual accuracy: ideally, the model’s probability distribution is such that any sampled word still fits the image. If the model is well-trained, the high-probability words should all be correct or at least not wrong. But if, say, the model is 60% on “dog” vs 40% on “cat” and it’s actually a dog, sampling could choose “cat” 40% of the time and be wrong. Beam would always pick “dog”. This is the precision-recall trade-off: sampling increases diversity (recall of different phrasings) but can drop precision (correctness). For our final system, we might lean on *deterministic decoding (beam)* for the primary output to ensure consistency. However, we can offer an option to sample for alternative captions if needed.

* **Minimum Bayes Risk (MBR) Decoding:** MBR is an approach where, rather than maximizing likelihood, we try to maximize a utility function that directly measures quality (like a metric). The procedure often is: sample a bunch of candidate captions from the model, then for each candidate, estimate its expected utility by comparing it to all other candidates under some similarity metric, then choose the candidate with highest expected utility (minimum risk of being far from the “true” best caption). In summarization, they used BERTScore or ROUGE as the utility. In captioning, one could use CIDEr or CLIPScore as the utility. Hessel et al. (2021) actually explored MBR with CLIPScore for image captioning: they found it can yield improvements in human preference because it picks a caption that is on average most similar to others the model could have generated, which tends to be a safer, consensus caption. The risk is it might also pick a somewhat boring caption. If the utility is CLIPScore, MBR is effectively picking the caption that best matches the model’s ensemble implicit opinion of the image. That could result in a very *accurate* caption. If the utility is CIDEr (comparing against the other sampled ones as pseudo-refs), it tends to choose a caption that uses common elements the model is confident about.

  * We could implement MBR like: generate N candidates via nucleus sampling; for each candidate, compute average BLEU/CIDEr/CLIPScore against the other N-1; select the highest scoring one. This adds computational cost (comparing many pairs). But if N is say 20, it’s not too bad. It might improve quality slightly in edge cases. However, because we actually have ground-truth references available for eval, a more straightforward approach is to just pick the one with highest CLIPScore out of samples. That itself often improves relevance (like a CLIP reranker).
  * So, a simpler **reranking method**: generate, say, 10 beams or 10 samples, then use a separate model or metric to pick the best. E.g., use CLIPScore: compute CLIP image-text similarity for each candidate and choose the max. This can fix cases where beam search’s top result might have slightly awkward phrasing or missed a detail that another beam had. Similarly, one can train a small classifier to predict which caption out of a set is better (learning from human labeled comparisons if available, similar to a reward model).
  * Reranking was used in older captioning pipelines: for example, generate with a language model then rerank with a CNN+RNN joint scoring network or with a model that checks for correctness. With modern CLIP, using it as a reranker is very effective because CLIP has seen a lot; if a candidate says something not consistent with image, CLIP will give a lower score.

* **Low-Latency Strategies:** In real deployment, if we need *real-time* captioning (e.g., for assistive devices or mobile), beam search might be too slow on CPU. We might then prefer greedy or a small beam. We can also reduce vocabulary (only allow likely words via a constrained decoding, though that risks missing some). Another approach is **knowledge distillation**: train a smaller model to mimic the output of our large model, so at inference we use the small one. For instance, train a TinyTransformer or even a new LSTM (small) on captions generated by our big model for many images. If done well, the small model can retain much of the performance but run faster on limited hardware. This has been done in NLP (student models).

  * Additionally, we can use **quantization**: compress model weights to 8-bit or use integer operations (some libraries allow quantizing transformers with minor accuracy loss, achieving maybe 2-4x speedups on CPU).
  * PyTorch 2.0’s compile and the ONNX export could help too. We might attempt to export the model to ONNX and run on an inference engine optimized for target hardware (like TensorRT for NVIDIA GPUs or OpenVINO for Intel).
  * Another strategy is to limit the maximum caption length. If we know captions rarely need to exceed 15 words, we can cut off there. That bounds worst-case latency. We can also start generating once the CNN is done (which is anyway needed) and do it progressively.
  * With batch processing, if captioning multiple images at once, we can use beam search in batch mode to amortize cost (most libraries support that).

  But given target is mid/high GPUs, latency isn’t a huge issue. If running on an A100, even a large model with beam search will generate in under 0.1s. The future edge compatibility is a consideration: for that, we’d keep in mind the possibility of switching to a leaner backbone (EfficientNet) and maybe an LSTM or distilled Transformer, and using greedy decoding or very small beam. Our modular design should allow toggling these for a deployment build.

* **Controllable Generation:** Not directly asked, but sometimes diversity can be improved by control. E.g., using **stochastic beam search** (choose between top beams randomly according to some distribution), or adding random noise to logits (temperature sampling). If we wanted to ensure each image might have multiple different captions over different runs (maybe for a slideshow different phrasing each time), we can just run nucleus sampling with different seeds. Our system could have a mode: “creative mode” uses sampling with temperature 1.0, “accurate mode” uses beam search.

In terms of usage:

* For the **COCO benchmark**, we will use **Beam Search (likely 5 beams)** to report our highest scores, since that’s what others do and it generally yields the best deterministic result. We might also report the score for nucleus sampling output averaged over a few runs to demonstrate the model can maintain quality even with randomness.
* For showcasing **diversity**, we can present example images with **multiple captions** our model can produce. For instance, "Image: \[a park scene]. Captions: (1) A group of people sitting on benches in a park. (2) Several people relax on park benches under trees. (3) People are seated outdoors in a public park area." – demonstrating variability in wording. These might come from sampling or diverse beam. We can mention this as a qualitative benefit of using sampling or diverse decoding: it avoids the “one-size-fits-all” caption problem.

**Re-ranking method clarity:** We likely will incorporate at least a CLIP-based re-ranker when we try to maximize metrics like CLIPScore/human alignment. If our primary aim is COCO metrics, a known trick was: generate 20 with beam and pick the one with highest SPICE (comparing to references). But that’s cheating for evaluation since references are known; can’t do that in real usage. Instead, CLIPScore as a re-ranker is fair since it's using image only. We’ll see – it might boost CLIPScore and arguably also human quality, at the expense of maybe slight drop in CIDEr if CLIP and references disagree on focus.

**Summarizing strategies:**

* We will implement **Beam Search** for default high-quality results.
* Provide an option for **stochastic decoding (top-p)** for varied results or to generate multiple candidates.
* Possibly use **Diverse Beam** or **MBR** if it proves beneficial in results (we’ll experiment).
* Emphasize that **for diversity** in a single-run scenario, introducing randomness (top-p) is the way to go, whereas beam ensures consistency and usually better metric alignment.
* Also, mention that advanced methods like **Self-BLEU** (to measure diversity among outputs for same image) can be used to quantify how different the multiple generated captions are. We can ensure our model doesn’t produce essentially the same sentence structure every time by evaluating self-BLEU or distinctness across multiple samples.

To connect with future edge deployment: If needed, we can switch to **greedy decoding on a small model** for real-time, but then maybe generate multiple times and pick best if we want to avoid beam overhead. Or even a two-pass system: a small model generates a caption quickly, then a large model (in cloud) can refine it if high precision is needed and latency allows.

**Conclusion of this section:** By combining these inference techniques, we can achieve a **balance between speed and diversity**. For the high-end setting (GPUs, not much limit on compute per image), one can do beam search with re-ranking to get an optimal caption. For a user-facing application wanting more creativity, nucleus sampling can be used to avoid sounding monotonous. Our system being modular and configurable will allow switching the decoding strategy easily (perhaps an argument like `--decode_strategy beam5` or `--decode_strategy nucleus_p0.9`).

Finally, we will provide some guidance on how to set these: e.g., “for highest CIDEr, use beam; for more varied outputs, use sampling; for multiple captions, consider diverse beam or nucleus sampling with different seeds; for very fast, use greedy or beam=1.”

## Integration, Modularization, and Deployment Considerations

Beyond model architecture and training, it's crucial to make the system **flexible, efficient, and maintainable** in a production or research setting. We address component modularization, PyTorch 2.x integration, containerization, reproducibility, and licensing:

### Component-Level Modularization

To easily experiment with different encoders, decoders, or other submodules, we will design the code in a **modular, plug-and-play fashion**. Concretely:

* **Separate Classes for Each Module:** We can define, for example, an `ImageEncoder` base class with subclasses like `ResNetEncoder`, `ViTEncoder`, `CLIPEncoder`, etc., each implementing a `forward(image)` method that outputs a tensor (e.g., feature map or patch embeddings) and perhaps spatial metadata (like coordinates if needed). Similarly, a `TextDecoder` base class with subclasses `LSTMDecoder`, `TransformerDecoder`, etc., implementing `forward(encoder_outputs, embedded_caption)` to generate output logits or sequences.
* **Unified Interface:** The main CaptioningModel can then have attributes `.encoder` and `.decoder` of these base types. Training code calls `features = model.encoder(image)`, then passes that to `model.decoder(features, caption_tokens)`. Because they share a common interface, swapping encoders is as easy as constructing the model with a different encoder object. For example:

  ```python
  encoder = ViTEncoder(pretrained='ViT-B/16')
  decoder = TransformerDecoder(model_name='gpt2')
  model = CaptionModel(encoder, decoder, fusion_method='cross_attention')
  ```

  If we want to try Swin, we instantiate `encoder = SwinEncoder(...)` and keep the rest the same.
* **Encapsulation of Attention Mechanism:** If we allow different attention mechanisms (say we implement an `AttentionModule` for the LSTM case separately from the transformer's built-in attention), we can similarly abstract that. In an LSTM-based decoder, we might have an `AttentionLayer` component that could be either `AdditiveAttention` or `MultiHeadAttention`. We can choose via config. In a transformer decoder, multi-head scaled dot is intrinsic, so that’s fixed.
* **Configuration Files:** We’ll use configuration (YAML or JSON or Python argparse) to specify which encoder/decoder/embedding/etc. to use. For example, a config might specify:

  ```yaml
  encoder: "clip_vitL14"
  decoder: "bert_decoder"        # (perhaps a Transformer encoder-decoder using BERT as decoder)
  use_object_features: true
  attention: "multi_head"
  ```

  The training script reads this and constructs the appropriate modules. This not only aids experimentation but also reproducibility (just save the config used for a given run).
* **Decoupling Data from Model:** We will keep data loading and preprocessing separate. The model will expect images in a normalized tensor form and caption tokens (with <BOS>, <EOS> tokens as needed). We can have different tokenizers (basic one vs BPE) and that should be part of the **embedding/decoder** module to handle (e.g., GPT2’s tokenizer vs a simple word list).
* **Testing New Ideas:** If someone wants to try a new fusion mechanism (say Co-Attention layer stack), they can create a new `FusionEncoder` subclass that takes both image and text inputs in forward. Our CaptionModel could detect if the encoder expects text as well (via introspection or a flag) and route accordingly. Alternatively, treat that as part of decoder (like a multimodal transformer that spans enc-dec).
* **Logging & Visualization Hooks:** Within each module, we can add hooks to extract intermediate outputs for analysis (like attention weights). E.g., our TransformerDecoder could optionally return attention maps if asked. This helps in debugging and visualizing as discussed.

Overall, modularization ensures that as research evolves (new encoders or decoders come out), we can integrate them with minimal changes. It also allows **ablation studies** easily (e.g., turn off a component by replacing it with an identity function or zero-out features to test importance).

From a software design perspective, we might use a framework like PyTorch Lightning or Hydra for config management. Lightning could structure model components well, though for full control we might stick to raw PyTorch + Hydra.

A side benefit: this modular design aligns with **OpenAI’s CLIP or Hugging Face’s models** usage – we can wrap those in our interface rather than writing everything from scratch. For instance, use `transformers.ViTModel` inside our ViTEncoder. Similarly use `AutoModelForCausalLM` for a GPT2 decoder, etc.

### Integration with PyTorch 2.x and `torch.compile`

PyTorch 2.x (specifically 2.0 and above) introduced the `torch.compile` function which can JIT compile models to improve performance. We will leverage these modern features:

* **`torch.compile`**: After we finalize the model, we can do `optimized_model = torch.compile(model)`. This will attempt to fuse operations and optimize the forward pass across modules. For example, it might fuse layernorm + linear ops or optimize the attention pattern. We have to ensure our model code is **compatible**: it should be mostly static in structure (no changing of model architecture on the fly). Our modular approach with Python classes should be fine, as compile will trace through calls as long as they’re not data-dependent control flows. We should avoid things like if statements that depend on input data shapes or values (except trivial ones). If we do have any (e.g., an if that does different stuff in training vs inference, we might separate those paths or use torch.compile only for inference mode).
* **Dynamic shapes**: For some cases like varying caption lengths, we rely on padding and masking. `torch.compile` can handle dynamic sequence lengths but it may perform better if shapes are consistent. We can compile with representative inputs (like max length).
* **Autocast and Gradscaler**: PyTorch AMP (automatic mixed precision) works seamlessly with `torch.compile`. We should use `with autocast():` in the training loop to get FP16 speedups on GPU. In inference, we can also use autocast for operations (particularly in CNN part or matrix mult) to speed up.
* **Memory**: We should monitor memory usage under compile; sometimes the compiled graph might use more memory for intermediate caching. PyTorch 2.0 has addressed a lot, but we can toggle compilation off if an issue arises (like fallback to eager).
* **`torchscript`**: Prior to compile, TorchScript was the way to optimize/serialize. We might not need TorchScript if compile suffices for performance, but for deployment (like exporting to C++ or mobile), TorchScript or ONNX export is needed. We can attempt to script or trace our model (especially if planning to deploy on mobile). We have to ensure any model components that are not scriptable (like some Hugging Face modules with dynamic control) are handled. Many HF models have `model.eval().to('cpu').half()` readiness for scripting or tracing now.
* **PyTorch 2.x features**: We will use the newest PyTorch 2.x, which has improved Transformer implementations (like scaled\_dot\_product\_attention in C++ for speed), and also better support for large models (Sharder, FSDP if needed for multi-GPU).
* **`torch.autograd.graph.save_on_cpu`**: For large models, PyTorch 2 allows offloading grads to CPU to save GPU memory. If we fine-tune something like T5-XL (3B params), we might utilize such memory-saving features.
* **`torch.compile` for inference**: We can compile a separate instance of model in eval mode for deployment for max speed. This compiled model can be put into a container for serving.

### Containerization and Reproducibility

To ensure the system runs reliably across environments and can be deployed easily, we plan to use **Docker containerization** (or similar):

* **Docker Image**: We will create a Dockerfile that starts from a known base (like `nvidia/cuda:11.8-cudnn8-runtime-ubuntu20.04` plus Miniconda, or use the official PyTorch Docker image for 2.x if available). In it, we install:

  * PyTorch (with the correct CUDA version), Torchvision, etc.
  * Needed Python packages (transformers, timm, numpy, etc.) pinned to versions that we tested (to avoid future version changes breaking things).
  * Our code (we can either build it into the image or mount it as volume in dev, but for release an image containing the code is simpler).
  * If using BLIP/CLIP etc., either their pip packages or we include their code. LAVIS library (for BLIP) can be pip installed. We must note licenses in image if distributing it outside.
  * The model weights: if large (hundreds of MBs), we might not bake them into the image (to keep image size smaller), but rather have the container download them on first run or mount a volume with weights. For a closed environment, including them is fine to ensure no external dependency.

* **Reproducibility**: We will fix random seeds (torch, numpy, etc.) for training runs to ensure we can reproduce the same results given same environment. We’ll document the seeds and any nondeterminism. Note, some ops in PyTorch/CUDA are nondeterministic (e.g., certain reduction operations). There is an option `torch.use_deterministic_algorithms(True)` which we can set for debugging reproducibility at cost of possibly slower or not available ops.

* **Configuration Saving**: Each training run’s config (model hyperparams, dataset split, seed, etc.) will be saved alongside the model checkpoint (perhaps as a JSON file or in the checkpoint metadata) so that we know exactly how it was produced. This helps when revisiting after months or handing over to others.

* **Version Control**: We will use Git to track code changes. Possibly tag releases that correspond to certain result snapshots. The Docker build will reference a specific commit hash to be precise.

* **Continuous Integration**: If this is a collaborative project, setting up a CI pipeline to build the Docker image and run basic tests (like ensuring a forward pass works for a small model) can catch environment issues early. For instance, ensure that our model can be loaded and run on CPU and GPU in the container.

* **Deployment**: The container can expose a service (maybe a simple Flask app or FastAPI) that loads the trained model and on HTTP request returns a caption for an input image. This wasn’t explicitly asked, but it's a likely scenario for using the model. We’d include configuration for loading e.g. the highest performing checkpoint and setting the desired decoding strategy by default (maybe beam).

* **Edge considerations**: For future edge use, we might export the model via ONNX and use something like TensorRT or CoreML. Containerization for edge (like Android APK or iOS app) is different, but we can at least ensure the model is exportable. For now, focusing on Docker for server GPU deployment is best.

* **Resource Allocation**: We may also containerize the training environment. That means if someone wants to replicate our training, they run the training container with appropriate GPU access and it just works (because all deps are in place). This avoids the "it works on my machine" problem.

* **Configurable environment**: We will allow environment variables or config files for paths (like where dataset is stored, where to output results). The container can use volumes for data in/out, to avoid making the image huge with embedded datasets or results.

**Reproducibility beyond code**: We will likely include references to random initialization or checkpoint initializations (like exact version of pretrained model weights used). For instance, if using CLIP weights, specify which checkpoint (like "OpenAI CLIP ViT-L/14, 336px model"). Many of these are versioned by their release.

### Licensing Implications

We must be mindful of licenses for all components to ensure that the final system can be used in the intended context (open-source, commercial, etc.) without legal issues:

* **Our Code**: We will choose a license for our own code (maybe MIT or Apache-2.0, since we are combining many MIT/Apache components – this is the most compatible). If this is an internal project, at least we need to avoid any viral licenses in dependencies that could affect usage.
* **Pretrained Models**:

  * **ResNet (Torchvision)**: Typically under BSD-style (PyTorch model zoo).
  * **ViT (Google)**: Their official model (in TF) is Apache, and implementations in HuggingFace or Timm are also Apache/MIT. No issue for commercial use.
  * **Swin (Microsoft)**: Official implementation MIT, weights are released for use.
  * **ConvNeXt (Facebook)**: That paper’s code was in MMSeg or detectron, likely Apache 2. Actually, the ConvNeXt code in PyTorch (timm) is permissive. Facebook sometimes releases under CC-BY for weights, but even that is fine (just requires attribution).
  * **EfficientNetV2 (Google)**: Apache 2.0 (model code in TF or timm).
  * **CLIP (OpenAI)**: This is a bit special. The code is MIT, but OpenAI’s weights have a non-commercial statement (though not a formal license). Using them in a research project is okay; for commercial, one should get OpenAI’s permission or use alternative weights. We plan to use **OpenCLIP** models (trained by LAION/EleutherAI) which are MIT licensed and trained on public data (LAION-400M or 2B). Those have truly open weights. We'll note: if we by chance use OpenAI’s weights, we should treat it as research-only. But better to avoid that.
  * **BERT, RoBERTa**: BERT is Apache 2, RoBERTa’s weights might be CC-BY (which in essence just requires attribution, which we can handle by citing the paper). That’s okay for commercial use. DeBERTa from Microsoft: The newer DeBERTa v3 is under MIT license (they mention it on HF model card). Old DeBERTa had a non-commercial license, but we can stick to v3.
  * **GPT-2**: According to OpenAI, GPT-2 is available for commercial use (they didn’t put a restrictive license on it; the model weights are accessible and many have used them in products). The GitHub says MIT license. So that should be fine. If any concern, we could use EleutherAI’s GPT-J or GPT-Neo (which are MIT). But GPT-2’s fine.
  * **T5**: Apache 2 (clear for any use).
  * **BART**: The weights on HF are under the same license as fairseq, which is MIT. Should be fine.
  * **Flamingo**: Not applicable for actual use since weights aren’t released (just for concept).
  * **BLIP/BLIP-2**: Salesforce’s LAVIS code (which includes BLIP) is BSD-3-Clause – very permissive. The pretrained weights they released can be used; I don’t see non-commercial restrictions. BLIP’s training data includes COCO, VG, etc. which is fine. BLIP-2 uses CLIP and Flan-T5 weights, both open. Flan-T5’s weights are from Google’s T5 (Apache).
  * **ALBEF**: Code is MIT, but ALBEF used some private data (15M images) for pretraining. The model weights are still usable; training data licensing doesn’t transfer to model outputs generally. Should be okay, but one should acknowledge usage if needed.
  * **COCO Dataset**: As training data, COCO images are mostly Flickr images under various CC licenses (some non-commercial, some no-derivs possibly). However, using them to train a model is typically allowed as a fair use or as per dataset terms (COCO is distributed for research). If we were selling a product directly containing COCO images, that’d be a problem; but a model trained on them is considered fine (with attribution in papers often given). For safety, a company might prefer training on only commercially free images (like Visual Genome has a share-alike license, etc.). But since nearly every caption model uses COCO, presumably it’s acceptable as long as we’re not redistributing the images. Possibly include a note like "Trained on MS-COCO captions. MS-COCO images are licensed as per <link>." in model card for transparency.
  * **Conceptual Captions**: It’s public data scraped from web, allowed for use with attribution. If we use it in training, we should cite it. Same for others like LAION (they are from common crawl; LAION dataset is CC0 IIRC because they are just links and embeddings).
  * **If the product is closed-source** but uses these models, we need to ensure none of the licenses force us to open-source our modifications. MIT/Apache/BSD do not; they only require keeping license notices. GPL would, but we are not using GPL components (most modern ML is Apache/MIT, except maybe some dependency like if we used one).
* **Attribution**: We should maintain a list of attributions for datasets and models:

  * e.g., "Portions of this work use the MS-COCO dataset, © 2015 Microsoft (used under fair use for research)." or as required.
  * "This model is built upon the CLIP model by OpenAI and OpenCLIP by LAION (MIT License), and the GPT-2 model by OpenAI (MIT License)."
  * In documentation or about page, list citations to BERT, ViT, etc. (They often require academic citation rather than legal, but it’s good practice).
  * If we distribute the code or model, include license files for any borrowed code. For example, if we included any part of HuggingFace Transformers code (which is Apache), keep the Apache license text in our repo.
* **Commercial Use Final Check**: The stack we plan (OpenCLIP, HF transformers, PyTorch) is all permissive. The main possible issue is if we accidentally use a model checkpoint with a noncommercial clause. We will double-check each pretrained weight:

  * HuggingFace model cards often state "license: \_\_\_\_". We'll choose ones marked as permissive. If any say "CreativeML Open RAIL" (like Stable Diffusion’s license) or "non-commercial", avoid for commercial context.
  * It's worth noting, some “Open” models have weird exceptions (like a license that prohibits use by certain entities as a joke, etc.). For safety, we stick to well-known ones like MIT, Apache, BSD, CC-BY.

In conclusion, **license compliance** will be ensured by using openly licensed resources and attributing them. We’ll prepare a LICENSE file for our project and possibly a README section listing all third-party components and their licenses (e.g., "Uses transformers vX.X (Apache 2.0), timm (Apache 2.0), COCO dataset (Creative Commons Attribution 4.0), ..." etc.). This transparency will make it easier for any future users to know what they can do.

---

All these research findings and decisions will guide us to implement an image captioning system that is **state-of-the-art** for accuracy, **flexible** for experimentation, and **robust** for deployment. By systematically upgrading each part of the classical model and using modern training techniques, we target a system that produces **more semantically accurate captions** and **more varied phrasing**, without sacrificing performance or reliability.

# Enhancing the Image Captioning System: Modern Architectures and Techniques

***Introduction:*** Image captioning models have progressed significantly since the original **“Show, Attend and Tell”** CNN+LSTM with soft attention. Modern approaches use transformers, richer vision backbones, and large-scale pretraining to generate more accurate and diverse captions. **Figure 1** illustrates this evolution: (a) the traditional encoder-decoder (CNN + attention + RNN) pipeline, (b) a basic transformer-based model (vision features encoded by CNN, then a Transformer decoder), and (c) a vision-language pretrained transformer model that leverages large image-text datasets. Our goal is to redesign the current ResNet-101 + LSTM system into a **modular** framework incorporating these advances – improving semantic accuracy and linguistic diversity, while keeping an eye on efficiency. Below, we explore upgrades in each component (vision encoder, attention mechanism, decoder, language embeddings, and fusion techniques), as well as training strategies, evaluation metrics, inference methods, and practical implementation considerations (modularity, PyTorch 2.x integration, deployment, and licensing).

## 1. Vision Encoder Upgrades

Modern vision backbones can provide **richer visual features** and better integration with attention mechanisms than the legacy ResNet-101. We evaluate several candidate encoders:

* **Vision Transformers (ViT)** – ViT treats an image as a sequence of patch tokens and applies a pure Transformer encoder with multi-head self-attention across the entire image. This global receptive field allows ViT to capture long-range relationships (e.g. context between distant objects) that ResNet’s localized convolutions might miss. ViTs pre-trained on large datasets (ImageNet-21k or JFT-300M) achieve superior classification accuracy and transfer well to tasks like captioning. The attention granularity in ViT is at the patch level, meaning the model can attend to fine image details or global layout as needed. On the downside, vanilla ViT has quadratic complexity in the number of patches – for high-resolution images this can be computationally intensive (though patch merging or smaller patch sizes can mitigate this). **Pretrained models:** ViT weights (e.g. B/16, L/14, huge variants) are available openly (many under Apache-2.0 or similar licenses via the Timm or HuggingFace hubs). These provide a drop-in replacement for a CNN encoder, outputting a sequence of patch feature vectors instead of a 2D feature map. In a captioning model, we can feed those vectors to the attention module or directly to a Transformer decoder. Given that model size is secondary and we have powerful GPUs, a large ViT (with, say, 24 or more layers) can be used to maximize feature richness.

* **Swin Transformer** – Swin is a hierarchical vision transformer that introduces a **shifted window** approach to limit self-attention to local regions, then merges patches progressively. This design is **computationally efficient** (linear complexity w\.r.t. image size) yet still allows cross-region interactions by shifting the window positions at each layer. Swin encoders achieved state-of-the-art results in detection and segmentation, indicating they learn strong multi-scale features. For captioning, Swin’s features provide a mix of local detail and global context, potentially improving descriptions of both small objects and overall scene. In comparisons, Swin and ViT are often among the top performers in accuracy vs. speed trade-offs. **Pretrained weights:** Microsoft released Swin-B, Swin-L etc. under MIT license, readily usable in PyTorch (e.g. via `timm` library). These models are heavier than ResNet-101 (Swin-L has \~197M params vs. 44M in ResNet-101) but the accuracy gain may justify it, especially since we prioritize caption quality over model size.

* **ConvNeXt** – This is a **next-generation CNN** that revisits ConvNet design with modern tweaks (ResNet-like architecture with larger kernels, SiLU activations, LayerNorm, etc.). ConvNeXt models (e.g. ConvNeXt-Large, \~200M params) have matched transformer accuracy on ImageNet while retaining the convolutional inductive biases. For our purposes, ConvNeXt could be a strong encoder if we want *CNN simplicity with SOTA performance*. It produces a feature map like ResNet, so integrating it into the existing attention module is straightforward (spatial feature vectors similar to what we have now). It may not offer the explicit long-range attention of ViTs, but its large receptive fields and deep layers give it rich semantic features. **Pretrained models:** ConvNeXt weights are available under permissive licenses (the official implementation is in PyTorch and models are in `timm`). Using ConvNeXt might also ease future deployment on edge devices, since CNNs are often easier to optimize on limited hardware.

* **EfficientNetV2** – EfficientNetV2 is an updated family of efficient CNNs that optimize the depth/width/resolution scaling with neural architecture search. These models achieve excellent accuracy with relatively few FLOPs by using fused MBConv layers and other training tweaks. For a captioning encoder, EfficientNetV2 could drastically reduce computation while still providing high-quality features. EfficientNetV2-L, for example, is \~120M params and yields competitive results on classification. **Feature richness:** It may not capture as wide a variety of high-level concepts as a ViT trained on enormous data, but it excels at general visual feature extraction. EfficientNetV2 might be a good choice if we later target **lightweight inference** (e.g. on mobile/edge) – we could even **swap in** an EfficientNet encoder in our modular design when exporting a smaller model. **Availability:** Pretrained EfficientNetV2 weights (B0-B3 for smaller, up to L) are available (Google Brain’s release under Apache-2.0 and in PyTorch via `timm`). This gives us flexibility to trade off some accuracy for efficiency if needed in the future.

* **CLIP Vision Encoder** – CLIP (Contrastive Language-Image Pretraining) is a model trained on 400 million image-text pairs to align image and text embeddings. Its vision encoder (ResNet or ViT-based) produces embeddings highly correlated with semantic concepts and their textual descriptions. Using a CLIP encoder in our captioning model can significantly enhance *semantic accuracy*: the features are already aligned to recognize a wide variety of objects, actions, and attributes as described in natural language. For example, CLIP’s features might more easily distinguish fine-grained categories or relate image content to relevant words (even if those words were rare or unseen in COCO). In practice, one approach (as done by *ClipCap*) is to freeze the CLIP image encoder and feed its output to the decoder via a small learned projection or mapping network. This leverages CLIP’s semantic knowledge without requiring us to train the encoder from scratch. **Pretrained models:** OpenAI’s CLIP ResNet50/ViT-B models are available (code is MIT-licensed), but OpenAI’s weights come with a note that *“any deployed use – whether commercial or not – is out of scope”*. To avoid licensing ambiguity, we can use **OpenCLIP** models (reimplementations trained on LAION data) which are MIT licensed and fully open. These include CLIP ViT-L/14 and even larger, providing excellent visual features. We should note that CLIP models are **computationally heavier** (e.g. ViT-L/14 has 768-dim embeddings and \~300M params in vision alone). But given our GPU environment, this is acceptable. For edge deployment later, we might switch to a smaller CLIP (ViT-B/32 or a ResNet) or not use CLIP if resources are constrained.

* **DiT (Document Image Transformer)** – DiT is a vision transformer specialized for document images (pretrained on text-heavy images like scanned documents). Its architecture is similar to ViT (self-attention over image patches). For general image captioning on MS-COCO, DiT is not directly relevant since it’s tuned for document layouts and OCR tasks. Unless our application involves captioning documents or charts, a standard ViT or CNN is more suitable. We mention DiT mainly because it’s a recent vision transformer variant – but for natural images, its pretraining is not aligned with our needs. If one were to use DiT weights, it would require fine-tuning on photographic images extensively to overcome its document bias. **Pretrained weights:** Available from Microsoft for document tasks (MIT license code), but we will prioritize encoders known to perform well on natural images and captions.

**Summary:** Replacing ResNet-101 with a **ViT or Swin Transformer** offers richer features and finer attention granularity, at the cost of higher computation. These transformer encoders can capture global context and subtle semantics, likely improving caption detail and correctness. **Commercial considerations:** ViT, Swin, ConvNeXt, EfficientNetV2 all have permissive licenses (Apache or MIT), so integrating them is legally straightforward. CLIP’s OpenCLIP variant is MIT licensed, making it safe for commercial use (whereas using OpenAI’s original CLIP weights would require caution). In our **modular design**, we’ll implement an `ImageEncoder` interface so we can plug in any of these backbones easily. For example, we might start experiments with a CLIP ViT-L/14 encoder for maximum semantic knowledge, and later try a ConvNeXt or EfficientNetV2 for a speed-focused variant – without rewriting the core code. The encoder outputs (whether a feature map or a sequence of patch features) will be fed into the attention mechanism of the decoder seamlessly. Overall, upgrading the vision encoder sets the foundation for a more capable captioning system that “sees” the image in greater detail and semantic depth.

## 2. Attention Mechanism Enhancements

The original Show-Attend-Tell model used a **single-head “soft” attention** (an additive attention mechanism) to focus on image features when generating each word. We can improve upon this in several ways to enhance how the model attends to visual information:

* **Scaled Dot-Product Attention (Transformer-style):** Modern attention uses the scaled dot-product formulation introduced by Vaswani et al. (2017). Given queries \$Q\$, keys \$K\$, and values \$V\$, the attention is computed as:

  $\text{Attention}(Q,K,V) = \text{softmax}\!\Big(\frac{Q K^T}{\sqrt{d_k}}\Big)\,V$

  where \$d\_k\$ is the key dimension. This formulation (with appropriate masking for decoder self-attention) is more efficient and parallelizable than the older additive attention used in Show-Attend-Tell, and it forms the basis of multi-head attention. By scaling the dot-products by \$\sqrt{d\_k}\$, it prevents extremely large values that could saturate the softmax. Adopting this mechanism in our decoder allows us to naturally integrate with transformer encoders and decoders. In practice, we will replace the previous attention module with a **MultiHeadAttention** module (PyTorch provides `nn.MultiheadAttention`) configured for our hidden dimensions. The dot-product attention not only improves computational efficiency (fewer small MLPs at every timestep, more matrix-multiplication friendly operations) but also tends to distribute focus more smoothly when used with multiple heads.

* **Multi-Head Attention:** Instead of producing one attention weight distribution over image regions per timestep, multi-head attention uses several independent “heads” (affine projections of queries/keys/values) that each focus on potentially different aspects of the image. Each head attends to the image features in a subspace, and the results are concatenated and linearly combined. This enables the model to consider **multiple visual sub-regions or concepts simultaneously**. For example, one head might concentrate on a person’s location while another head looks at what the person is holding, allowing the decoder to incorporate both pieces of information into the next word decision. Empirically, multi-head attention improves the model’s capacity to encode complex scenes – it can attend to multiple objects at once and capture their relationships. In terms of implementation, when using a Transformer decoder, multi-head cross-attention is built-in. If we were enhancing an LSTM-based decoder, we could implement multi-head by computing multiple sets of attention weights (with learned projection matrices for each head) and combining them. Pseudocode for cross-attending to image features with multi-head attention might be:

  ```python
  # image_feats: [N_regions, D], decoder_state: [1, D]
  for each head h in 1...H:
      attn_weights_h = softmax( (decoder_state W_q^h) @ (image_feats W_k^h).T / sqrt(d_k) )
      head_context_h = attn_weights_h @ (image_feats W_v^h)
  context = concat(head_context_1...head_context_H) @ W_o
  ```

  This would yield a `context` vector that integrates different attention perspectives. By using multi-head attention, we expect the model to gain **better coverage of the scene** (important for caption completeness) and potentially more **robustness**, since even if one head focuses on an irrelevant part, others might catch the relevant parts.

* **Cross-Modal (Co-Attention) Mechanisms:** In the standard encoder-decoder setup, the decoder’s cross-attention attends to image features (encoder output) at each step. We can extend this with more elaborate co-attention designs. **Co-attention** refers to schemes where image and text features attend to each other, possibly in multiple rounds. For example, a **bilateral co-attention** module could allow image features to be dynamically refined by textual context while text attends to image. In practice, models like **ViLBERT** and **LXMERT** (for VQA) split the transformer into two streams with alternating cross-attention between modalities. For captioning, one approach is to have an **interactive fusion layer** before decoding: e.g., apply a transformer encoder that takes both image region features and an initial textual context (such as an embedding of previously generated words or a special "<BOS>" token) and performs self-attention over the union of modalities. This can be done by concatenating image and text tokens and allowing full self-attention (with modality-specific positional encodings and masks to prevent text from seeing future text). Such a **unified transformer** would implicitly perform co-attention: image-to-text and text-to-image attention happen within the self-attention.

  Another approach is sequential co-attention: first attend image→text to get which words (or queries) are relevant for each region, then text→image to get image attention weighted by that. However, since our decoder generates text progressively, the simplest integration of co-attention is to stick with the standard cross-attention at each timestep (which is text attending to image) – but potentially stacking multiple cross-attention layers. Stacking could let the decoder refine its attention in multiple hops. In practice, a transformer decoder already has multiple layers, so later layers can revisit attention with more context from earlier layers.

  We should also mention **object-to-text attention** in another sense: e.g., if we have detected object bounding boxes (see object-based attention below), we might incorporate a **co-attention module that aligns detected objects with words or word embeddings**. For instance, a co-attention could align the "person" region with the word "man" embedding, etc., during training, to improve grounding. Models like **ALBEF** explicitly align image and text embedding spaces before fusion, which can be seen as a form of co-attention training (via contrastive loss and momentum distillation, see section 5).

  In summary, cross-modal co-attention enhancements aim to ensure **image and text features mutually inform each other** better than a one-way attention. This often yields more precise descriptions (the text is tightly grounded in image content). Architecturally, we will primarily rely on the Transformer's multi-head cross-attention in each decoder layer (which is a strong form of co-attention already). We remain open to adding an extra **multimodal encoder** block (with full self-attention on image+text tokens) if experiments suggest benefit, as done in some captioning transformers. Our modular design will allow inserting such a module if needed (e.g., an additional Transformer that takes image features and an encoded partial caption to refine the image features before final decoding).

* **Object-Level Attention:** The current system likely uses a grid of image features (e.g. a 14×14 feature map from ResNet) for attention. A powerful alternative is **object-based attention**, where the model attends over a set of detected object region features instead of uniform grid cells. Anderson et al. (2018) introduced this in the **Bottom-Up and Top-Down** model by using a pretrained Faster R-CNN to obtain region proposals (like 36 regions per image) with associated feature vectors. The captioning model then performs attention on these region features. The advantages are twofold: (1) The attention targets are **semantically meaningful objects/parts**, which improves interpretability and often leads to more specific captions (since the model explicitly “knows” about distinct objects). (2) It reduces the number of elements to attend over (36 regions vs 196 grid cells), focusing computation on salient areas. **Figure 3** shows an object-based attention pipeline: an object detector extracts appearance features for proposals (and geometry features like bounding box coordinates), then the encoder/decoder attends to those region features when generating the caption. The example caption “Three men wearing suits” attends to three detected person regions (red boxes) and perhaps a suits/clothing region (green box)【39†】.

  We will incorporate object-based attention by utilizing pretrained detection-based features. One practical way: use an existing dataset of region features (the Bottom-Up Attention repository provides pre-extracted features on COCO). In our architecture, instead of CNN features, the encoder can input a set of region features (each 2048-D plus perhaps 6-D geometry info for \[x,y,w,h] and object class probabilities). We then implement attention over these region features similar to before. Optionally, we could include the geometry features in the attention computation (Herdade et al. 2019 introduced **geometric attention** – multiplying attention weights by a learned function of region spatial relation). This helps encode spatial relationships like “next to” or relative sizes.

  Using object-level features tends to improve metrics like SPICE (which focuses on object presence and relations) since the model more reliably mentions the objects in the scene. It also makes attention heatmaps more interpretable: we can highlight which bounding box was focused on for each word, which users find intuitive. We must note, however, that relying on a frozen detector limits the model to the detector’s label space (80 object classes for COCO). If something outside those classes appears, the detector might not provide a region, and the caption model could miss it. In contrast, a ViT might pick it up via its features if it was pre-trained broadly. One compromise is to use object features as additional inputs, not exclusive ones, or use a more comprehensive detector (like one trained on Visual Genome with 1600 classes, as in VinVL).

  Given our priority is caption accuracy and diversity, we will likely use object-based attention as an **option**. We will implement a module to project region features and feed them into the decoder’s attention. During training, we might mix both region features and CNN features (some works do multi-feature fusion). Licensing-wise, using a pretrained Faster R-CNN (from Detectron, etc.) is fine (typically under Apache 2.0 or similar). The region features themselves can be used freely; they derive from COCO images but using them in a model is standard practice.

* **Adaptive Attention (Visual Sentinel):** Not every word in a caption is grounded in the image (think of articles, pronouns, or purely textual connectors like “a” or “on”). Lu et al. (2017) introduced an **adaptive attention** mechanism that allows the decoder to sometimes ignore the image features when generating certain words. They add a “visual sentinel” vector – essentially an extra memory in the LSTM that represents the decoder’s internal thought when not attending – and a gate that decides whether to use image attention or rely on this sentinel for each word. The model learns to, for example, put less weight on image features when producing determiners or morphological endings, thus not overloading the visual attention with non-visual tokens. This improved fluency without hurting content: e.g., ensuring the model doesn’t try to ground the word “the” in an image region. In a Transformer decoder, a similar effect can be achieved by adding gating on the cross-attention output: e.g., a learned sigmoid gate that weighs the cross-attention context vs. the decoder’s self-attention context. We can incorporate an adaptive gating if needed by modulating the cross-attention outputs per timestep. This is a relatively small addition (a linear layer to compute gate value from decoder state). If our analysis shows the model making errors like dropping words or repeating due to always attending, adaptive attention could help by giving it the flexibility to fall back to its language model memory.

* **Attention on Attention (AoA):** Huang et al. (2019) proposed an “Attention on Attention” module to further refine attended information. The idea is to compute not only the attention context vector but also an **information gate** that indicates how much useful information is in that context. They then combine the context and the gate to produce a final attended representation. This was used in the **AoANet** model and showed improvements over basic attention. Essentially, AoA can filter out less relevant attended features and keep only the crucial parts (kind of like a second pass that says “even though you attended here, how relevant is it?”). We could integrate AoA by, for instance, after computing the dot-product attention context, passing both the context and the original query through a small fusion (as per their formula) to get a modified context. This adds a bit of complexity, but since AoA achieved strong results (it was state-of-the-art around 2019), it’s a proven enhancement. If we use a pure Transformer decoder, one could argue the feed-forward networks in each layer might learn a similar gating, but explicitly adding AoA could still be beneficial.

* **Transformers and Stacked Attention:** Many recent captioning models use **stacked transformer layers** (like 6-12 layers) for the decoder, each with multi-head self-attention, cross-attention, and feed-forward. This stacking itself allows multiple “hops” of attention: early layers might focus on low-level image features, later layers on higher-level semantics (some works observed that in transformers, different heads or layers attend to different things). We will be adopting that paradigm by using a transformer decoder, so in effect we are already moving to a much more powerful attention system than the single-layer attention in Show-Attend-Tell. Research like Zhu et al. (2018) even tried **multi-layer attention supervision**, forcing each decoder layer to contribute to the loss. In our case, we might not supervise intermediate layers, but we will leverage deep attention implicitly.

**Impact on Caption Quality:** By enhancing the attention mechanism, we aim to improve both **interpretability** and **expressiveness** of the model:

* **Diversity and Completeness:** With multi-head and object-based attention, the model can attend to multiple elements in the scene, which should reduce instances of missing important objects or details. For example, a single-head model might caption an image as "a man riding a bike" if it mainly attends to the man, missing that the man is carrying a **surfboard**. A multi-head model might simultaneously attend to the man and a strange object he’s holding, allowing it to produce "a man riding a bike carrying a surfboard". This leads to richer, more complete captions. Empirically, object-based attention models have shown higher recall of objects (reflected in SPICE and higher CIDEr since references often mention those objects). Multi-head attention provides a more robust representation that can capture multiple attributes at once (color, action, object category simultaneously).
* **Interpretability:** Attention heatmaps from the model become more informative. With object-based attention, we can literally highlight boxes for each attended object per word, making a transparent connection between words and image regions. Even with grid features, we can overlay an attention map. Multi-head attention introduces complexity in interpretation (different heads might attend differently), but one can often average heads or select a representative head for visualization. Many transformer-based captioning papers show attention maps that align well with the described content (e.g., attending to “pizza” region when saying “pizza”) which builds trust in the model. Also, if we incorporate adaptive attention, we can interpret the gating decisions – e.g., see that the model turned “off” image attention when generating a non-visual word, which matches intuition.
* **Better Handling of Relationships:** By possibly including geometric attention or just by virtue of multi-head covering different spatial positions, the model might better express relationships (e.g., “next to”, “on”). If one head focuses on object A and another on object B, the decoder can learn to output a relational phrase between them. Some models explicitly incorporate relation-attention (e.g., **Object Relation Transformer** by Herdade et al., which we effectively cover by geometric encoding and multi-head). We expect these enhancements to help SPICE’s relation sub-score (which measures if relationships like “person on horse” are captured).
* **Technical Efficiency:** Multi-head attention and scaled dot product are highly optimized on modern hardware (especially with PyTorch’s xFormers or FlashAttention libraries). So even though it’s more complex than a single-head, it may actually be faster to run on GPU. This means we get these benefits largely “for free” computationally when using a transformer-based implementation.

In implementation, we will transition the model to use a Transformer decoder, which inherently includes scaled dot-product and multi-head attention. Thus, many of these attention improvements will come along naturally. For object-based attention, we will add an option to use region features as the encoder output and ensure the cross-attention attends over those. For adaptive gating or AoA, we will include them if needed as extensions on top of the basic Transformer decoder block (since they require a bit of custom coding).

Given that model size is not our primary concern, adding a few extra parameters for gates or additional attention layers is acceptable. We will monitor during training that these help and do not cause optimization difficulties (sometimes too much flexibility can make training slower to converge – e.g., the model might learn to turn off image attention entirely if not guided well; we must ensure the balance is right by maybe using appropriate initialization or slight regularization on gates to not degenerate).

In summary, moving from a single-head, single-pass attention to a **multi-head, multi-layer, and potentially object-grounded attention** will give the model a much stronger ability to dissect the image and associate parts of it with words. We anticipate captions that are **more detailed**, **more accurate**, and aligned with human-like attention patterns (e.g., focusing on salient objects). This sets the stage for leveraging modern decoders that can fully exploit these richer attention mechanisms.

## 3. Modern Decoder Architectures

Upgrading the decoder is a key step in modernizing the captioning system. The current decoder is an LSTM (possibly with BERT embeddings fed in, as noted). LSTMs, while effective, have limitations in capturing long-range dependencies and cannot easily leverage the parallel computation of GPUs for sequence generation. Modern decoders are largely Transformer-based, often pre-trained on massive text data. We will compare and consider several decoder architectures, focusing on how they improve caption fluency and integration with vision:

* **Transformer Decoder (Autoregressive)** – A Transformer decoder (as used in Vaswani et al. 2017 for translation) consists of a stack of self-attention and cross-attention layers with feed-forward networks. Using a transformer decoder brings multiple benefits: it can attend to the whole generated prefix (thanks to self-attention with causal masking) rather than just the last hidden state as an LSTM does, which gives it a richer representation of partial captions. It also enables **parallel processing** of sequence elements during training (processing all positions with masking in one forward pass, instead of sequentially as an RNN). For inference, it can cache keys/values and still achieve efficient sequential generation. We plan to replace the LSTM with a Transformer Decoder of, say, 6–12 layers. This alone has proven to boost captioning performance in many works – simply because the transformer is better at modeling language (especially longer or more complex sentences) and at multi-modal attention fusion.

  We can initially train this transformer decoder from scratch on COCO. However, transformers shine even more when initialized with **pretrained language model weights**. This leads us to consider options like GPT-2, BART, T5, etc. as the starting point. But before that, let's note integration: PyTorch and HuggingFace Transformers provide an easy way to do this via the `VisionEncoderDecoderModel` API. For example, we could instantiate a `ViTModel` as encoder and a `GPT2LMHeadModel` as decoder and get a combined model that handles cross-attention from the encoder to GPT-2’s layers. This gives us a quick path to try a GPT-2 based decoder.

* **GPT-2 / GPT-like Decoder:** GPT-2 is a unidirectional language model (decoder-only transformer) trained on huge web text data. It has learned to produce fluent, coherent English text and has a vast vocabulary and knowledge of language patterns. Integrating GPT-2 as our decoder (with cross-attention added) means our model starts with strong English generation ability. It should greatly improve the *fluency* and *diversity of phrasing* in captions, since GPT-2 can generate more varied descriptions than a model trained only on the relatively formulaic COCO captions. Research like **ClipCap** has shown that even without fine-tuning the LM heavily, using a GPT-2 to generate captions from CLIP image features is effective. We would, of course, fine-tune it on our data with cross-attention active to ensure it remains image-grounded.

  One challenge: GPT-2’s training was purely text, so it has no built-in cross-attention to images. The HuggingFace `VisionEncoderDecoderModel` addresses this by initializing new cross-attention weights in the GPT-2 layers (or by using an encoder-decoder architecture under the hood where GPT-2 acts as decoder). This might require a bit of careful tuning since those weights start random. However, since GPT-2’s self-attention and feed-forward weights are pretrained, the model should still have a strong language backbone.

  GPT-2 comes in sizes (small \~117M, medium \~345M, large \~774M, xl 1.5B). Even the medium or large could be feasible on a single high-end GPU with mixed precision. For a start, GPT-2 base (117M) might be too small to fully leverage, but GPT-2 large (774M) could significantly enrich generation (some captioning works use models of that order of magnitude). We will likely experiment with GPT-2 medium or large for a balance of performance vs memory.

  **Integration example:** Using HuggingFace, it’s as straightforward as:

  ```python
  from transformers import VisionEncoderDecoderModel
  model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(
      "openai/clip-vit-base-patch32", "gpt2-medium"
  )
  model.config.decoder_start_token_id = tokenizer.bos_token_id
  model.config.pad_token_id = tokenizer.eos_token_id
  ```

  This would create a CLIP-ViT-B/32 + GPT2-medium captioner. We would then fine-tune it on COCO. For our purpose, we might swap CLIP-ViT with a better encoder and GPT2-medium with GPT2-large, for instance.

* **BART and T5 (Encoder-Decoder Transformers):** BART (Lewis et al. 2019) is a transformer trained as a text autoencoder (it can be thought of as a combination of GPT and BERT – an encoder-decoder that learned to reconstruct corrupted text). T5 (Raffel et al. 2020) is another encoder-decoder transformer trained on a multi-task mixture (including translation, summarization, etc.) in a “text-to-text” framework. Both have shown excellent language generation and understanding capabilities.

  We can utilize the *decoder* part of these models for captioning. In fact, one could feed the image through some transformation to act as “encoder input” to these models. For example, we could map image features to a sequence of “pseudo-tokens” and feed them into T5’s encoder, and then let T5’s decoder produce the caption. This is the approach taken by models like **Oscar** and **VinVL** (they fed object tags as keywords into BERT or encoder-decoder models). Alternatively, as mentioned, we can do the easier thing: use the VisionEncoderDecoderModel with BART or T5 as the decoder, which sets up cross-attention from our image encoder to the BART/T5 decoder.

  **BART:** It has a vocab of \~50k BPE tokens and is very fluent. It’s good at summarization; captioning is similar in that it must distill visual content into a sentence. We could initialize the decoder with `facebook/bart-large` (400M params). BART-large has 12 decoder layers, which might be more than needed for short captions, but more layers can model more subtle patterns. We will fine-tune it on COCO; since BART was pre-trained on a lot of text (mostly news, books, etc.), we expect it to handle grammar and synonyms well. It might even use richer vocabulary than typical captions (which could be good for diversity, if not going out of domain too much).

  **T5:** T5 comes in sizes (Small 60M, Base 220M, Large 770M, 3B, 11B). T5-large (770M) or T5-3B would be potent decoders. Google’s **Flan-T5** is a variant fine-tuned on instructions, which might yield even more descriptive style. BLIP-2 used a FlanT5-XL (3B) as the language model with success. Using something like Flan-T5 large or XL as our decoder could allow extremely fluent and even instruction-following style captions (which might be overly verbose, but we can control style via training). T5 is "text-to-text", which means even tasks like classification are cast to text. For us, it means it’s very flexible. We would provide an image and maybe some prompt (like “Caption:” token) and let T5 generate. But the simplest is still hooking image features into its cross-attention and treating it like a normal decoder.

  The integration challenges for BART/T5 are similar: they expect an encoder output. HuggingFace’s `VisionEncoderDecoderModel` likely wraps an encoder-decoder architecture. If we specify `decoder_pretrained_model="t5-large"`, it creates a T5 encoder and decoder by default. We might actually consider *using T5’s encoder as well*, feeding image features into it after some projection. Another approach: use a simplified encoder (like a single linear layer) to map image features to the T5 encoder input space (d\_model=1024 for T5-large) and then rely on the T5 decoder. This effectively uses T5 as a fancy language model with cross-attention.

  One must note: T5 uses a sentencepiece vocabulary and expects input in a certain format. We will need to handle tokenization accordingly (HuggingFace does that easily). Also, T5’s training includes a lot of general knowledge – it might inject more world knowledge into captions (sometimes good, e.g. recognizing famous landmarks if in image; sometimes risky, e.g. making assumptions).

* **Flamingo (Vision-Language Few-Shot Learner):** DeepMind's Flamingo (2022) is a specialized architecture for few-shot learning with vision and text. It keeps a large language model (like 70B Chinchilla) frozen and inserts learned “gated cross-attention” layers that allow vision features to condition the text generation. Flamingo is incredibly large and not publicly available in full, but conceptually it shows a way to leverage a **frozen LLM** for image captioning by only training a small adapter. Our scenario likely doesn’t involve such a giant model due to computational constraints. However, we can borrow ideas:

  * Use a **frozen decoder** (like we could keep GPT-2’s weights mostly frozen except cross-attention layers, or keep T5 frozen) and only train the minimal interface (like Flamingo’s gated attn or BLIP-2’s Q-Former, see section 5). This would drastically cut training cost and avoid overfitting small data, since the language model stays general. We would then rely on the pretraining to carry language fluency, and just adjust how it “sees” the image.
  * If a smaller open Flamingo reproduction becomes available (LAION released a preliminary OpenFlamingo with 9B parameters), we could consider fine-tuning that. But that’s still heavy, and OpenFlamingo is under a non-commercial license at the moment.

  For our modernization, we likely won’t implement Flamingo directly (since it's huge), but it influences our thinking on **frozen LLM + tiny adapter** approach that we discuss more in multi-modal fusion (section 5 with BLIP-2 style).

* **Perceiver IO:** Perceiver IO (DeepMind 2021) is a modality-agnostic model that uses a latent array with cross-attention to ingest inputs and produce outputs. It has been used for multimodal tasks including visual QA. Its strength is handling very high-dimensional input (like large images or video frames) by compressing via cross-attention into a fixed small latent space. For captioning, one could feed image patches into PerceiverIO and then decode text from the latents. While intriguing, PerceiverIO might not outperform a well-tuned ViT+Transformer decoder on this task, especially given we have strong pretrained alternatives. It also might be a bit complex to integrate with pre-trained pieces (though one could pretrain PerceiverIO on image-text data from scratch). Considering time and that other established methods are available, we will not focus on Perceiver for now. It remains a candidate for future exploration, especially if we aim to unify various tasks or if a pretrained Perceiver for images becomes available.

**Integration Challenges and Adaptation Techniques:** When replacing the LSTM with a Transformer-based decoder or a pre-trained model, we face a few challenges:

* **Vocabulary and Tokenization:** The LSTM model likely used a word-level vocabulary limited to COCO words. GPT-2, BART, T5 use subword tokenization (Byte-Pair or SentencePiece). We will adopt the tokenizer of the chosen LM for our model. This means our training captions must be tokenized into those subwords. It’s beneficial because it handles rare/new words gracefully (breaking them into subwords). We need to ensure that at inference we can map the generated subwords back to readable text (the HF libraries handle that). Also, if the COCO dataset contains words that the pretrained model’s vocab doesn’t include as a single token (like some proper nouns or onomatopoeia), it will just split them – not a problem. If COCO has very domain-specific words (not really, it’s everyday language), the LM might not have seen some (like maybe some MS-COCO specific terms? Unlikely).

* **Special Tokens:** We should define what token signifies the start and end of a caption. For GPT-2, there's no special start (it typically expects a prompt). We can set the BOS token as the start of generation and EOS as the end. In training, we will likely prepend a start token and append an end token to each target sequence. We also need to handle padding in batches properly and ensure the model doesn’t generate beyond the end token.

* **Cross-Attention Initialization:** If we use something like `GPT2LMHeadModel.from_pretrained`, the model has no cross-attention layers by default. The VisionEncoderDecoder pipeline may under the hood convert GPT-2 into an encoder-decoder architecture by adding cross-attention weights. Alternatively, one could take a GPT-2 model and manually insert cross-attention modules in each layer. HuggingFace actually has a class `GPT2Model` with `add_cross_attention=True` option which does exactly that (used for tasks like summarization where GPT-2 can attend to an input). We will leverage those tools so we don't have to implement it from scratch. The newly added weights (queries from decoder to keys of encoder) will be randomly initialized, so early in training the model might rely mostly on its language model prior before learning to use the image. We might consider strategies like **gradual unlocking** – e.g., first train the cross-attention layers lightly while keeping LM weights fixed or at a lower lr, then fine-tune everything – to stabilize training. This is something to experiment with.

* **Dimension Matching:** The image encoder output dimension must match the decoder’s expected encoder-hidden-state size for cross-attention. For example, if we use ViT-B/16 (768-dim output) and GPT-2 medium (1024-dim hidden), we need a linear projection from 768 to 1024 for the encoder output before feeding to cross-attn. The VisionEncoderDecoder handles this by likely having an `encoder.config.hidden_size` and `decoder.config.hidden_size` – if different, it adds a projection layer internally. If doing manually, we’ll add an `nn.Linear(768,1024)` to the encoder output. We should confirm that shapes line up; any mismatch and cross-attn won’t work.

* **Training with Pretrained LM:** These large LMs can easily overfit small data or forget their general language if trained with an inappropriate learning rate. A common adaptation technique is **two-stage training**: first, freeze the LM and only train the encoder and cross-attention (so the model learns to inject visual info into the LM without altering its language generation too much), then unfreeze more and fine-tune gently. Another is using a **lower learning rate** for the LM weights and higher for new layers. We will likely use an optimizer that allows per-parameter groups with different lr’s (HuggingFace’s Trainer or manual PyTorch loop). For instance, cross\_attn and encoder parameters at lr=1e-4, GPT-2’s own weights at 1e-5.

* **Memory and Speed:** A 774M decoder and a big encoder can be heavy. We’ll use mixed precision (float16) to reduce memory. We might also employ gradient checkpointing on the decoder (HuggingFace models often have a flag for that) to save memory at compute cost. PyTorch 2.0’s `torch.compile` should help speed, as will using optimized kernels for self-attn (FlashAttention can be considered if needed). Since our deployment is on GPU, this is acceptable, but we should be aware inference will also be heavier (we address inference strategies in section 8).

* **Sequence Length:** COCO captions are usually < 20 tokens (words). Transformer LMs are trained on much longer contexts (up to 1024 tokens for GPT-2). So our use is well within their capability. No need to worry about positional embeddings length. But one note: these models might be prone to continue generating beyond what we want, because they were trained to not stop until sentence end or max length. We ensure to enforce an end token. They also might insert unnecessary punctuation or quotations if they misinterpret the task format (less likely if we fine-tune well). We should train them with a straightforward format: no additional prompt text, just begin generation at start token given the image context. That keeps it simple.

* **Replacing LSTM Implementation:** Our code will drop the old LSTM-based decoder entirely and use the HuggingFace transformer or a custom Transformer. This simplifies certain things (HuggingFace will handle a lot of complexity like caching for us). We will need to integrate with our training loop and ensure that loss is computed correctly (for sequence generation we usually mask out the loss beyond EOS and for padded tokens). We’ll likely use cross-entropy loss on the decoder’s output distribution at each time step.

**Implementation Templates:** A couple of snippets for clarity:

* *Using HuggingFace:*

  ```python
  from transformers import VisionEncoderDecoderModel, AutoTokenizer
  # Suppose we choose ViT-B/16 and GPT-2 Large
  model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(
      "google/vit-base-patch16-224", "gpt2-large"
  )
  tokenizer = AutoTokenizer.from_pretrained("gpt2-large")
  # Important: GPT2 doesn't have EOS token by default in its tokenizer; add if missing
  if tokenizer.eos_token is None:
      tokenizer.add_special_tokens({'eos_token': '<|endoftext|>'})
      model.decoder.resize_token_embeddings(len(tokenizer))
  model.config.eos_token_id = tokenizer.eos_token_id
  model.config.pad_token_id = tokenizer.eos_token_id
  model.config.decoder_start_token_id = tokenizer.bos_token_id or tokenizer.eos_token_id
  # The model is now ready to train; it will handle cross-attention internally.
  ```

  During training, we will feed images and input/output sequences (with appropriate shifting for teacher forcing). The HuggingFace `VisionEncoderDecoderModel` expects inputs like `model(pixel_values=image_pixels, labels=caption_ids)` and it will do the rest, returning a loss. This is very convenient for initial implementation.

* *Custom approach:* If we go lower-level (for flexibility):

  ```python
  encoder = ViTModel.from_pretrained('google/vit-base-patch16-224')
  decoder = GPT2LMHeadModel.from_pretrained('gpt2-large', add_cross_attention=True)
  # Add cross-attn will insert cross-attn layers in each decoder block.
  # Freeze some parts if needed:
  for param in decoder.base_model.parameters():
      param.requires_grad = False  # e.g., freeze all except cross-attn
  for block in decoder.transformer.h:  # each decoder block
      block.crossattention.requires_grad_(True)  # unfreeze cross-attn sublayers
  # Then create a CaptionModel that calls encoder and decoder accordingly.
  ```

  We’d also have to manually manage the fusion of encoder output: ensure `encoder_outputs.last_hidden_state` is passed to `decoder`’s `encoder_hidden_states` in forward. This is what HF’s VisionEncoderDecoder does behind the scenes.

  Given the maturity of HF models, we will likely use them to avoid reimplementing training loops for cross-attention etc. That also means we get the benefit of pretrained weights easily.

* **Expected Improvements with Modern Decoders:**

  * *Semantic and Syntactic Fluency:* A transformer decoder or pre-trained LM will produce more human-like sentences. It will handle grammar (plural/singular agreement, proper article usage) much better than an LSTM that only saw limited data. It will likely be able to rephrase sentences in multiple ways (which is good for diversity if we sample). For instance, an LSTM might always say “A group of people standing next to each other,” whereas GPT-2 might sometimes say “Several people stand side by side” or “a crowd of people gathered.” This variability comes from its richer training. Also, it will avoid some common RNN issues like sometimes repeating phrases or getting stuck in a loop (transformers can repeat too, but the training and large corpus reduce this).
  * *Longer Captions and Detailed Descriptions:* If desired, a transformer can generate longer outputs capturing more detail (we might need to adjust training to not always stop at the first thought). COCO captions are somewhat short by design, but in other datasets or applications, a larger decoder could produce multi-sentence descriptions. We might experiment with length by not cutting off at one sentence if the model wants to say more and see if it still remains relevant. Metrics like CIDEr and SPICE typically evaluate one sentence, but from a product perspective, sometimes more detail is better (depending on requirements).
  * *Knowledge and Vocabulary:* Pretrained LMs bring world knowledge. For example, if a landmark is in the image (e.g., Eiffel Tower), a vanilla model might say “a tall tower” while a pretrained LM might know the iconic structure and name it (if our dataset had examples or if CLIP recognized it and cross-attn passes the clue). They also have larger vocabularies, so instead of saying "vehicle" generically, the model might say "truck" or "ambulance" if appropriate. However, caution: the model could also hallucinate names (like calling a random building “the White House” when it’s not) if not properly grounded. Ensuring strong visual grounding via cross-attention and maybe CLIP features mitigates that. The language model’s knowledge is a double-edged sword – great for completeness, risky for hallucination. Training on actual image data mostly tunes it to not hallucinate beyond what's plausible in images.
  * *Reduced Training Data Need for Language:* Since the LM has learned English, we don’t need to “re-teach” it basic syntax with our small dataset. We primarily need to teach it to describe images briefly and factually. This makes data usage more efficient – effectively **transfer learning** from billions of text tokens. We might see faster convergence in terms of language metrics (like BLEU) because the baseline is already good.
  * *Integration with Embeddings (Section 4):* If we incorporate BERT or other embeddings as additional features, a transformer decoder can easily accept them (as additional input tokens or as context vectors). But if we go with a large pre-trained LM, we might not need separate BERT embeddings because the LM’s own embeddings are already powerful. However, we could consider using CLIP text embeddings or detected object label embeddings appended as context tokens to the decoder input (some models do: provide the names of detected objects as “hints” to the decoder). A GPT-2 or T5 could handle such prompts, potentially improving object coverage. This bridges into multi-modal fusion (section 5).

To wrap up this section: We will likely implement two decoder setups for experimentation:

1. **Transformer Decoder trained from scratch on COCO** – as a baseline modern architecture (to isolate improvements due to architecture alone).
2. **Pretrained LM as Decoder (e.g., GPT-2 or BART)** fine-tuned on COCO – to measure the gain from transfer learning.

We expect both to outperform the LSTM significantly, with the pretrained one giving the best results in fluency and possibly CIDEr. We will ensure modularity such that switching the decoder type is easy (the training loop and encoder remain largely the same; only the decoder class changes). This modularity also future-proofs us: if a new state-of-the-art language model (e.g., GPT-3 sized but open-source like Bloom or OPT) becomes feasible to fine-tune, we could integrate it similarly.

Finally, regarding **licensing**: GPT-2, BART, T5 are all open-source friendly (MIT or Apache licenses). Using them in our system (even for commercial) is fine, just need to acknowledge. We will use HuggingFace which provides model weights and their usage terms (GPT-2 is MIT license as per OpenAI; BART is fairseq MIT; T5 is Apache). So no blockers.

## 4. Language Model Embedding Options

The current system mentions optional integration with BERT embeddings. That implies they may have been feeding BERT token embeddings or contextual embeddings into the caption decoder. We will analyze modern embedding choices and how they can enhance the captioner's language understanding:

* **Pretrained Contextual Encoders (BERT, RoBERTa, DeBERTa):** These models (BERT by Devlin et al. 2018, RoBERTa by Liu et al. 2019, and DeBERTa by He et al. 2021) are powerful at producing contextual representations for each token in a sentence. One way to use them in captioning is to utilize their embeddings as input features to the decoder. For example, instead of learning word embeddings from scratch, one could input the caption words into BERT and take either the last-layer or some middle-layer embeddings as the sequence that the decoder RNN attends to or uses for output prediction. In an LSTM-based model, one could feed these embeddings as the input at each timestep (replacing learned embeddings). In a transformer decoder model, one could similarly initialize the token embedding matrix with BERT's or even concatenate BERT's contextual output with the transformer's own layers (kind of like a two-stream decoder).

  However, using BERT during inference is heavy, and it somewhat overlaps with what a transformer decoder does by itself. The original use in our system might have been to leverage BERT’s understanding of word semantics to help the LSTM (which otherwise sees limited data). With our approach of using a pretrained transformer decoder, we inherently have contextual language knowledge, possibly reducing the need for an auxiliary BERT.

  Still, let's consider **embedding quality**: BERT’s embeddings are **context-sensitive** – e.g., the word “bank” in “river bank” vs “finance bank” will have different vectors. If we just use BERT’s static token embeddings (the input embedding matrix), that’s less useful than using its contextual output. But to get context outputs, we’d have to run BERT on the entire caption (which can be done during training when the whole caption is known, but not easily during inference as context is building). If we only use BERT’s output for full sentences after generation, that could be used to re-rank or compute a loss (like BERTScore, which we discuss in evaluation). But feeding it into generation step-by-step is non-trivial unless we perform at each step (which is too slow).

  A simpler approach if we wanted to infuse BERT’s semantic knowledge is **fine-tune** the decoder to minimize a distance between its hidden states and BERT’s hidden states for the same ground-truth caption (knowledge distillation). Some research did this: forcing the caption model’s representations to align with BERT or RoBERTa representations of the reference caption, thus inheriting some linguistic properties. This can improve metrics like CIDEr a bit, as shown by the "BERTScore loss" or similar in some works.

  **RoBERTa** is an optimized BERT (trained longer, no next-sentence task). **DeBERTa** further improves on BERT with disentangled attention (content vs position) and an enhanced mask decoder, achieving state-of-the-art on many NLP tasks. Using DeBERTa’s embeddings could theoretically provide even better contextual cues. But the complexity of integrating a big BERT-like model into training might not be worth the incremental gain if our decoder is already a large LM.

* **GPT Embeddings:** Instead of an encoder model’s embeddings, one can use a generative model’s internal embeddings. But typically, if we use GPT-2 as decoder, we automatically use its embeddings for our words. Those embeddings were trained on lots of text, capturing semantics (e.g., synonyms have similar vectors). We likely don’t need an external embedding in that case. If we for some reason used a scratch decoder, we could consider initializing its word embedding matrix with GloVe or word2vec or something. But using subword models like BPE makes that less straightforward (though one could average GloVe vectors for subwords or similar – rarely done, better to rely on the model’s own learning or a full LM’s weights).

* **T5 or BART Embeddings:** These would similarly bring their knowledge if used as part of the model. If we, say, didn’t use the full T5 model but wanted its embedding layer, note that T5’s embedding is tied with its encoder/decoder input embedding (they share, and also tied with output softmax). Using it alone might not unlock full power without the self-attention architecture that goes with it.

* **CLIP Text Embeddings:** CLIP has a text encoder that maps text to the same space as images. One could try to ensure the caption model’s text features are aligned with CLIP’s text embeddings for the same caption. For example, a loss that the decoder’s end-of-sequence hidden state is close to CLIP’s embedding of the reference caption. This would encourage the model to generate sentences that are not only similar to references but also *CLIP-similar to the image* implicitly (since CLIP text embedding alignment means image-text alignment). In fact, one could incorporate CLIP text encoder in a two-stream model: use CLIP text encoder to encode partial captions during beam search and CLIP image encoder to compute similarity to guide search (that’s more of an inference reranking approach – see evaluation). As an embedding option, one could feed CLIP text embeddings of certain keywords into the decoder. For instance, feeding in CLIP embedding of “dog” vs “cat” might help pick correct word if the image embedding is closer to one or the other – but this is a bit contrived since the model can learn that mapping itself.

  Another possible usage: Use CLIP’s vocabulary of concepts. Some advanced captioning systems generate not just words but also incorporate a step of predicting a set of image tags and then generating a sentence from them. If CLIP is used to predict a set of prominent words from the image (via cosine similarity between image embedding and text embeddings of many words), we could use those words as auxiliary inputs. E.g., "Tags: beach, ocean, sunny" and then the caption might incorporate or at least be informed by those. That becomes a multi-modal fusion or multi-task approach.

* **Comparing Representations and Resource Needs:**

  * *Quality:* BERT/RoBERTa/DeBERTa capture deep semantic relationships (they’ve been used to evaluate captions via BERTScore because of this). If our decoder is not pretrained, adding these could drastically help it avoid weird phrasing or incoherence. But if our decoder *is* a pretrained LM, it already has that knowledge.
  * *Resource Requirements:* Running a BERT-large for each caption generation step is not feasible for real-time (it’s \~24 layers, 340M). Even BERT-base at each timestep would slow inference a lot. If using it only offline (e.g., for training as feature or loss), that’s manageable. DeBERTa-large or RoBERTa-large are similarly heavy. DistilBERT (smaller distilled model) could be a lighter alternative if we really wanted run-time embeddings – but DistilBERT’s benefits might be overshadowed by just training the decoder more.
  * *Licensing:* BERT is Apache 2.0 – no issue. RoBERTa (from Facebook) I believe has CC-BY for weights (which is fine, just attribute). DeBERTa v3 is MIT. GPT and T5 we already covered (both open). CLIP’s text encoder from OpenAI has the same license note as its vision (non-commercial caution), but OpenCLIP’s text models (which are essentially the same architecture) are MIT. So if we use CLIP’s text enc or its outputs, we can rely on OpenCLIP implementation.

* **Open-Source Availability:** All these models are available on HuggingFace model hub. BERT-base/large, RoBERTa-base/large, DeBERTa, etc. So integrating them is straightforward in code. It’s more a design decision whether to use them inside the caption model or just for evaluation.

**Our Approach Regarding Embeddings:** Given we plan to use a pretrained transformer decoder (GPT-2 or T5), that inherently provides strong embeddings. Thus, we might **not need a separate BERT** feed-in. If we were sticking to a smaller custom transformer decoder trained from scratch, we could consider *initializing its token embeddings* from GloVe or word2vec to give it a head-start on word semantics. Many caption models in late 2010s did initialize word embeddings with GloVe (Common Crawl vectors) for faster convergence. But with subword tokenization, it’s less common now. Also, large corpora have been absorbed by these pre-trained models anyway.

One thing to consider: **specialized vocabulary**. COCO captions have some onomatopoeic words (like "motorcycle vroom", rarely), or the style often omits pronouns (they don't usually say "there is a ..."). A pretrained LM might sometimes produce "There is a cat on the bed." because that’s natural English, whereas COCO references might say "A cat on a bed." It’s not a big problem if the metric scoring doesn’t penalize "There is". But some metrics might count it as extra words. We can either fine-tune the LM to match that style or even modify generation to drop "There is" if it appears (some research has done that to mimic reference style).

**Other Embedding Types:** There was work on using **attribute embeddings** – like detect attributes (color, material) and feed those. Or use **wordnet/hierarchical embeddings** to help model generalize (like embedding "dog" and "animal" such that model can relate if needed). These are mostly outdated by contextual models now which inherently learn those relations.

**OpenAI’s Transformer Insertion:** One crazy option: Use something like GPT-3 or ChatGPT via API to re-rank or refine captions. But that's not an embedding integration, more like a post-processing (e.g., "ChatGPT, rewrite this caption to be more detailed."). It's outside our current scope (and not fully controllable offline, plus licensing).

**Licensing Recap:** Using BERT or others would require including their license in our documentation. All are permissive, so no conflict. If we did use e.g. Google's T5, we also should note it’s Apache (which is fine).

**Conclusion on Language Embeddings:** We will rely on **pretrained decoders** for our main model, which incorporate high-quality embeddings. We might not explicitly integrate BERT or RoBERTa into the generation pipeline. Instead, we might use such models for **evaluation metrics (BERTScore)** or **loss functions** (e.g., a cosine similarity in BERT embedding space between generated and reference, as an auxiliary objective to encourage semantic similarity beyond n-grams).

However, if resources allow, we could try an experiment: train a caption model where at each decoding step, the hidden state is concatenated with the corresponding BERT-base contextual embedding of the partial sentence (which means running BERT up to that word). This was not common, because it’s heavy and partial sentence BERT embedding might not add much beyond the decoder’s own hidden state (which is supposed to encode the partial sentence). Likely more fruitful is to use BERT as a teacher for the final sentence representation as mentioned (some call it “semantic supervision”).

**Component to keep an eye on:** If we end up not using BERT explicitly, we should remove or refactor whatever part of current system integrated BERT embeddings. Possibly they just replaced the word embedding matrix with BERT’s. If that’s the case, that’s a simple one-time initialization that we can do or not do. In our new system with subword tokens, we wouldn’t do that.

In summary, our plan:

* Use the **decoder’s own embeddings** (coming from GPT-2 or T5) as they are already rich.
* Possibly incorporate a **CLIP-based loss or reward** to align text and image (discussed in training/evaluation).
* Possibly use **BERT/RoBERTa in evaluation** (for metrics like BERTScore or for human eval assistance).
* Ensure no licensing issues by sticking to open models.

## 5. Multi-Modal Fusion Techniques

A critical aspect of image captioning is how to **fuse visual and textual information** effectively. Beyond the basic architecture (encoder provides features, decoder attends to them), there are advanced fusion techniques that can improve alignment and representation learning. We examine several:

* **Co-Attention (Two-way attention):** As touched on earlier, co-attention involves **bi-directional interaction** between image and text features. Instead of a unidirectional “attend image from text” (which a decoder does), co-attention modules allow image features to also attend to text features. This was popular in VQA models (like **BAN, DCN**, etc.) and in some captioning models where they refine image features based on partial captions. One architecture is to have a **multimodal Transformer encoder** that takes both modalities: e.g., as in the **Oscar** and **Unified VLP** models, they input object region features and their detected labels together into a transformer. Another architecture is the **Parallel Co-Attention Network** – separate image and text branches that have cross-attention at each layer (like LXMERT did). For captioning, during generation, text is growing, so a full symmetric co-attention is tricky in real-time. However, one can apply co-attention in a **preprocessing or pre-fusion step**: for example, before decoding the first word, run a few rounds of co-attention between image features and an embedding of the image’s predicted keywords or the BOS token, to produce a fused representation that carries alignment information.

  A practical method for us, influenced by recent **Vision-Language Pretraining (VLP)** models:

  * Use a **multimodal encoder (MME)**: This would be a transformer that takes image region features plus (possibly) some placeholder text tokens (or tags). For instance, ALBEF and others use one transformer where half of the input is image patches, half is text tokens (like the caption words, either ground truth for pretraining or generated for downstream). They train that multimodal encoder with objectives like masked language modeling and image-text matching. After such pretraining, the multimodal encoder can be used to initialize a captioning model’s encoder and cross-attention layers.
  * Another approach is the **Mixture-of-Encoder-Decoder (MED)** from BLIP. BLIP’s architecture had an image encoder, a text encoder (for understanding tasks), and a text decoder (for generation tasks), with a clever scheme to switch between them. But the key is they allow both understanding (via an encoder) and generation (via a decoder) with shared vision backbone.

  Concretely, for us: we could incorporate co-attention by adding a few **“fusion layers”** after the vision encoder and initial text embedding. Perhaps the simplest: After encoding the image, and before the decoder, insert a cross-modal transformer encoder that takes image features and a learned “query” embedding (or multiple queries which could represent the latent caption embedding). This is similar to BLIP-2’s **Q-Former**. BLIP-2 froze the image encoder and the big language model, and trained a small Q-Former that has 32 query tokens which attend to the image and produce 32 embeddings that then condition the LLM. This can be seen as a co-attention: the query tokens gather information from image (like visual words). We could adopt this idea: have a set of learned query vectors (size maybe 10-20) that do cross-attention on the image encoder output (like region features or patch tokens). These query embeddings after training will represent an “abstract” of the image in a form digestible by the text decoder. We then feed those as input to the decoder (either as prefix tokens or via cross-attention). BLIP-2 found that keeping the image encoder and decoder frozen and just training this Q-Former is very efficient and still effective.

  Implementing a Q-Former: it’s essentially a mini-transformer with self-attention among queries and cross-attention from queries to image features. We can integrate something similar by either using BLIP-2’s public implementation for Q-Former or writing our own. If we use it, we should note BLIP-2’s license (BSD) and that it uses CLIP image features by default (which we also plan to use possibly).

* **Gated Fusion:** Beyond attention, another mechanism is using **gating or Hadamard products** to combine modalities. For example, **FiLM (Feature-wise Linear Modulation)** layers (Perez et al. 2018) use a neural network (often based on text) to generate scaling and shifting coefficients that modulate the visual feature maps. In captioning, one could imagine the partial text embedding generating a gate to apply to image features or vice versa. Lu et al.’s adaptive attention is a kind of gated fusion (it gates between visual context vector and an LSTM memory).

  We can incorporate gating in simpler ways:

  * **Modality Gate:** Learn a scalar or vector gate \$\mathbf{g} = \sigma(W \[h\_t; v\_t])\$ that weighs how much of the visual feature \$v\_t\$ vs. textual state \$h\_t\$ to use at that step. Then the decoder output could be \$o\_t = g \* v\_t + (1-g) \* h\_t\$ (with maybe a linear layer after). This is akin to adaptive attention where \$h\_t\$ is the "visual sentinel" carrying linguistic info.
  * **Early Fusion vs Late Fusion:** We can fuse image and text features at various stages. Early fusion means combine them before feeding into a joint model (like concatenating image region features with word embeddings as one sequence for a transformer – which is effectively what Oscar did by adding object tags into the sequence). Late fusion might mean combine outputs of separate models (like take an image description from one channel and refine with text channel – not usual in captioning though).

  Gated fusion ensures that if the image is not informative for some part (or the model is confident from language alone), it can reduce image influence, and vice versa. This can help avoid issues like overstating something that the language prior might cause (e.g., captioner might generically say "on a sunny day" often; a strong image gating could suppress that if image features don't support "sunny").

* **ALBEF / CLIP-based Fusion:** **ALBEF (Li et al. NeurIPS 2021)** introduced a training strategy: Align **BEFORE** Fuse. They first align image and text encoders via a contrastive loss (making their CLS embeddings close for matching pairs), then fuse with a multimodal transformer. The key idea is to get a head start by contrastive alignment so that cross-modal attention in the fusion stage doesn’t struggle to relate features. We could mimic this by using CLIP’s aligned encoders (which are already aligned). In essence, if we use CLIP image encoder and CLIP text encoder (for say tags or something) our modalities start in a shared space. ALBEF also used **momentum distillation** – they maintained a moving average (slow copy) of the model to generate soft targets for image-text contrastive learning, improving robustness to noisy data. That’s more relevant for pretraining on web data. For our supervised setup, it may not apply strongly, but it’s something to note if we fine-tune on larger datasets with noise.

  ALBEF’s architecture:

  * Image encoder (CNN or ViT),
  * Text encoder (BERT-based),
  * Multimodal encoder (Transformer that takes both).
    They generate captions by feeding \[MASK] tokens to the text side and doing MLM (masked language modeling) – not exactly an auto-regressive decode, but a fill-in approach (SimVLM similarly used prefix language modeling). We can glean from ALBEF that using a **contrastive loss** (we’ll discuss in training) along with generative loss can improve fusion – because it forces the model’s learned embedding spaces to be meaningful for cross-modal retrieval, not just generation.

* **BLIP (Bootstrapping Language-Image Pretraining, 2022):** BLIP-1 uses a **Mixture of Encoder-Decoder (MED)** architecture. It has:

  * An image encoder,
  * An *optional* text encoder for understanding tasks,
  * A text decoder for generation tasks,
  * A strategy to switch between them depending on the task.
    For captioning, it uses the decoder. For image-text retrieval or VQA, it could use the encoder or both. BLIP’s innovation was also in training: it had a **Captioner** model and a **Filter** model. They generated captions for web images and filtered out noisy ones by checking image-text alignment (creating their own pseudo-ground truth to fine-tune on). That’s beyond our current scope, but interesting for possibly using large unlabelled data.

  **BLIP-2** (2023) we discussed: it’s all about frozen encoders (image & text) and a learned Q-Former bridging. That’s a powerful fusion: using the strengths of CLIP and a frozen LLM (FlanT5) with a trained bridge yields SOTA on many tasks zero-shot. For our design:

  * We can certainly adopt the concept of a small trainable multimodal transformer (the Q-Former) to connect a frozen image encoder and a frozen text decoder. This drastically reduces training cost and can leverage very large decoders (since we don’t train them). If we freeze CLIP ViT-L and, say, freeze GPT-2 large, and just train a Q-Former of, e.g., 2–4 layers with maybe 32 query vectors, we might get excellent results with much less data. BLIP-2 shows that even with 1–2M training pairs from CC, they beat models that were fully fine-tuned with more data, thanks to the powerful frozen components.
  * The downside of freezing: we might not fully adapt to the style of COCO (e.g., GPT-2 might produce longer captions with more flowery language or certain constructions that are uncommon in COCO). But a bit of fine-tuning of the LLM might be needed to adjust style if desired.

  We can try a compromise: freeze most of the LLM except maybe the final linear and layer norms (so it can adjust output distribution slightly).

* **Contrastive Models (CLIP, ALIGN, CoCa):** We should mention **CoCa (Contrastive Captioner)**, a 2022 Google model that combined a contrastive loss (like CLIP) with a captioning loss in one architecture. CoCa’s architecture was basically an encoder (ViT-g) and a decoder (transformer) attached. During training, the encoder’s CLS went to a contrastive head against text embeddings (like CLIP), and the decoder produced caption text (like usual). This multi-task training achieved SOTA results (it effectively did both image-text alignment and captioning).

  For fusion, it means the model’s encoder learns to produce features good for both matching and generation. We can incorporate the spirit of CoCa by including a **contrastive alignment objective** in our training (train image encoder and decoder such that the image embedding and the final caption embedding are pulled together for the right pair, and apart for wrong pairs). This doesn’t change architecture, but influences how the fusion learns to represent concepts. We’ll discuss training methodology for that.

* **Code-Level Fusion Examples:**

  * *Fusion as Extra Inputs:* For example, we can feed object tags detected into the decoder. Implementation: detect objects with a detector (or use ground truth labels in experiments), then append those words (or a summary sentence like "objects: cat, couch") to the beginning of the decoder input. The decoder then gets a prompt that might help it mention those. This was done in **Oscar** model, which added object tags as input and also as a loss (predict tags). Oscar improved CIDEr and SPICE by ensuring object names are covered. We could replicate something like this by leveraging an object detector or even CLIP to generate tags.

  * *Pseudo-code for Co-Attention:* If we had a two-stream:

    ```python
    # Given image features (img_feats) and embedded text tokens (txt_feats)
    # One layer of co-attention:
    # Image attends to text
    img_attn = MultiHeadAttention(query=img_feats, key=txt_feats, value=txt_feats)
    # Text attends to image
    txt_attn = MultiHeadAttention(query=txt_feats, key=img_feats, value=img_feats)
    # Update features
    img_feats = Norm(img_feats + img_attn)
    txt_feats = Norm(txt_feats + txt_attn)
    ```

    Stacking that would allow iterative refinement. This would be akin to parts of LXMERT. But during captioning, text part grows, so one might do this co-attention only once after generating a draft or using teacher-forcing at training (with ground truth text). In our design, a better approach is unify them in one transformer as earlier.

  * *Pseudo-code for Q-Former style fusion:*

    ```python
    # img_feats: [N_regions, D]
    # Initialize M learnable query embeddings
    queries = nn.Parameter(torch.randn(M, D))
    for layer in range(L):
        # Self-attend among queries
        queries = SelfAttn(queries)
        # Cross attend: queries as query, image feats as key/value
        queries = CrossAttn(query=queries, key=img_feats, value=img_feats)
        # maybe feed-forward, etc.
    # After L layers, 'queries' contains M fused features summarizing image.
    # Use these as prompts to decoder or as context via cross-attn to decoder.
    ```

    We would train those query parameters and the attention layers from scratch (or initialize from some distribution). BLIP-2 did something akin to that with 32 queries and 6 layers, if I recall correctly.

  * *Pseudo-code for gating:*

    ```python
    # Suppose we have current decoder hidden state h_t and image context vector v_t from attention
    gate = torch.sigmoid(W_h @ h_t + W_v @ v_t)  # elementwise or scalar
    fused = gate * v_t + (1 - gate) * h_t
    ```

    Then `fused` goes through output layer to predict word. We can make W\_h, W\_v learnable (maybe biases to encourage initial setting like gate=1 to start with image then adjust). This is a simple addition that could be added to a transformer decoder's cross-attention output step.

* **Visualization Strategies for Fusion:** When we have these complex fusion approaches, visualizing how the model is working is important:

  * We can visualize cross-attention heatmaps as usual (for each generated word, highlight image regions).
  * If using co-attention, we can also visualize what image attends in text and vice versa. For instance, in a co-attention layer we can see which words (or subwords) an image region was most strongly attending to. During training with ground-truth text, this could reveal alignments (like region of "dog" attends mostly to the token "dog" in text).
  * For models like Q-Former, since it outputs a fixed number of queries, we might try to interpret each query’s role – maybe one query consistently represents “background context”, another “main subject”, etc. We can see which image patches each query attends to (by looking at cross-attn weights). In BLIP-2, one could see, e.g., query 5 is focusing on faces if present, query 2 on context, etc. That gives some interpretability to the internal representation.
  * We can also visualize gating values over time: see if the model is indeed using high gate (relying on image) for content words and low gate (relying on its own language memory) for filler words or very generic phrases.
  * Another approach is to visualize learned embedding spaces: e.g., plot image features and text features in the same space (if we enforce alignment). If well-aligned, embeddings of matching image and caption will cluster. This is more of a diagnostic for alignment objective.

**Summary of Fusion Plan:** We will incorporate **multi-modal fusion** at multiple points:

* On the **architecture level**, our chosen decoder (transformer) inherently fuses via cross-attention (text queries to image keys).
* We will consider adding a **learned query transformer (Q-Former)** in between encoder and decoder to better extract visual tokens for the decoder. This is especially useful if decoder is largely frozen and expects a certain kind of input. If we fully fine-tune decoder, we might not need it, but it could still help.
* We will use **contrastive alignment** (loss-based) to ensure image and text modalities share information (like CLIP-style loss or by using a common embedding space).
* We will experiment with **gated fusion** if we observe issues like the decoder ignoring image (one can detect that if, say, we zero out image features and the model still outputs something sensible, that means it wasn’t relying on them enough).
* For **multi-task**, we might incorporate tasks that help fusion, like image-text matching classification (did the generated caption match the image?), or masked token prediction (to ensure model learns to look at image to fill blanks).

**Licensing in Fusion Models:**

* ALBEF, BLIP code is mostly under MIT/BSD and we can reuse ideas or maybe some code from LAVIS (an open library by Salesforce) which has BLIP implementation under BSD.
* If we use CLIP’s alignment, that’s MIT via OpenCLIP, so fine.
* CoCa was not released in detail but the concept of combined losses is not protected.
* Q-Former and BLIP-2’s weights: we could use their pre-trained Q-Former (which is trained to output 32 queries corresponding to image content). If we plug that in and then fine-tune with our decoder, that could give a jumpstart. BLIP-2’s pretrained Q-Former is a small model we can likely incorporate (and it’s in LAVIS library, license BSD). We have to be mindful to cite BLIP-2. The weights themselves, not sure if LAVIS includes them or requires download from a model zoo – probably HF has them under a friendly license.

Thus, multi-modal fusion will be a significant part of our improved system. By aligning and jointly modeling image and text through these techniques, we aim for a model that truly understands the image and can articulate it in words, rather than just tagging or shallow mapping. This will reflect in higher-quality captions, especially for more complex scenes where relationships and context matter.

## 6. Training Methodologies

How we train the model can be as impactful as the architecture itself. We'll explore various training strategies to maximize caption accuracy and diversity, beyond standard maximum-likelihood training:

* **Two-Stage or Curriculum Learning:** We touched on **curriculum learning** earlier – presenting easier tasks first. In captioning, one curriculum approach is to start with simpler captions or simpler objectives. For instance, start by training the model to predict *image tags* or single-word labels, then move to full sentences. Alternatively, start with captions of lower complexity (shorter length, or containing only one clause) then gradually include longer, compound sentences. Another curriculum dimension is by image complexity (images with one salient object vs. crowded scenes). As noted, Zhang et al. (EMNLP 2022) measured difficulty via cross-modal similarity. We could approximate that by taking each training pair and scoring it with CLIP: pairs with high CLIP image-caption similarity are presumably easy (the caption describes well what CLIP sees), whereas low similarity pairs might indicate unusual captions or very detailed ones (harder). We could then schedule training to focus on high-similarity pairs for a few epochs, then expand.

  Another form: **teacher forcing schedule**. Initially, we always feed ground-truth words to the decoder (classic teacher forcing). As training progresses, some schedules start feeding the model its own predicted words some percentage of the time (to simulate test conditions and mitigate exposure bias). This is known as **scheduled sampling** (Bengio et al. 2015). It can help the model not to be brittle at inference when ground truth isn't available. We could incorporate scheduled sampling: e.g., after epoch 5, with probability 0.1 use the model's predicted word instead of the true next word when computing the next step, and gradually increase that probability. However, pure scheduled sampling sometimes makes training unstable (non-differentiable sampling); some do it only in later training or couple it with reinforcement learning.

* **Self-Critical Sequence Training (Reinforcement Learning):** A widely used approach in image captioning is to fine-tune the model with reinforcement learning to directly optimize metrics like CIDEr. This is often called **Self-Critical Sequence Training (SCST)** by Rennie et al. (2017). The algorithm:

  1. Use the current model to sample a caption (say using multinomial sampling or beam search).
  2. Also get a baseline caption (often the model's greedy output).
  3. Compute a reward – typically CIDEr score of the sampled caption against references.
  4. Compute reward\_baseline (CIDEr of baseline).
  5. The loss is then the negative expected reward: \$L = -(R(\text{sample}) - R(\text{baseline})) \sum\_{t}\log P(y\_t^{(sample)})\$.

  This trains the model to generate outputs that improve CIDEr over its own baseline performance, thus maximizing expected CIDEr. SCST resulted in large boosts in CIDEr (often +10 or more) at the cost that captions can become a bit more repetitive or generic (since CIDEr rewards n-gram overlap, the model might overfit to frequent phrases). But with careful tuning (like not running RL too long, or mixing with MLE loss), it’s very beneficial. We plan to implement a phase of SCST after initial cross-entropy training to polish the model for metric gains.

  We can also experiment with optimizing other metrics:

  * **CIDEr+SPICE:** some have tried a weighted sum of CIDEr and SPICE as reward, to balance factuality and qualitative aspects.
  * **CIDEr + Diversity Penalty:** one could add a term that penalizes repeating a caption seen in training or that encourages using rare words (to increase diversity).
  * **CLIPScore as Reward:** Another idea: use CLIPScore (which measures image-caption similarity directly) as part of reward. A caption that is very aligned with the image (CLIPScore high) should be good, even if wording differs from references. If our aim is beyond just pleasing COCO metrics, optimizing CLIPScore can lead to more image-faithful captions. E.g., a model might mention an object not in references (thus lowering CIDEr) but which is clearly in the image (CLIPScore would catch that). Hessel et al. (EMNLP 2021) showed CLIPScore correlates well with human judgment. We could define reward = CIDEr + λ \* CLIPScore, and try RL on that. However, that study also noted combining CLIPScore with CIDEr yields marginal improvement beyond CIDEr alone in terms of standard metrics. But perhaps in human eval it helps.

  We have to be cautious not to overweight CLIPScore if using it, because CLIPScore might encourage describing something even if minor (since CLIP might see it). Could lead to overloading caption with details (some might consider that good, but if it harms readability, humans might not like it).

  Implementation detail: computing CIDEr every iteration is expensive (requires computing n-gram stats vs references). Typically, one uses COCO API which is in C for speed. It's okay if batch size is moderate. CLIPScore computation means running CLIP on image and text, which is also heavy, but if batch on GPU it's manageable. We could restrict RL to maybe every few batches or something if performance is an issue.

* **Rewarding Diversity or Penalizing Repetition:** If the goal includes linguistic diversity, we might incorporate something like:

  * **Distinct n-gram reward:** e.g., reward = CIDEr + α \* (number of distinct bigrams in caption). This could encourage adding a secondary phrase or adjective. But it could also encourage adding fluff.
  * **Repetition penalty:** during decoding, some do a penalty on log-probabilities if a word is already used. For RL, one could define a negative reward if caption has a repeated 4-gram (for example). This gets complex in RL, easier handled in inference with nucleus sampling or penalty.

  We should note, baseline LSTM captioners often had issues of repeating "… on a beach on a beach" etc. Transformers with cross-attn handle that better typically. If we see repetition problems, we can address it via penalties or by adjusting beam search to avoid them.

* **Multi-Task Training:** As discussed, we can train the model on multiple tasks simultaneously, which can improve the visual representation and language grounding:

  * **Image-Text Matching (ITM):** Add a classifier head to predict if an image and a caption belong together or not (a binary classification). We can feed the image and either the ground-truth caption (positive) or a mismatched caption (negative) through our model and take some joint representation (like maybe the decoder’s start token output after attending image, or a pooled encoder-decoder feature). Then train with binary cross-entropy to distinguish real pairs vs fake. This encourages the model to produce representations that not only generate good captions but also understand alignment (helpful to not produce things that don't match image because then that representation would fail ITM). ALBEF and others used such an ITM loss along with contrastive.
  * **Masked Language Modeling (MLM):** We can randomly mask some words in the ground truth caption and ask the model to predict them given the image and the rest of caption. This is like how BERT is trained, but now conditioned on image too (like in **UniLM or VLP** efforts). This might help the model utilize context better and not rely only on left-to-right history. It also provides a way for the model to learn from partial captions, potentially enhancing its ability to start mid-sentence (though that’s not needed in normal generation).
  * **Object Classification Auxiliary:** Use the image encoder’s output to predict object presence (as earlier mentioned in multi-task section of analysis). For example, add a sigmoid multi-label classifier on top of encoder that predicts the 80 COCO classes that appear. Train it with ground truth object labels (we can get those from COCO annotations). This encourages encoder to recognize objects even if caption didn't mention them, giving richer info to decoder. Ideally, the decoder then has the capacity to mention those if relevant. This addresses the issue where captions might skip some objects. The model might choose to include them if it knows they're there, improving recall. We must be careful to not confuse the model (like it might start adding objects that are in image but maybe not central – but if they are indeed in image, it's not wrong, though references might not mention them).
  * **Caption in other language or style (if data available):** not applicable for COCO, but multi-task could include training on conceptual captions (which are simpler, factual style) to broaden style.
  * **Reinforcement learning with human feedback (RLHF):** We discussed RL with metrics; RLHF goes further by using actual human preference ratings. If we had a dataset of e.g. pairs of captions for an image with human preference of which is better (like the Lan et al. 2020 **CIDErBtw** or humans ranking outputs), we could train a reward model and then fine-tune on that. This is heavy on data collection though. Alternatively, we ourselves or testers can manually tweak and see how output is qualitatively, but not a full RLHF pipeline due to time. There are some relatively small datasets like Google’s **Chrome UX Captions** (human rated) or the COCO human evaluation from 2015 (they had some human scores). But not sure if publicly available in form we can train on. If this were a product, we could slowly incorporate user feedback (e.g., user edits to captions) to refine it. But that's long-term.

  We will focus on what we can do now: metric optimization (which is surrogate for human satisfaction up to a point).

* **Self-Supervised Pretraining:** The "modernize" objective suggests maybe using new training paradigms beyond supervised on COCO:

  * We could pretrain our model on a large image-text dataset (Conceptual Captions, Flickr30k, SBU, VizWiz, etc.). If we have access/time, training even on Conceptual Captions 3M could improve initialization. Or we can initialize from a model that was already pretrained on such (like use weights from OFA or from BLIP if available). However, using someone else's weight might entangle license (e.g., OFA might be MIT though).
  * **Unsupervised data with pseudo labels:** BLIP did this effectively (generate captions for images and train on them). We could do a mini version: perhaps use our model (after supervised training) to caption additional images (like images from OpenImages or unlabeled COCO test), filter good ones with CLIPScore, then fine-tune on those as augmentation. This might increase diversity (because those images might have scenarios not fully in train).
  * Given time constraints, probably we won't do large-scale pretraining ourselves, but we'll leverage existing pretrained models as our form of pretraining.

* **Hyperparameter and Optimization Techniques:**

  * We'll likely use **AdamW** optimizer (Adam with weight decay) as it's standard for transformers.
  * A **learning rate schedule** (warm-up then cosine decay) is common to stabilize training of large models.
  * Mixed precision and gradient accumulation to handle large batch effective size without OOM.
  * Possibly use **gradient clipping** if we see any exploding gradients with RL (since RL gradients can be noisy).
  * Use **early stopping** or checkpoint averaging: for cross-entropy training, one might stop when CIDEr on val plateaus, then switch to RL.
  * Keep an eye on **overfitting**: If the model gets too high BLEU/CIDEr on train vs val, might regularize (dropout in transformers, which is usually already at 0.1 by default, might suffice).
  * We might also ensemble models or use **beam search ensemble** at inference for a boost, but that's more of an inference hack than training method.

**Licenses for training data:**

* COCO is fine for our usage (just mention it).
* If we use any external (like Conceptual Captions is open accessible with link at CC license, fine).
* Some may use Flickr30k (some images might not be all commercial license, but as a widely used research dataset it’s considered okay to train).
* We'll mostly stick to COCO to avoid complexity, but conceptually these training strategies apply if we had more data.

**Outcome Expectations:**

* After cross-entropy training (MLE), we might get CIDEr \~120. After SCST RL, perhaps push it 5-10 points higher (depending on baseline, could reach 130+).
* Multi-task and pretraining should improve sample efficiency and potentially metrics like SPICE and BERTScore, making captions more semantically correct and complete.
* We need to watch that diversity doesn’t drop severely after RL (common issue: model finds a safe mode to maximize CIDEr by overusing common words, leading to less lexically diverse outputs). If that happens, we might tune the reward (e.g., include a diversity term or stop RL a bit earlier).
* We’ll evaluate intermediate outputs qualitatively to ensure they remain descriptive and not overly robotic (one reason to incorporate maybe some human-like metric or constraints).

Finally, **reproducibility** in training: we will fix seeds and possibly use deterministic algorithms for LSTM part if any remains (for transformers it's fine). Logging everything (loss curves, metric values, maybe attention maps) helps track training dynamics. We'll likely leverage frameworks like Weights & Biases or simple CSV logging to record results for each experiment, so we can compare variants (this is important given all these possible variations like with or without RL, with different decoders, etc.).

In summary, our training plan might be:

1. **Phase 1:** Supervised MLE training with cross-entropy (maybe 10-20 epochs or until val CIDEr saturates).

   * Possibly incorporate multi-task losses (ITM, tags) during this phase.
   * Use teacher forcing (with maybe scheduled sampling towards end if needed).
2. **Phase 2:** Reinforcement fine-tuning (SCST) optimizing CIDEr (maybe + CLIPScore).

   * We’ll use a small learning rate here (since model is nearly converged on MLE).
   * Perhaps mix a bit of MLE loss still (some do a weighted sum: e.g., 0.5*RL + 0.5*MLE for stability, at least in initial iterations).
3. **Phase 3 (optional):** Fine-tune or adjust for diversity/human-preference. For example, if RL made sentences too templated, we can do a tiny epoch of MLE again or adjust decoding parameters at inference to reintroduce diversity.
4. Evaluate on validation and test splits with all metrics.

We will also handle **configuration management** so that any combination of these can be turned on/off easily (like a config flag for "use\_rl", "use\_itm\_loss", etc.). This ensures we can ablate which training method contributed what improvement.

## 7. Evaluation Metrics

Moving beyond BLEU, we will employ a suite of evaluation metrics to thoroughly assess caption quality:

* **BLEU (1-4):** BLEU scores evaluate n-gram overlap between the generated caption and references. BLEU-1 focuses on unigram (word) precision, BLEU-4 up to 4-grams. We will report BLEU-4 primarily (as is standard in papers) and possibly BLEU-1 to gauge how many individual words overlap. BLEU is useful but has well-known issues for captions: it doesn’t account for synonyms or grammatical fluency well. It's heavily influenced by exact word matches, so if our model says "kids" and reference says "children", BLEU-1 won't give credit. Still, BLEU-4 is included for legacy comparison (most earlier models reported it). Typically, a good model might get BLEU-4 in the mid-30s on COCO (with 5 refs). Human captions often reach \~0.35-0.40 BLEU-4 when comparing one human to others.

* **METEOR:** METEOR is based on uni-gram alignment between candidate and references, with recall and precision combined via a harmonic mean, and includes stemming and synonym matching via WordNet. It also has a fragmentation penalty to punish scrambled word order. METEOR tends to correlate better with human judgment than BLEU for captions. It ranges 0-1; on COCO, good models might get around 0.27-0.30 METEOR. We will use METEOR to check if the model is conveying meaning even if wording differs, since METEOR will catch stems and synonyms. If our model uses a different tense or plural vs singular but same meaning, METEOR gives partial credit.

* **ROUGE-L:** ROUGE-L measures the longest common subsequence (LCS) recall between candidate and references. It's a proxy for overlap allowing gaps. Many caption papers report ROUGE-L (as a percentage). It's somewhat redundant with BLEU and METEOR, but we can include it for completeness. It often correlates with BLEU to some degree, focusing more on recall. A high ROUGE-L means the candidate contains most of the reference words in order (not necessarily contiguous but in sequence). If our model tends to paraphrase with synonyms, ROUGE-L might drop while METEOR might not as much (since METEOR catches synonyms).

* **CIDEr:** The Consensus-based Image Description Evaluation (CIDEr) metric is specifically designed for captions. It uses TF-IDF weighted n-gram matching, averaging over reference captions. The intuition is to reward n-grams that are important (appear in some references but not all references of all images get high IDF). It normalizes by number of references. CIDEr is currently the main metric for COCO competition because it correlates reasonably with human consensus and doesn't overly penalize creative phrasing if it's consistent with what people mention. We will heavily use CIDEr for optimization (RL) and evaluation. It’s often scaled by 10 in reports (some say 120 instead of 1.20, etc.). We will treat it as a numeric value \~0-∞; typically COCO captions models range 0-1 (like 1.2 = 120 if scaled). A really good model might get \~1.3 (130). Humans (as a "system" vs other human references) score around 0.85-0.9 on CIDEr because individual human might not mention everything all other 4 mention. But models can exceed that by combining content from multiple references (which might actually be verbosity). It's known that optimizing CIDEr heavily can lead to overspecified captions that cram a lot of details (because if some references mention X and others mention Y, model says both to get both n-grams). Sometimes that’s fine if image indeed has both, but can sound unnatural. We will watch out for that. Nonetheless, high CIDEr typically indicates the caption is covering many points of agreement with references.

* **SPICE:** SPICE (Semantic Propositional Image Caption Evaluation) parses both candidate and references into scene-graph tuples (objects, attributes, relations) and computes an F-score. For example, it checks if the objects mentioned by the candidate are a subset of those in references, and similarly for relations. SPICE correlates strongly with human judgment on content (what is described) because it ignores fluency. It gives separate scores for objects, attributes, and relations, and combines them. We will use SPICE to evaluate how well the model captures the semantics of the scene. SPICE is slower (requires NLP parsing; the COCO evaluation server uses Stanford Scene Graph Parser). We'll compute it on the test set or validation after generation (maybe not every training epoch due to speed). A good model might get SPICE \~0.22-0.25. Humans get maybe \~0.30 on SPICE (because they identify more unique objects correctly). SPICE is especially useful if we want to see improvement in relational descriptions – e.g., if after an upgrade model better says "on the table" vs just listing objects, relation sub-score will go up. We aim to improve SPICE by using better attention and training signals (like including relation terms in data or forcing usage of attributes).

* **CLS-based Metrics (BERTScore, MoverScore):**

  * **BERTScore** calculates similarity between candidate and references in embedding space (from BERT or other LM). It matches each word in candidate to a similar word in reference via cosine similarity of contextual embeddings, and computes precision/recall/F1. It captures semantic similarity even if exact words differ. We'll use BERTScore with a strong model like RoBERTa-large as recommended. BERTScore F1 typically correlates with human judgments well. If our model describes something accurately but with different words, BERTScore will reflect that. For example, "a young boy" vs "a little kid" might score high on BERTScore even if BLEU low. We'll compute BERTScore to ensure semantic adequacy.

  One caution: BERTScore can sometimes be too generous (if candidate is a more verbose description that logically encompasses references, BERTScore will be high even if extraneous info is present, as long as it doesn’t contradict references).

  It's also good to see if RL optimization of CIDEr might reduce BERTScore sometimes (if model starts using very common words to game n-grams, semantic richness might drop). We want to keep BERTScore high as a check (some papers use BERTScore to fine-tune or evaluate when focusing on semantics).

  * **MoverScore** (less common) uses Word Mover's Distance concept with BERT embeddings. Possibly skip for brevity, since BERTScore suffices.

* **CLIPScore:** As discussed, CLIPScore uses CLIP to measure image-caption similarity directly. They compute it by embedding the image and caption with CLIP and taking their cosine similarity (then scaling to, say, 0-100 range by a factor). It's reference-free (only needs the image and candidate). We'll use CLIPScore as a sanity check that our captions remain relevant to the image, especially if we do heavy language model integration or RL. For instance, if we fine-tuned strongly on text and maybe model learned a bias (like always saying "a beautiful image of ..."), CLIPScore might drop if that phrase isn't actually aligning with image content. If we see CLIPScore going down while CIDEr goes up, it's a red flag that model might be focusing on pleasing references rather than describing actual image (overfitting references). Ideally, our improvements raise both CIDEr and CLIPScore.

  For reference, human captions have CLIPScore \~0.95 (depending on model, if using CLIP ViT-B or ViT-L). A good model often gets around 0.93-0.94 CLIPScore (slightly lower than humans since sometimes they miss something obvious that CLIP would expect, or use an uncommon phrasing CLIP hasn't seen as much). We will target maintaining or improving CLIPScore compared to baseline.

* **Diversity Metrics:** We will also evaluate diversity of language:

  * **Vocabulary Size:** Count of unique words generated across the test set. If our model uses 800 distinct words vs baseline 600, it’s more diverse lexically (assuming similar quality).
  * **Usage of Rare Words:** We can measure how often words outside the 1000 most common in train appear (if appropriate).
  * **Distinct-N:** For test set, compute distinct-1 and distinct-2 (percentage of unique unigrams and bigrams among all generated captions). A model that repeats stock phrases will have lower distinct-2. If our enhancements truly improve diversity, we should see distinct-2 go up. (One has to be careful as some improvements might only produce diversity by adding irrelevant stuff, but combined with human eval, we can judge).
  * **Self-BLEU:** a measure sometimes used in GAN text generation to measure how similar a model's outputs are to each other. Compute BLEU between every pair of generated captions in the test set; a low self-BLEU means outputs are varied. If a model always produces similar sentence patterns, self-BLEU will be high. We can sample multiple captions per image and measure diversity among them too (like variance or entropy of output distribution).

  These diversity metrics aren't standard for COCO eval but are useful for research analysis, given our goal to not just maximize metric but also variety. We'll primarily use them to ensure that after reinforcement learning, we haven’t collapsed the language to a template.

* **Human Evaluation Protocols:** Automatic metrics have limitations, so human evaluation is the gold standard:

  * We might not have time to do a full user study, but ideally, we would do something like: show human evaluators a random sample of images with either our caption vs baseline caption (A/B test) or vs ground-truth and ask which is better or if equally good.
  * Criteria often include correctness (does the caption correctly describe the image?), completeness (does it miss important details?), and fluency (is it well-phrased and grammatical?).
  * In the 2015 COCO test, they used two measures: *M1:* percentage of captions evaluators labeled as better or equal to human caption, *M2:* percentage that passed the Turing Test (evaluator couldn’t tell if it was human or machine). We could simulate a small version of that.
  * For example, we can gather \~100 image outputs from our model and maybe from a baseline or from ground truth, and have colleagues (or ourselves, but that introduces bias) rate them.
  * If doing formal: create a questionnaire where each image appears with two captions (blind who is model/human) and ask "Which caption is better? A, B, or tie?" and "Does caption A sound human-written? yes/no".

  Since we likely cannot do a large human study, we rely on metrics like CIDEr, SPICE, CLIPScore which correlate with human judgment to some extent. BERTScore and CLIPScore now often are considered proxies for human eval (especially CLIPScore for factual accuracy).

  But for final evaluation in a report, we might say something qualitatively like: "The improved model generates more detailed and image-specific descriptions as seen in examples, without loss of fluency. In a small anecdotal human survey, people preferred our captions X% of the time over baseline."

* **Examples and Qualitative Analysis:** We will also provide a few example outputs to illustrate differences:

  * Before/after or ours vs baseline on some tricky images (like multiple objects, where baseline might miss one and ours captures both).
  * Show an example where metrics might be misleading: e.g., our caption uses a synonym but metrics struggle, yet semantically it's good. This underscores the need for metrics like BERTScore.

* **Benchmarking:** We'll compare our results to published state-of-the-art:

  * Current SOTA on COCO (Karpathy test split) might be something like BLIP-2 or an ensemble with around 140+ CIDEr, BLEU4 \~0.38, METEOR \~0.30, SPICE \~0.24. We will see how close we get.
  * We'll list baseline (maybe the original Show-Attend-Tell or a vanilla transformer baseline) and our enhancements.
  * Also note if we meet or exceed human performance in some metrics (commonly, good models exceed human on BLEU and CIDEr because they can incorporate multi-reference info, but no model truly "beats" human in an absolute sense of quality).

The broad set of metrics ensures we evaluate from different angles:

* N-gram overlap (BLEU, ROUGE, CIDEr),
* Semantic content (SPICE, BERTScore),
* Relevance (CLIPScore),
* Readability/human-likeness (somewhat METEOR covers fluency better than others due to fragmentation penalty, and human eval).
* Diversity (Distinct, etc.).

We’ll be careful in interpretation: e.g., a slightly lower BLEU but higher SPICE might be fine (maybe model uses a synonym consistently, hurting BLEU, but actually describes correctly). Conversely, a model could have high BLEU by copying training phrases without truly looking at image; CLIPScore or SPICE would catch that problem.

We also plan to follow the evaluation guidelines strictly (like using the official COCO evaluation scripts) to ensure comparability. We'll cite any metrics if needed (like mention CLIPScore correlation findings, SPICE paper for scene graphs, etc.).

## 8. Inference Strategies

At inference (deployment) time, we have several decoding strategies to balance quality, diversity, and speed:

* **Greedy Decoding:** This picks the highest probability word at each timestep (argmax of the model's output distribution). It's the simplest and fastest (only one sequence to compute). Greedy ensures a deterministic single output for a given image. However, it often yields safe, high-probability captions that might be **generic** or overly short. The model’s learned probability may put a safe caption like "a person standing in a room" as highest likelihood for many images, even if a more specific caption was possible, because the safe one fits many images. Greedy can thus hurt diversity and sometimes accuracy (if the model has uncertainty, it might avoid committing to a detail and choose a bland word).

  On the plus side, greedy decoding tends to maximize the model's perceived probability, which correlates somewhat with typicality. For usage where consistency and predictability are valued (maybe some real-time systems prefer a stable output), greedy could be acceptable. But for best descriptive power, we usually prefer strategies that explore alternatives.

* **Beam Search:** Beam search explores multiple candidate sequences in parallel. We keep a beam of size B (e.g. 3, 5, or 10). At each step, we expand each partial sequence by all possible next words, then prune to the top B overall. This finds sequences with higher overall probability than greedy because it can circumvent locally suboptimal choices. Beam search typically improves coherence and completeness of captions up to a point (a moderate beam like 5 is common in literature). With beams, the model can consider some less likely early word if it leads to a better continuation later.

  However, large beam sizes can lead to very generic, high-probability captions (the infamous "beam search degeneration" in language generation where beam search yields repetitive or overly common phrases because those maximize probability). In image captioning specifically, beam beyond size \~5-7 often yields diminishing returns on CIDEr and can even drop diversity. A known phenomenon: as beam size grows, captions tend to converge to similar safe descriptions. The highest probability sequence might be one that hedges ("a person in a room") that is not specific but is safe given the model's learned distribution. So there's often a sweet spot (beam 3 to 5).

  We will likely use **beam size 5** for generating our final captions when evaluating against reference metrics. This is what many recent models do (some do beam 5 or 10).

  We may also apply some heuristics with beam:

  * **Length normalization:** Without it, beam search tends to prefer shorter sequences (since probability multiplies terms, shorter can have higher product). Many implementations add a length penalty or normalize by length. The COCO evaluation uses a brevity penalty in BLEU, but for generation, folks often use a length normalization factor (like divide log-prob by length^α, α \~0.7).
  * In our experiments, we'll ensure beam search doesn't produce overly short captions (lack detail) by using either mild length normalization or minimum length constraint (like at least 3-4 words).

  Beam search is slower than greedy – beam 5 means roughly 5 times more computations per step, though some parts can be vectorized. On a GPU, beam 5 is fine for a single sentence generation in tens of milliseconds. But on CPU or mobile, maybe not if the model is big (that’s where efficiency concern for real-time comes in).

* **Top-K Sampling:** Instead of deterministic choice, we can randomly sample the next word from the model's probability distribution restricted to the top K most likely words. This introduces variability and can yield more creative or diverse captions on different runs for the same image. For instance, if model is somewhat unsure whether an object is a "boat" or "ship", sampling might sometimes pick "boat", sometimes "ship", whereas argmax might always pick one. This helps especially if we want multiple caption options or to avoid the same phrasing every time.

  Choosing K (like 5, 10, 50) is important:

  * If K is too high, we might pick a really unlikely word that makes no sense (like a hallucination).
  * If K is too low, it's close to greedy.
    A typical default might be K=20 or 50 in some text generation tasks. But for captioning, maybe smaller, since we want mostly correct nouns and such (the model's probability distribution tends to already heavily peak on one or two plausible objects).

  We might experiment: for example, generate 3 captions with top-K (K=20) sampling and see if they are valid and diverse.

  We need to set a random seed for reproducibility if needed, or just allow randomness for variety.

* **Top-p (Nucleus) Sampling:** Instead of fixed K, choose the smallest set of words such that their cumulative probability ≥ p (like 0.9). Then sample from that set. This adapts to confidence: if the model is very certain (one word has p=0.95), nucleus will take that 0.95 alone (essentially deterministic in that case). If model is uncertain and has many possible words each with small probability, nucleus ensures we include as many as needed to cover e.g. 90% mass – that could be 5 words or 50 depending on distribution. This usually results in more natural randomness than top-K, because it doesn't include very low probability tail words that would rarely be chosen by a human either (since if tail is beyond 0.1 cumulative mass, those words were extremely unlikely under model).

  We likely prefer nucleus sampling over top-K for diversity generation. A common setting is p=0.9 or 0.95. This means sometimes only a couple options (if one is 0.8 next is 0.1, total 0.9, then top-2 considered), sometimes many if distribution flat. It tends to reduce weird out-of-context picks.

  We'll try nucleus with p=0.9 for generating alternate captions or for some qualitative exploration of model's range.

* **Temperature:** Both top-K and nucleus sampling can be combined with a temperature parameter. Temperature T=1 means no change. T < 1 (like 0.7) makes distribution sharper (more greedy), T > 1 (like 1.5) makes it flatter (more random). If we find model is too deterministic, we can raise T to inject more randomness; if it's too random, lower T. We should be careful though: raising T can cause model to output lower-probability words including possibly incorrect ones. Lowering T can converge to greedy. Possibly we leave T=1 and rely on top-p to control randomness.

* **Diverse Beam Search:** If we want a set of captions with differences, we can run **diverse beam search** (Vijayakumar et al. 2016) which groups beams and adds a penalty for beams in the same group sharing words. For example, generate 2 groups of 3 beams each, and penalize if two beams use the same bigram. This encourages exploring different phrasings or interpretations.
  We might not implement full diverse beam search, but we can approximate by generating multiple beams and post-filter them for uniqueness or by tweaking score with an anti-repetition penalty across beams.

  Another approach: just run nucleus sampling multiple times to get multiple captions (likely simpler and yields variety).

* **MBR (Minimum Bayes Risk) Decoding:** This is advanced but interesting: generate a bunch of candidate captions (via sampling or beams), then choose the one that has the highest average similarity to all other candidates according to some metric. If we use e.g. CIDEr or BLEU as the utility, MBR tends to select a caption that is sort of centrally phrased among the candidates (not too outlier). This can improve quality if the model sometimes produces one weird caption among mostly good ones – MBR will likely avoid the weird one.

  MBR with CLIPScore as utility was used to refine diversity outputs in some work. For instance, generate 50 samples, then pick the one with highest average CLIPScore against the others (or highest minimum CLIPScore vs others) which tends to be a safe but relevant caption.

  MBR is computationally expensive (need to evaluate similarity for many pairs). But if we generate, say, 10 candidates, we can do it. It might give a small boost in metrics/human pref because it picks a consensus best.

  We could also use a simpler re-ranking: generate beam 5 and also 5 random samples, have 10, then choose the one with best CLIPScore or highest sum of CIDEr+CLIPScore. Re-rankers like that often improve output (some winning submissions have an ensemble or use a second model to choose best out of N).

  Since we plan to integrate CLIPScore as a metric, using it for re-reranking is logical: e.g., if we generate with beam search and get 5 candidates, we can compute CLIPScore for each and pick the max. That ensures the chosen caption is very aligned with image content as per CLIP.

* **Low-Latency Considerations:** In deployment, if running on a server with GPU, even a beam of 5 with a big model can be done in tens of milliseconds per image, which might be fine (COCO test \~5k images, even 0.1s each is 8 minutes to process offline).

  But if we consider edge devices or real-time constraints:

  * We might opt for **greedy or beam=2** to reduce compute.

  * Or use a smaller model specifically distilled for speed. We can consider knowledge distillation: train a small Transformer or even a CNN+RNN model to mimic the large model's output on training data. This small model (say MobileNet + small GRU) could run fast on mobile. It will sacrifice some accuracy though.

  * Another approach is **quantization** of the model (int8 or float16 on CPU). For edge, one might convert the model to a format like CoreML or TFLite and use int8. That can be done once we have a final model, if needed. Many transformer ops can be quantized with minimal loss.

  * **Partial captioning / streaming:** If real-time description is needed (like narrating a video live), one might generate word by word as image changes. Our model isn't specifically built for streaming, but a fast greedy decode each second could serve as a running commentary. That’s more relevant to video captioning though.

  * If generating multiple captions (for user to choose), we should ensure not too slow. Perhaps generate top-3 beams or samples. That's likely fine.

* **Final Deployment Mode vs Evaluation Mode:** For evaluation (to compare with references), typically one uses beam search to get a high CIDEr-scoring caption. For a user-facing scenario, one might actually prefer a bit of randomness to avoid monotony if it's an app used repeatedly by same user (so image captioning apps sometimes have a "try another caption" button which just resamples).

  Possibly we provide an option: default uses beam for a solid caption; a "More captions" button could show 2-3 alternatives using nucleus sampling to highlight other aspects.

  For example, image of a family on beach: one caption might emphasize "A family posing on a beach", another (via sampling) might say "Two children and their parents on a sandy beach under a clear sky". Both correct, focusing differently. Such diversity can be user-pleasing as they get a fuller description if needed.

* **Ensuring Non-Repetition and Coherence:** We'll apply typical constraints like no repeating the exact same phrase twice (if beam tries a loop like "the the the", we usually handle it by language model itself, but if it happens, we can add bigram blacklist).

  * Possibly implement a rule that if a word has been output and model tries to output it again immediately and it doesn’t make sense (like repeating noun twice), skip that token or lower its score (there’s something called "coverage penalty" or "no repeat n-gram constraint" often used in summarization to avoid repeated phrases).

  Transformers are pretty good at not repeating, especially if well-trained, but when optimized for metrics, they sometimes repeat adjectives or prepositional phrases.

In our evaluation code, we will implement beam search with an optional length penalty (commonly length normalization with alpha \~0.8 as used in some papers). We'll also implement nucleus sampling for generating alternate outputs.

We'll present results mostly for beam (since that's how metrics are normally reported). But in discussion, we can mention the model is capable of generating multiple diverse captions and perhaps show examples.

**Summing up:** We will likely use:

* **Beam search (size 5)** for primary results evaluation and scenario where a single best caption is needed.
* **Nucleus sampling** for generating alternatives or demonstrating diversity.
* Possibly **CLIPScore re-ranking** if we find it improves output (we can test if picking the CLIPScore-best of beam outputs increases CLIPScore a lot and maybe even CIDEr slightly).
* We'll ensure our generation code is integrated with PyTorch's efficient methods (transformers library has `generate()` which supports beam and sampling with various settings, we might use it to avoid bugs).
* For completeness, we could provide both the raw generation output and reference comparisons for metrics, and maybe output some additional info like attention maps for a couple images as part of analysis (not needed for the final system but for understanding and demonstration in a report).

Finally, we’ll address **containerization/integration** (though it belongs in earlier section 8 in guidelines, the user specifically asked to "Also provide guidance on: ... Containerization strategies and configuration reproducibility, licensing implications". That part we put in integration section likely.)

Given that section 8 was inference strategies, we've covered that thoroughly. We'll include a bit about PyTorch 2.x integration and containerization in a conclusion or so, but the instructions had those as separate bullets to discuss. Actually, those were mentioned at the end of user query:

* "Component-level modularization ... PyTorch 2.x ... containerization ... licensing"
  We addressed modularization in each relevant part and an earlier note. To ensure we fully answer: It's likely better to dedicate a final short section summarizing those integration best practices. However, the guidelines explicitly mention them under "Also provide guidance on:", which implies they want that included. Possibly a final heading "Integration & Deployment Considerations" as we did in analysis.

Yes, the user specifically enumerated those in the end. So we should indeed add a final section addressing them. Maybe:

## Integration and Deployment Considerations

And cover:

* Modular design (how to keep things flexible),
* PyTorch 2.x & `torch.compile` usage for speed,
* Containerization (Docker for reproducible environment),
* Reproducibility (seed, config, track versions),
* Licensing (ensuring all components can be used commercially and open-sourced as needed, mention any that cannot, etc.).

Let's add that as an actual section to ensure completeness.

## Integration and Deployment Considerations

Implementing the above improvements requires a careful, **modular design** and attention to deployment details:

* **Modular Architecture:** We will structure the code in a component-based manner to allow easy experimentation. For example, we define an `ImageEncoder` interface (with subclasses for ResNet, ViT, Swin, etc.), a `TextDecoder` interface (subclasses for LSTM, Transformer, GPT-2, etc.), and an `AttentionMechanism` module (could be single-head, multi-head, co-attention, etc.). The captioning model can then be composed by plugging these modules together. This makes it simple to swap, say, a CLIP encoder in place of a ResNet, or to compare an LSTM decoder vs. a Transformer decoder, by changing configuration rather than rewriting code. We’ll use configuration files or flags to specify which encoder/decoder to use (e.g., `--encoder=vit_base --decoder=gpt2_large`). This **component-level modularization** fosters rapid experimentation and also helps in maintenance – each module can be developed and tested in isolation (we can unit-test that the encoder produces the correct shape output, that the decoder can consume a given input shape, etc.). It also facilitates *future upgrades*: if a new SOTA backbone (say, SwinV3) comes out, we can write a new `SwinV3Encoder` class and drop it in.

* **PyTorch 2.x and `torch.compile`:** Our implementation will leverage the latest PyTorch features for performance. PyTorch 2.0’s `torch.compile` can **just-in-time compile** the model for improved inference speed and memory optimization. After constructing the model, we can do `model = torch.compile(model)` which will fuse operations and optimize execution on the given backend. This is especially beneficial for transformer models with many small operations. We’ll ensure our model code is compatible with `torch.compile` (i.e., avoid dynamic control flows that can’t be traced). Preliminary tests have shown `torch.compile` can yield significant speedups for encoder-decoder transformers on GPUs. We’ll also use **automatic mixed precision** (`torch.cuda.amp.autocast`) during training and inference to exploit Tensor Cores for faster computation – most of our operations (matrix multiplies in attention/MLP) run fine in FP16. We will maintain stability by keeping critical parts (like the final output layer during training) in FP32 as needed (AMP does this automatically for certain ops).

  We also plan to utilize **device-appropriate optimizations**: for GPU, operations like multi-head attention benefit from fused kernels (PyTorch’s scaled-dot product attention is optimized in 2.0). For CPU or mobile deployment, we may consider exporting the model (e.g., via ONNX or TorchScript) and using runtime optimizations or quantization for faster CPU inference. PyTorch 2.x still supports TorchScript; we can **TorchScript** the model for environments where Python runtime overhead is undesired (e.g., C++ server deployment). Our modular design helps here: we can script each submodule if needed, or the whole model if it’s compatible (Hugging Face transformers are mostly scriptable or have `generate` methods for C++).

* **Reproducibility and Configuration:** We will fix random seeds for training (ensuring that results are reproducible across runs) and log all relevant hyperparameters and training settings. Using a config file (YAML/JSON) or experiment tracking (like Weights & Biases) will allow us to record the exact settings used for a given model checkpoint (learning rates, curriculum schedule, random seed, etc.). This way, if an issue arises or if we want to fine-tune further, we know the exact baseline to start from. We’ll also version control the code (Git) and possibly tag releases of model weights with corresponding code versions. For example, if we release a model, we will include a README specifying “Trained with ViT-B/16 encoder initialized from ImageNet-21k, GPT-2 large decoder initialized from OpenAI weights, using MS-COCO train split, cross-entropy training 15 epochs, then CIDEr optimization 5 epochs. Seed 42.” – these details ensure that others (or we in the future) can replicate or build on the work. We will also test the model on a fixed validation set periodically during training to monitor for overfitting or regressions.

* **Containerization and Deployment:** To guarantee a consistent environment, we will create a **Docker container** for the training and inference code. The Dockerfile will be based on an official PyTorch image (ensuring correct CUDA drivers, etc.) or a minimal Ubuntu image where we install specific versions of PyTorch, Transformers, etc. We’ll pin library versions (e.g., transformers 4.X, PyTorch 2.0.1) so that the behavior remains the same. The container will also include scripts to run training or inference, making it easy to deploy on different machines (from a local GPU workstation to a cloud VM) with identical results.

  For inference serving, we might create a separate lighter container that just contains the model weights and a small HTTP server (e.g., using FastAPI or Flask) to accept image inputs and return captions. This server can incorporate features like batching (processing multiple images in parallel for throughput) and can be scaled horizontally by orchestrating multiple containers behind a load balancer if needed. Because our model is modular, we could even host different variants (e.g., a “lightweight model” container using a smaller encoder for real-time applications, and a “high-accuracy model” container using the full transformer) and route requests accordingly.

  Using Docker also aids reproducibility: anyone with the image can run the model without worrying about dependency conflicts. It encapsulates the environment including any needed data preprocessing (we’ll bake in code to, say, normalize images and tokenize text exactly as during training).

* **Edge Deployment Considerations:** While our primary target is mid-to-high GPU servers, we keep an eye on eventual edge deployment. This influences some choices:

  * We will try to keep the model as **efficient as possible** without sacrificing quality – e.g., using a ViT-B instead of ViT-L if it gives 95% of performance for half the compute. The modular design means we can easily swap in a more efficient encoder or quantize the model. For instance, we could quantize weights to int8 for CPU inference. Tools like PyTorch Quantization or ONNX Runtime’s quantization could be applied to the model, and since transformers are mostly matrix mults, quantization should yield a good speed-up with minor accuracy loss.
  * We will structure the model so that it can run in a **streaming or batched mode**. For example, ensure the decoder can generate tokens one by one if needed (important for not blocking too long in an interactive setting). HuggingFace Transformers’ `generate()` method already does efficient incremental decoding, which we will leverage.
  * If deploying on mobile (e.g., iOS CoreML or Android NNAPI), we would convert the model (perhaps after distilling or trimming it) to those formats. Our code choices (like standard ops) and avoiding custom layers (where possible) will ease this. For example, by using HuggingFace’s implementations for attention, we ensure compatibility with their export tools to CoreML or TFLite.

* **Licensing and Open-Source Implications:** We choose components and data with permissive licenses to enable both open-source and commercial use:

  * All code we write will be original or derived from permissively licensed sources (we will license our code under MIT or Apache 2.0). We will include copyright notices for any third-party code we incorporated (e.g., if we use some utility from COCO evaluation script (which is BSD) or from Transformers (Apache 2.0), we retain their license text as required).
  * **Pretrained Models:** ViT, ResNet, Swin, etc. are under Apache or MIT licenses (weights provided by official sources are intended for free use). BERT, GPT-2, T5, etc. are also under permissive licenses (Apache 2.0 or similar) – we will double-check model cards on HuggingFace for any unusual restrictions, but e.g. GPT-2’s weights are published by OpenAI without a non-commercial clause (OpenAI’s GPT-2 is MIT licensed) and widely used in commercial products. CLIP’s code is MIT and OpenCLIP’s models are MIT; OpenAI’s CLIP weights have a caution that they’re not tested for all use cases, but no formal prohibition – still, we opt for the OpenCLIP reimplementation weights which are completely open. We will include an acknowledgment of CLIP’s paper and OpenAI in documentation (attribution is good practice even if not legally required, to credit the source).
  * **Datasets:** MS-COCO’s images are a mix of Flickr images mostly under varying Creative Commons licenses. The COCO terms allow use for research and development. For a commercial product, one typically doesn’t redistribute the images (we don’t plan to, we only use them to train). The captions in COCO were written by crowdworkers and are freely available; using them to train a model is generally considered fair use (and is standard in the field). We will still acknowledge MS-COCO dataset in any release. If we incorporate Conceptual Captions or others, we also note those (Conceptual Captions is under CC BY 4.0, meaning we should cite the source; using it to train a model and then using the model commercially is allowed with attribution).
  * **Trained Model Outputs:** Typically, the captions our model produces are not direct copies of training captions (unless the model memorized something, which with proper regularization and size is unlikely beyond maybe very generic sentences). Thus, there's low risk of copyright issues in generated captions. They should be considered original or at least transformative outputs. However, to be safe, we ensure the model isn’t overfitting (e.g., we check it doesn’t just output training captions verbatim). And we will include a disclaimer about potential biases or errors (since the model is trained on human captions that may have subjective or biased elements, e.g., assuming gender from context – we should mention that and possibly mitigate it).
  * **Commercial Use:** With all components open or appropriately licensed, there is no legal barrier to using this system in a commercial product or releasing it open-source. We will maintain a list of licenses: e.g., “This project incorporates the following open-source resources: PyTorch (BSD), Transformers (Apache 2.0), MS-COCO 2014 dataset (Creative Commons), OpenCLIP model weights (MIT), etc.” in documentation, to provide full transparency.

* **Configuration and Reproducibility:** Each experiment run (with certain choices of encoder/decoder/attention and training method) will be documented, and we can provide a configuration file or script for it. For example, a YAML might specify:

  ```yaml
  model:
    encoder: vit_base_patch16_224
    encoder_pretrained: ImageNet21k
    decoder: gpt2_large
    decoder_pretrained: openai-gpt2-large
    attention: multi-head
  training:
    epochs: 15
    batch_size: 64
    optimizer: AdamW(lr=5e-5)
    schedule: cosine_decay
    rl_after_epoch: 12
    reward: CIDEr
    mlm_loss: false
    itm_loss: true
  data:
    train_set: COCO_train2014
    val_set: COCO_val2014
    image_size: 384
  ```

  This makes it easy to rerun or tweak. We will also provide **preprocessing scripts** to ensure anyone can reproduce how images were normalized, how captions were tokenized (especially important for BPE-based models to avoid mismatches in vocabulary indexing). Using HuggingFace’s tokenizer with a specific version ensures consistent tokenization.

  In summary, by adhering to modular design, using PyTorch 2.x optimization features, containerizing the environment, and respecting licenses, we will create a **robust, reproducible, and deployable** image captioning system. This system not only pushes the accuracy and diversity of captions with modern techniques, but is also engineered for practical use and future extensibility.
